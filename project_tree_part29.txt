Questa Ã¨ la parte 29 di project_tree. Continua da quella precedente.

        for method_frame, _properties, body in channel.consume(queue):
            handler(body.decode())
            channel.basic_ack(method_frame.delivery_tag)
            count += 1
            if limit and count >= limit:
                break
        channel.cancel()
        connection.close()
    except Exception as exc:
        logger.error(f"[RabbitMQ] consume failed: {exc}")

## modules/mobile/note_interface.py
"""note_interface.py
Interfaccia HUD per Samsung Note10+ in stile Jarvis.
"""
from __future__ import annotations

import threading
import time

try:
    import requests
    import speech_recognition as sr
    import pyttsx3
    from kivy.app import App
    from kivy.uix.label import Label
    from kivy.uix.boxlayout import BoxLayout
    from kivy.core.window import Window
    from kivy.clock import Clock
except Exception:  # pragma: no cover - librerie opzionali
    requests = None
    sr = None
    pyttsx3 = None
    App = object  # type: ignore
    Label = object  # type: ignore
    BoxLayout = object  # type: ignore
    Window = object  # type: ignore
    Clock = object  # type: ignore

try:
    from voice.engine.elevenlabs_tts import ElevenLabsTTS
except Exception:  # pragma: no cover
    ElevenLabsTTS = None

HOTWORDS = ["aion", "signore", "analizza questo", "dimmi aion"]


class HUDApp(App):
    """Semplice interfaccia grafica translucida."""

    def build(self):
        if hasattr(Window, "clearcolor"):
            Window.clearcolor = (0, 0, 0, 0)
        self.label = Label(text="AION HUD", color=(0, 1, 1, 1), font_size="20sp")
        layout = BoxLayout(orientation="vertical")
        layout.add_widget(self.label)
        if hasattr(Clock, "schedule_interval"):
            Clock.schedule_interval(self._tick, 1)
        threading.Thread(target=self._listen_loop, daemon=True).start()
        return layout

    def _tick(self, _):  # pragma: no cover - placeholder animazione
        pass

    def _speak(self, text: str) -> None:
        if ElevenLabsTTS:
            try:
                ElevenLabsTTS().synthesize(text, voice="Jarvis")
                return
            except Exception:
                pass
        if pyttsx3:
            engine = pyttsx3.init()
            engine.say(text)
            engine.runAndWait()

    def _listen_loop(self) -> None:
        if not sr:
            return
        recognizer = sr.Recognizer()
        mic = sr.Microphone()
        with mic as source:
            recognizer.adjust_for_ambient_noise(source)
        while True:
            with mic as source:
                audio = recognizer.listen(source, phrase_time_limit=4)
            try:
                text = recognizer.recognize_google(audio, language="it-IT").lower()
            except Exception:
                continue
            if any(hw in text for hw in HOTWORDS):
                response = self._ask_backend(text)
                self._speak(response)
                self.label.text = response

    def _ask_backend(self, prompt: str) -> str:
        if not requests:
            return "Elaboro, signore..."
        try:
            resp = requests.post("http://localhost:8000/ask", json={"prompt": prompt}, timeout=3)
            if resp.ok:
                return resp.json().get("response", "")
        except Exception:
            pass
        return "Elaboro, signore..."


def start_mobile_hud() -> None:
    """Avvia l'app mobile HUD."""
    HUDApp().run()


[TRONCATO]

## modules/mobile_flutter/__init__.py

## modules/mobile_flutter/flutter_bridge.py
"""Launcher for the Flutter-based mobile Jarvis UI."""
from __future__ import annotations

import subprocess
import shutil
from pathlib import Path

PROJECT_DIR = Path(__file__).resolve().parent.parent.parent / "mobile_jarvis_ui"


def start_mobile_ui() -> None:
    """Attempt to launch the Flutter app."""
    flutter = shutil.which("flutter")
    if not flutter:
        print("âš ï¸ Flutter SDK not found. Please run the app manually from mobile_jarvis_ui.")
        return
    try:
        subprocess.Popen([flutter, "run", "-d", "android"], cwd=str(PROJECT_DIR))
        print("ðŸ“± Mobile Jarvis UI launched")
    except Exception as exc:
        print(f"âš ï¸ Unable to launch Flutter app: {exc}")

## modules/optional/elevenlabs_tts.py
"""
Modulo: elevenlabs_tts.py
ResponsabilitÃ : Sintesi vocale tramite ElevenLabs API.
Autore: Mercuriusâˆž AI Engineer
"""

class ElevenLabsTTS:
    def __init__(self, api_key=None):
        self.api_key = api_key or "inserisci_qua_la_tua_chiave"
        self.base_url = "https://api.elevenlabs.io/v1/text-to-speech"

    def is_available(self):
        return bool(self.api_key and "inserisci_qua" not in self.api_key)
    
    def speak(self, text: str, voice_id="default", output_path="output.mp3"):
        """
        Genera audio da testo usando ElevenLabs API (stub demo).
        """
        if not self.is_available():
            return "[âŒ API key ElevenLabs mancante]"
        # Esempio di chiamata API (stub, va integrato con richiesta reale)
        return f"[âœ”ï¸ Stub ElevenLabs]: testo '{text}' inviato a {self.base_url}"


## modules/optional/huggingface_tools.py
"""
Modulo: huggingface_tools.py
ResponsabilitÃ : Integrazione task avanzati HuggingFace (NLP, NLU, visione, ecc.)
Autore: Mercuriusâˆž AI Engineer
"""

class HuggingFaceTools:
    def __init__(self):
        try:
            from transformers import pipeline
            self.pipeline = pipeline
            self.ready = True
        except ImportError:
            self.pipeline = None
            self.ready = False

    def is_available(self) -> bool:
        return self.ready

    def run_task(self, task: str, input_data: str) -> str:
        """
        Esegue un task NLP/NLU HuggingFace (es. sentiment-analysis, summarization, ecc.).
        """
        if not self.ready:
            return "[âŒ HuggingFace non disponibile]"
        try:
            pipe = self.pipeline(task)
            result = pipe(input_data)
            return str(result)
        except Exception as e:
            return f"[âŒ Errore HuggingFace]: {e}"

## modules/optional/n8n_connector.py
"""
Modulo: n8n_connector.py
ResponsabilitÃ : Interfaccia per workflow automation tramite n8n (API, webhook).
Autore: Mercuriusâˆž AI Engineer
"""

import requests

class N8NConnector:
    def __init__(self, webhook_url=None):
        self.webhook_url = webhook_url or "http://localhost:5678/webhook/test"
    
    def is_available(self):
        # Prova di reachability
        try:
            response = requests.get(self.webhook_url, timeout=2)
            return response.status_code == 200
        except Exception:
            return False

    def run_task(self, payload: dict):
        """
        Invia un payload a un workflow n8n tramite webhook POST.
        """
        if not self.is_available():
            return "[âŒ n8n non raggiungibile]"
        try:
            response = requests.post(self.webhook_url, json=payload, timeout=5)
            return f"Risposta n8n: {response.status_code} - {response.text}"
        except Exception as e:
            return f"[âŒ Errore n8n]: {e}"

## modules/optional/plugin_manager.py
"""
Modulo: plugin_manager.py
ResponsabilitÃ : Rileva e gestisce moduli opzionali/plugin caricabili runtime.
Autore: Mercuriusâˆž AI Engineer
"""

import importlib

OPTIONAL_PLUGINS = [
    "modules.optional.huggingface_tools",
    "modules.optional.n8n_connector",
    "modules.optional.elevenlabs_tts",
    "modules.optional.vosk_stt"
]

class PluginManager:
    def __init__(self):
        self.plugins = {}
        self._load_plugins()

    def _load_plugins(self):
        for plugin_path in OPTIONAL_PLUGINS:
            try:
                module = importlib.import_module(plugin_path)
                # Usa la prima classe trovata nel modulo (convenzionale)
                cls = [obj for name, obj in vars(module).items() if isinstance(obj, type) and name != "PluginManager"][0]
                self.plugins[plugin_path] = cls()
            except Exception as e:
                self.plugins[plugin_path] = f"[âŒ Non caricato]: {e}"

    def is_plugin_available(self, plugin_name):
        plugin = self.plugins.get(plugin_name)
        return hasattr(plugin, "is_available") and plugin.is_available()

    def run_plugin_task(self, plugin_name, *args, **kwargs):
        plugin = self.plugins.get(plugin_name)
        if hasattr(plugin, "run_task"):
            return plugin.run_task(*args, **kwargs)
        return "[âŒ Metodo run_task non disponibile]"

    def list_plugins(self):
        return {k: "[OK]" if hasattr(v, "is_available") and v.is_available() else v for k, v in self.plugins.items()}

## modules/optional/vosk_stt.py
"""
Modulo: vosk_stt.py
ResponsabilitÃ : Speech-to-Text offline tramite Vosk.
Autore: Mercuriusâˆž AI Engineer
"""

class VoskSTT:
    def __init__(self, model_path="model"):
        try:
            import vosk
            self.vosk = vosk
            self.model = vosk.Model(model_path)
            self.ready = True
        except Exception:
            self.vosk = None
            self.model = None
            self.ready = False

    def is_available(self):
        return self.ready

    def transcribe(self, file_path: str) -> str:
        """
        Trascrive un file audio offline con Vosk (stub demo).
        """
        if not self.ready:
            return "[âŒ Vosk non disponibile]"
        try:
            import wave
            wf = wave.open(file_path, "rb")
            rec = self.vosk.KaldiRecognizer(self.model, wf.getframerate())
            results = []
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    results.append(rec.Result())
            results.append(rec.FinalResult())
            return "\n".join(results)
        except Exception as e:
            return f"[âŒ Errore Vosk]: {e}"

## modules/sandbox_executor/secure_executor.py
"""
Modulo: secure_executor.py
Descrizione: Esecuzione sicura di codice Python in sandbox controllata con timeout.
Autore: Mercuriusâˆž AI Engineer
"""

import sys
import io
import multiprocessing
import traceback
import contextlib

class SecureExecutor:
    def __init__(self, timeout: int = 5):
        """
        timeout: tempo massimo di esecuzione in secondi
        """
        self.timeout = timeout

    def _run_code(self, code: str, return_dict):
        """Esegue codice Python in ambiente isolato e cattura output e errori."""
        stdout = io.StringIO()
        stderr = io.StringIO()
        try:
            sys.stdout = stdout
            sys.stderr = stderr
            exec(code, {})
        except Exception:
            return_dict['error'] = traceback.format_exc()
        finally:
            return_dict['output'] = stdout.getvalue()
            return_dict['stderr'] = stderr.getvalue()

    def execute(self, code: str) -> dict:
        """
        Esegue codice Python con timeout e isolamento tramite multiprocessing.
        Ritorna un dict con chiavi: output, stderr, error.
        """
        manager = multiprocessing.Manager()
        return_dict = manager.dict()

        proc = multiprocessing.Process(target=self._run_code, args=(code, return_dict))
        proc.start()
        proc.join(self.timeout)

        if proc.is_alive():
            proc.terminate()
            return {
                "output": "",
                "stderr": "",
                "error": "Execution timed out."
            }

        # Se l'errore non Ã¨ stato catturato da exec, assegna stringa vuota
        if 'error' not in return_dict:
            return_dict['error'] = ""

        return dict(return_dict)

# Test esecuzione diretta
if __name__ == "__main__":
    executor = SecureExecutor(timeout=3)
    code_snippet = """
print('Hello from sandbox!')
for i in range(3):
    print(i)
"""
    result = executor.execute(code_snippet)
    print("Output:", result['output'])
    print("Error:", result['error'])

## modules/start_fullmode/initializer.py
"""
Modulo: initializer
Descrizione: Avvio completo del sistema Mercuriusâˆž in modalitÃ  autonoma.
Autore: Mercuriusâˆž AI Engineer
"""

import os
import time
from modules.ai_kernel.agent_core import AgentCore
from modules.voice_bridge.audio_interface import AudioInterface
from modules.stream_vision.video_pipeline import VideoPipeline

class SystemInitializer:
    def __init__(self):
        self.agent = AgentCore()
        self.audio = AudioInterface()
        self.vision = VideoPipeline()

    def initialize_environment(self):
        """Setup iniziale del sistema."""
        print("[INIT] Configurazione ambiente...")
        os.environ['MERCURIUS_MODE'] = 'full'
        time.sleep(1)

    def start_components(self):
        """Avvia tutti i moduli principali."""
        print("[INIT] Avvio moduli principali...")
        self.vision.start()
        self.audio.initialize()
        self.agent.boot()

    def start_fullmode(self):
        """Avvia il sistema in modalitÃ  autonoma completa."""
        self.initialize_environment()
        self.start_components()
        print("[INIT] Mercuriusâˆž avviato in modalitÃ  FULLMODE.")

# Avvio diretto
if __name__ == "__main__":
    system = SystemInitializer()
    system.start_fullmode()

## modules/strategic/__init__.py

## modules/strategic/strategic_brain.py
"""Strategic Brain module integrating gpt_engineer with Mercuriusâˆž.
"""
from pathlib import Path
from typing import List

from modules.goal_manager import GoalManager, Goal
from modules.gpt_engineer_wrapper import GPTEngineerWrapper
from modules.llm.azr_reasoner import AZRAgent
from orchestrator.genesis_orchestrator import GenesisOrchestrator


class StrategicBrain:
    """High level manager that routes goals to LLMs and falls back to GPT-Engineer."""

    def __init__(self, workspace: str = "strategic_projects") -> None:
        self.goal_manager = GoalManager()
        self.orchestrator = GenesisOrchestrator()
        self.validator = AZRAgent()
        self.builder = GPTEngineerWrapper(project_path=workspace)
        Path(workspace).mkdir(exist_ok=True)

    def load_goals(self, goals_path: str) -> None:
        """Load goals from a text file."""
        path = Path(goals_path)
        if not path.exists():
            return
        for line in path.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if line:
                self.goal_manager.add_goal(line)

    def execute_goal(self, goal: Goal) -> str:
        """Process a single goal using the orchestrator and fallback with GPT-Engineer."""
        result = self.orchestrator.route_task(goal.name)
        analysis = self.validator.analyze(result.get("response", ""))
        if isinstance(analysis, str) and analysis.startswith("âŒ"):
            # invalid response: trigger GPT-Engineer
            return self.builder.generate_project(goal.name)
        return result.get("response", "")

    def run(self) -> List[str]:
        """Run through all pending goals and return list of outputs."""
        outputs = []
        while True:
            next_goal = self.goal_manager.get_next_goal()
            if not next_goal:
                break
            output = self.execute_goal(next_goal)
            outputs.append(output)
            self.goal_manager.complete_goal(next_goal.name)
        return outputs

## modules/strategic/strategic_runner.py
"""CLI per eseguire il modulo StrategicBrain."""
import argparse
from pathlib import Path

from .strategic_brain import StrategicBrain


def main() -> None:
    parser = argparse.ArgumentParser(description="Run Strategic Brain goals")
    parser.add_argument("goals", nargs="?", default="goals.txt", help="File dei goal o singolo obiettivo")
    args = parser.parse_args()

    brain = StrategicBrain()

    if Path(args.goals).is_file():
        brain.load_goals(args.goals)
    else:
        brain.goal_manager.add_goal(args.goals)

    results = brain.run()
    for res in results:
        print(res)


if __name__ == "__main__":
    main()

## modules/stream_vision/__init__.py
"""
Package stream_vision
Contiene pipeline di elaborazione video (placeholder minimale).
"""

## modules/stream_vision/video_pipeline.py
"""
Modulo: video_pipeline.py
Descrizione: Gestione realistica della pipeline video con placeholder di fallback.
Autore: Mercuriusâˆž AI Engineer
"""

import cv2
import threading

class VideoPipeline:
    def __init__(self, source=0, use_placeholder=False):
        """
        source: indice webcam o percorso file
        use_placeholder: se True usa il placeholder semplice senza OpenCV
        """
        self.source = source
        self.use_placeholder = use_placeholder
        self.active = False
        self.capture_thread = None

    def _process_frame(self, frame):
        """Elabora il frame video (placeholder per elaborazioni future)."""
        print("[VISION] Frame catturato.")
        return frame

    def _capture_loop(self):
        """Ciclo continuo di cattura video con OpenCV."""
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            print("[VISION] Impossibile aprire la sorgente video.")
            return

        while self.active:
            ret, frame = cap.read()
            if not ret:
                break
            self._process_frame(frame)
        cap.release()
        print("[VISION] Video terminato.")

    def start(self):
        """Avvia la pipeline video (reale o placeholder)."""
        if self.active:
            return
        print(f"[VISION] Avvio pipeline video su '{self.source}' " + 
              ("(placeholder)" if self.use_placeholder else "(reale)"))
        self.active = True
        if self.use_placeholder:
            print(f"ðŸ“¹ VideoPipeline avviata su '{self.source}' (placeholder)")
        else:
            self.capture_thread = threading.Thread(target=self._capture_loop)
            self.capture_thread.start()

    def stop(self):
        """Arresta la pipeline video."""
        if not self.active:
            return
        self.active = False
        if self.capture_thread:
            self.capture_thread.join()
        print("ðŸ›‘ Pipeline video fermata")

# Esempio di esecuzione diretta
if __name__ == "__main__":
    vp = VideoPipeline(source=0, use_placeholder=False)
    vp.start()
    import time
    time.sleep(5)
    vp.stop()

## modules/stream_voice/__init__.py

## modules/vision_audio/__init__.py

## modules/vision_audio/note10_jarvis_bridge.py
"""note10_jarvis_bridge.py
Modulo: note10_jarvis_bridge
Descrizione: trasforma un Note10+ in assistente vocale continuo in stile Jarvis.
"""

from __future__ import annotations

import logging
import queue
import threading
import time
from typing import Callable, List, Optional

try:
    import sounddevice as sd
except Exception:  # pragma: no cover - sounddevice may not be available
    sd = None

try:
    import whisper
except Exception:  # pragma: no cover - whisper may not be installed
    whisper = None

try:
    import vosk
except Exception:  # pragma: no cover - vosk may not be installed
    vosk = None

try:
    import requests
except Exception:  # pragma: no cover
    requests = None

HOTWORDS = [
    "tu che ne pensi aion",
    "analizzami questo aion",
    "tu che dici aion",
    "vero aion",
    "giusto aion?",
]

logger = logging.getLogger(__name__)


class VoiceListener:
    """Microfono sempre attivo con hotword detection."""

    def __init__(self, hotwords: Optional[List[str]] = None, model: str = "base"):
        self.hotwords = [h.lower() for h in (hotwords or HOTWORDS)]
        self.model_name = model
        self._queue: queue.Queue[bytes] = queue.Queue()
        self._stop = threading.Event()
        self._callback: Optional[Callable[[str], None]] = None
        self.use_whisper = False
        self.use_vosk = False
        self._init_models()

    def _init_models(self) -> None:
        if whisper:
            try:
                self.whisper_model = whisper.load_model(self.model_name)
                self.use_whisper = True
                logger.info("Whisper model ready")
            except Exception as exc:  # pragma: no cover
                logger.warning("Whisper load failed: %s", exc)
        if not self.use_whisper and vosk:
            try:
                self.vosk_model = vosk.Model("model")
                self.use_vosk = True
                logger.info("Vosk model ready")
            except Exception as exc:  # pragma: no cover
                logger.warning("Vosk load failed: %s", exc)

    def start(self, on_trigger: Callable[[str], None]) -> None:
        self._callback = on_trigger
        threading.Thread(target=self._listen_loop, daemon=True).start()

    def stop(self) -> None:
        self._stop.set()

    def _audio_cb(self, indata, frames, time_info, status) -> None:
        self._queue.put(bytes(indata))

    def _listen_loop(self) -> None:
        if not sd:
            logger.error("sounddevice non disponibile")
            return
        with sd.InputStream(channels=1, samplerate=16000, callback=self._audio_cb):
            while not self._stop.is_set():
                time.sleep(0.1)
                if not self._queue.empty():
                    data = b"".join([self._queue.get() for _ in range(self._queue.qsize())])
                    text = self._transcribe(data)
                    if text:
                        lowered = text.lower().strip()
                        if any(h in lowered for h in self.hotwords):
                            logger.info("Hotword detected: %s", lowered)
                            if self._callback:
                                self._callback(lowered)

[TRONCATO]

## modules/voice_bridge/activation_hook.py
from interface.genesis_bridge import GenesisBridge

class VoiceActivation:
    def __init__(self):
        self.bridge = GenesisBridge()

    def process_input(self, speech: str) -> str:
        if self.bridge.activate_from_voice(speech):
            self.bridge.trigger_activation("voce")
            return "GENESIS attivato!"
        return "Comando vocale ignorato."

## modules/voice_bridge/audio_interface.py
"""
Modulo: audio_interface
Descrizione: Interfaccia vocale per input STT e output TTS nel sistema Mercuriusâˆž.
Autore: Mercuriusâˆž AI Engineer
"""

import os

class AudioInterface:
    def __init__(self):
        self.microphone_ready = False
        self.tts_ready = False

    def initialize(self):
        """Inizializza le risorse audio."""
        print("[AUDIO] Inizializzazione microfono e TTS...")
        self.microphone_ready = True
        self.tts_ready = True

    def listen(self):
        """Simula acquisizione audio (STT)."""
        if not self.microphone_ready:
            return "[AUDIO] Microfono non inizializzato."
        print("[AUDIO] Ascolto... (placeholder)")
        return "comando vocale simulato"

    def speak(self, text):
        """Simula output vocale (TTS)."""
        if not self.tts_ready:
            return "[AUDIO] TTS non inizializzato."
        print(f"[AUDIO] Parla: {text}")

# Esecuzione diretta
if __name__ == "__main__":
    audio = AudioInterface()
    audio.initialize()
    command = audio.listen()
    audio.speak(f"Hai detto: {command}")

## modules/voice_bridge/dia_model_mock.py
"""
Mock per dia.model se il pacchetto non Ã¨ installabile localmente.
"""

class Dia:
    @staticmethod
    def from_pretrained(model_name: str):
        class DummyModel:
            def generate(self, text):
                import numpy as np
                return np.zeros(44100)
        return DummyModel()

## modules/voice_bridge/multimodal_controller.py
"""
Modulo: multimodal_controller
Descrizione: Gestione input/output multimodale vocale per Mercuriusâˆž.
Autore: Mercuriusâˆž AI Engineer
"""

import time
from modules.voice_bridge.speech_to_text import WhisperSTT

# TTS avanzato (Nari Dia) + fallback
try:
    from modules.voice_bridge.nari_dia_tts import NariDiaTTS
    TTS_ENGINE = "nari"
except ImportError:
    from modules.voice_bridge.text_to_speech import TextToSpeech
    TTS_ENGINE = "pyttsx3"

class MultimodalController:
    def __init__(self):
        self.stt = WhisperSTT()
        if TTS_ENGINE == "nari":
            self.tts = NariDiaTTS()
        else:
            self.tts = TextToSpeech()

    def listen_and_respond(self, audio_file_path: str, ai_callback):
        """
        Ascolta un file audio, lo trascrive, passa il testo all'AI,
        e vocalizza la risposta.
        """
        print("ðŸŽ§ Ricezione vocale in corso...")
        input_text = self.stt.transcribe(audio_file_path)
        print("ðŸ—£ Input:", input_text)

        response = ai_callback(input_text)
        print("ðŸ§  Risposta AI:", response)

        time.sleep(0.5)  # Ottimizzazione dialogo
        self.tts.speak(response)
        return response

# Esecuzione di prova
if __name__ == "__main__":
    def mock_ai(text):
        return f"Hai detto: {text}"

    mmc = MultimodalController()
    mmc.listen_and_respond("sample_audio.wav", mock_ai)

## modules/voice_bridge/nari_dia_tts.py
import soundfile as sf
from modules.voice_bridge.dia_model_mock import Dia


class NariDiaTTS:
    def __init__(self, model_name="nari-labs/Dia-1.6B"):
        self.model = Dia.from_pretrained(model_name)

    def speak(self, text: str, output_path="output.wav"):
        """
        Genera audio da testo utilizzando Nari Dia.
        """
        output = self.model.generate(text)
        sf.write(output_path, output, 44100)
        # Riproduzione audio (opzionale)
        # playsound(output_path)

## modules/voice_bridge/pyttsx3_tts.py
# modules/voice_bridge/pyttsx3_tts.py
"""
Modulo: pyttsx3_tts.py
Descrizione: Sintesi vocale offline compatibile con qualsiasi Python (usando pyttsx3).
"""

import pyttsx3

class Pyttsx3TTS:
    def __init__(self, voice_id=None):
        self.engine = pyttsx3.init()
        if voice_id:
            self.engine.setProperty('voice', voice_id)

    def speak(self, text: str):
        self.engine.say(text)
        self.engine.runAndWait()

## modules/voice_bridge/speech_to_text.py
"""
Modulo: speech_to_text
Descrizione: Interfaccia vocale STT usando Whisper per Mercuriusâˆž.
Autore: Mercuriusâˆž AI Engineer
"""

import whisper

class WhisperSTT:
    def __init__(self, model_name="base"):
        self.model = whisper.load_model(model_name)

    def transcribe(self, audio_path: str) -> str:
        """Esegue la trascrizione da un file audio."""
        try:
            result = self.model.transcribe(audio_path, language='it')
            return result['text']
        except Exception as e:
            return f"[STT Error] {e}"

# Test
if __name__ == "__main__":
    stt = WhisperSTT()
    testo = stt.transcribe("sample_audio.wav")
    print("Trascrizione:", testo)

## modules/voice_bridge/text_to_speech.py
"""Simple gTTS based TTS engine."""
from gtts import gTTS
import os
import tempfile

class TextToSpeech:
    def __init__(self, lang: str = "it"):
        self.lang = lang

    def speak(self, text: str):
        tts = gTTS(text=text, lang=self.lang)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as f:
            tts.save(f.name)
            os.system(f"mpg123 -q {f.name}" if os.name != "nt" else f"start {f.name}")

## modules/voice_bridge/tts_engine.py
# modules/voice_bridge/tts_engine.py
"""
Modulo: tts_engine.py
Descrizione: Motore TTS basato su pyttsx3 per la sintesi vocale offline.
"""
from modules.speech import TextToSpeech

class Pyttsx3TTS(TextToSpeech):
    """
    Wrapper per il motore di sintesi vocale pyttsx3 (alias di TextToSpeech).
    """
    def __init__(self, voice_id=None):
        super().__init__(voice_id=voice_id)

## modules/voice_bridge/voice_loop.py
"""Interactive voice loop using Whisper STT and gTTS."""
import os
from modules.voice_bridge.speech_to_text import WhisperSTT
from modules.voice_bridge.text_to_speech import TextToSpeech


def start_listening():
    stt = WhisperSTT()
    tts = TextToSpeech()
    print("[VOICE] Say 'exit' to stop. Provide path to .wav file for recognition.")
    while True:
        path = input("Audio file> ")
        if path.strip().lower() == "exit":
            break
        if not os.path.exists(path):
            print("File not found")
            continue
        text = stt.transcribe(path)
        print("[STT]", text)
        tts.speak(text)

## modules/voice_bridge/whisper_interface.py
"""
Modulo: whisper_interface
Descrizione: Interfaccia locale per trascrizione vocale usando Whisper (stub).
Autore: Mercuriusâˆž AI Engineer
"""

class WhisperSTT:
    def __init__(self):
        self.language = "it"

    def transcribe(self, audio_path: str) -> str:
        """
        Simula la trascrizione vocale di un file audio.
        In una versione reale, chiamerebbe whisper transcribe(audio_path).
        """
        print(f"[WHISPER] Trascrizione simulata del file: {audio_path}")
        return "Questo Ã¨ un esempio di trascrizione da audio."

# Esempio
if __name__ == "__main__":
    stt = WhisperSTT()
    testo = stt.transcribe("demo.wav")
    print(f"Risultato: {testo}")

## monitoring/__init__.py

## monitoring/health_check.py
# monitoring/health_check.py

"""