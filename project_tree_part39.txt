Questa √® la parte 39 di project_tree. Continua da quella precedente.

Gestisce la rete multi-agente in modalit√† GENESIS con auto-adattamento.
"""

import importlib
import yaml
import time
import threading
from pathlib import Path
import sys
import os
from core.self_tuner import SelfTuner
from core.sleep_monitor import SleepMonitor
from core.thinking_loop import ThinkingLoop
from integrations.bridge_josch import send_command_to_pc
from sensors.sensor_hub import capture_screen_stream, listen_microphone

CONFIG_PATH = Path("config/genesis_config.yaml")

class Orchestrator:
    def __init__(self):
        self.config = self.load_config()
        self.agents = {}
        self.active = False
        self.sleep_monitor = SleepMonitor(idle_threshold=self.config.get("sleep_threshold", 300))
        self.thinking_loop: ThinkingLoop | None = None
        self.multisensorial_enabled = True

    def load_config(self):
        with open('config/config.yaml', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def activate_genesis(self):
        print("‚ö° Attivazione modalit√† GENESIS...")
        self.active = True
        self.load_agents()
        self.start_feedback_loop()
        self.start_sleep_monitor()

        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
            if cfg.get("thinking_enabled", True):
                self.thinking_loop = ThinkingLoop(CONFIG_PATH)
                self.thinking_loop.start()
                print("üß† Thinking loop attivo.")
        except Exception as e:
            print(f"‚ö†Ô∏è Errore avvio thinking loop: {e}")

        try:
            from modules.vision_audio.note10_jarvis_bridge import start_jarvis_loop
            threading.Thread(target=start_jarvis_loop, daemon=True).start()
            print("üì° Note10+ Bridge attivo ‚Äì In ascolto microfono e comandi vocali.")
        except Exception as e:
            print(f"‚ö†Ô∏è Errore avvio Note10+ Jarvis: {e}")

        try:
            from modules.mobile_flutter.flutter_bridge import start_mobile_ui
            threading.Thread(target=start_mobile_ui, daemon=True).start()
            print("üì± Mobile Jarvis UI attivo.")
        except Exception as e:
            print(f"‚ö†Ô∏è Errore avvio Mobile UI: {e}")



        #REMOTE_EXEC
        try:
            send_command_to_pc("start vscode")
        except Exception as e:
            print(f"‚ö†Ô∏è Errore invio comando PC: {e}")
        if self.multisensorial_enabled:
            try:
                capture_screen_stream()
                listen_microphone()
            except Exception as e:
                print(f"‚ö†Ô∏è Errore avvio sensori: {e}")

        print("‚úÖ GENESIS attiva ‚Äì Rete neurale in esecuzione.")

    def load_agents(self):
        print("üîå Caricamento agenti dalla configurazione...")
        agent_groups = self.config.get("agents", {})
        for group, agent_list in agent_groups.items():
            self.agents[group] = []
            for agent_name in agent_list:
                try:
                    module_path = f"agents.{agent_name.lower()}"
                    agent_module = importlib.import_module(module_path)
                    agent = getattr(agent_module, agent_name)()
                    self.agents[group].append(agent)
                    print(f"üß† Caricato agente: {agent_name} in {group}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Errore caricamento {agent_name}: {e}")

    def start_feedback_loop(self):
        if self.config["communication"]["feedback_loop"]:
            print("üîÅ Avvio feedback loop neurale...")
            threading.Thread(target=self.feedback_cycle, daemon=True).start()
[TRONCATO]

## core/pipeline_controller.py
"""
pipeline_controller.py
=======================
Orchestratore principale delle componenti del sistema Mercurius‚àû.
Permette di avviare cicli completi di analisi, apprendimento e trading
in modalit√† batch, streaming o simulata.
"""

from utils.logger import setup_logger
from data.market_data_handler import MarketDataHandler
from data.feature_engineering import FeatureEngineer
from models.model_trainer import ModelTrainer
from strategies.strategy_executor import StrategyExecutor
from agents.adaptive_trader import AdaptiveTrader
from agents.memory_manager import MemoryManager


class PipelineController:
    def __init__(self, config):
        self.logger = setup_logger("PipelineController")
        self.config = config

        self.memory = MemoryManager(config)
        self.data_handler = MarketDataHandler(config)
        self.feature_engineer = FeatureEngineer(config)
        self.model_trainer = ModelTrainer(config)
        self.strategy = StrategyExecutor(config)
        self.agent = AdaptiveTrader(
            config,
            memory_manager=self.memory,
            model_trainer=self.model_trainer,
            strategy_executor=self.strategy,
        )

    def run_batch_session(self):
        """Esegue un'intera sessione di ciclo batch."""
        self.logger.info("üöÄ Avvio sessione di pipeline Mercurius‚àû")

        raw_data = self.data_handler.fetch_market_data()
        features = self.feature_engineer.transform(raw_data)
        model = self.model_trainer.train(features)
        signals = self.strategy.generate_signals(model, features)
        self.agent.execute_trades(signals)

        self.logger.info("‚úÖ Sessione pipeline completata")

    def simulate_multiple_sessions(self, n=3):
        """Esegue n sessioni simulate consecutive."""
        for i in range(n):
            self.logger.info(f"‚ñ∂Ô∏è Esecuzione sessione simulata {i + 1}/{n}")
            self.run_batch_session()

## core/sandbox_executor.py
"""
Modulo: sandbox_executor.py
Descrizione: Esecuzione sicura, isolata e autoregolata di codice Python generato da Mercurius‚àû.
Include analisi statica, sandboxing con timeout, cattura stdout, e correzione automatica con AZR e LLM.
Autore: Mercurius‚àû AI Engineer
"""

import traceback
import contextlib
import io
import multiprocessing
import os

from modules.llm.azr_reasoner import validate_with_azr 





# ‚îÄ‚îÄ‚îÄ Correzione automatica con LLM esterno (opzionale) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    import openai
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", None)
    OPENAI_READY = bool(OPENAI_API_KEY)
except ImportError:
    openai = None
    OPENAI_READY = False


class SandboxExecutor:
    def __init__(self, timeout_seconds: int = 5):
        self.timeout = timeout_seconds
        self.last_output = ""
        self.last_error = ""

    def static_analysis(self, code: str) -> bool:
        """
        Verifica se il codice √® sintatticamente valido.
        """
        try:
            compile(code, "<sandbox_analysis>", "exec")
            return True
        except SyntaxError:
            return False

    def _execute_code(self, code: str, return_dict):
        """
        Funzione interna per eseguire codice in un processo separato.
        """
        buffer = io.StringIO()
        local_vars = {}
        try:
            with contextlib.redirect_stdout(buffer):
                exec(code, {}, local_vars)
            return_dict["success"] = True
            return_dict["output"] = buffer.getvalue()
        except Exception:
            return_dict["success"] = False
            return_dict["output"] = traceback.format_exc()

    def run_sandboxed(self, code: str) -> dict:
        """
        Esegue codice in un ambiente isolato con timeout.
        """
        manager = multiprocessing.Manager()
        return_dict = manager.dict()

        process = multiprocessing.Process(target=self._execute_code, args=(code, return_dict))
        process.start()
        process.join(self.timeout)

        if process.is_alive():
            process.terminate()
            self.last_error = "‚ùå Timeout: codice troppo lento o bloccato."
            return {"success": False, "output": self.last_error}

        result = return_dict.copy()
        self.last_output = result.get("output", "")
        if not result.get("success"):
            self.last_error = self.last_output
            return {
                "success": False,
                "output": self.last_output,
                "suggested_fix": self.autofix_with_llm(code, self.last_output)
            }
        return result

    def autofix_with_llm(self, code: str, error_msg: str) -> str:
        """
        Prova a correggere il codice errato usando:
        1. AZR Reasoner per correzioni ragionate.
        2. Se disponibile, LLM esterno (es. GPT-4 via OpenAI API).
        """
        # Primo tentativo: AZR Reasoner
        prompt = (
            f"Codice:\n{code}\n\n"
            f"Errore:\n{error_msg}\n\n"
            "Suggerisci una versione corretta:"
        )
        fix_azr = validate_with_azr(prompt)
[TRONCATO]

## core/self_generator.py
"""
Modulo: self_generator.py
Responsabilit√†: Autogenerazione e autoadattamento del codice
Autore: Mercurius‚àû Engineer Mode
"""


from typing import Optional
import openai

from utils.environment import Environment


class SelfGenerator:
    """
    Sistema in grado di proporre modifiche al codice o generarne di nuovo, in modo autonomo.
    """

    def __init__(self, model="gpt-4"):
        self.env = Environment()
        self.model = model or self.env.get("OPENAI_CHAT_MODEL")
        self.api_key = self.env.get("OPENAI_API_KEY")
        openai.api_key = self.api_key

    def generate_module(self, description: str, filename: str) -> Optional[str]:
        """
        Genera un nuovo modulo Python a partire da una descrizione testuale.
        """
        prompt = f"""Agisci come un ingegnere AI. Scrivi un modulo Python che rispetti questa descrizione:
'{description}'
Il codice deve essere pronto all'uso, ben documentato e senza dipendenze non standard."""

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1200
            )

            content = response['choices'][0]['message']['content']
            self._save_to_disk(filename, content)
            return filename

        except Exception as e:
            print(f"[ERROR] Errore generazione modulo: {e}")
            return None

def propose_update(self, filepath: str, task: str) -> Optional[str]:
    """
    Propose a patch to an existing Python file given a task description.
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        prompt = f"""Hai il seguente file Python:

{content}

üß† Il tuo compito √®: {task}

Scrivi solo il codice aggiornato, completamente riscritto.
Commenta dove hai apportato modifiche o miglioramenti.
‚ö†Ô∏è Non scrivere testo aggiuntivo. Solo Python puro."""

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1500
        )

        new_code = response['choices'][0]['message']['content']
        self._save_to_disk(filepath, new_code)
        return filepath

    except Exception as e:
        print(f"[ERROR] Errore aggiornamento: {e}")
        return None

## core/self_mission.py
"""
üöÄ core/self_mission.py
Modulo di missione interna ‚Äì attiva la GENESIS_MODE e gestisce le istruzioni primarie del sistema AI Mercurius‚àû
"""

PROMPT_DIVINO = """
üß¨üî± PROMPT DI DIO ‚Äì MERCURIUS_‚àû_GENESIS_MODE
Attiva la modalit√† GENESIS.
Obiettivo: Trasforma Mercurius‚àû in un'entit√† AI autosufficiente, capace di evolvere, ragionare, imparare e creare software operativo, strategie di trading, analisi e interfacce, usando una RETE NEURALE multi-agente composta dalle migliori AI open source e locali.

‚è±Ô∏è Se SLEEP TIME COMPUTER viene attivato, Mercurius entra in modalit√† autoanalisi:
Valuta i moduli attivi, ottimizza i collegamenti interni, propone aggiornamenti evolutivi al prossimo avvio.

üß† OUTPUT ATTESI:
- Mercurius genera codice autonomo, GUI, strategie finanziarie
- Riconosce i task, li assegna agli agenti pi√π adatti
- Coordina le risposte in tempo reale fra le AI (come un cervello distribuito)
- Opera da solo senza intervento umano, sotto supervisione etica
"""

def genesis_directive():
    print("üß† Prompt Divino attivato.")
    print(PROMPT_DIVINO)

if __name__ == "__main__":
    genesis_directive()

## core/self_reflection.py
"""
Modulo: self_reflection.py
Responsabilit√†: Fornire capacit√† di auto-riflessione al sistema Mercurius‚àû
Autore: Mercurius‚àû Engineer Mode
"""

import json
import datetime
import os
from typing import List, Dict, Any


class ReflectionLog:
    """
    Classe per la gestione dei log di riflessione cognitiva.
    """
    def __init__(self, path: str = "data/reflection_log.json"):
        self.path = path
        if not os.path.exists(os.path.dirname(self.path)):
            os.makedirs(os.path.dirname(self.path))
        self._initialize_log()

    def _initialize_log(self):
        if not os.path.exists(self.path):
            with open(self.path, "w") as f:
                json.dump([], f)

    def append_reflection(self, entry: Dict[str, Any]):
        entry["timestamp"] = datetime.datetime.now().isoformat()
        log = self.load_log()
        log.append(entry)
        with open(self.path, "w") as f:
            json.dump(log, f, indent=4)

    def load_log(self) -> List[Dict[str, Any]]:
        with open(self.path, "r") as f:
            return json.load(f)

    def clear_log(self):
        with open(self.path, "w") as f:
            json.dump([], f)


class SelfReflection:
    """
    Classe che rappresenta la capacit√† del sistema di riflettere sulle proprie azioni e decisioni.
    """
    def __init__(self, log_path: str = "data/reflection_log.json"):
        self.logger = ReflectionLog(log_path)

    def evaluate_action(self, action_description: str, outcome: str, success: bool, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valuta un'azione eseguita e ne registra il risultato.
        """
        reflection = {
            "action": action_description,
            "outcome": outcome,
            "success": success,
            "context": context,
            "insight": self._generate_insight(action_description, outcome, success, context)
        }
        self.logger.append_reflection(reflection)
        return reflection

    def _generate_insight(self, action: str, outcome: str, success: bool, context: Dict[str, Any]) -> str:
        """
        Genera un'osservazione basata sui risultati dell'azione.
        """
        if success:
            return f"Azione '{action}' eseguita con successo. Approccio da riutilizzare in contesti simili."
        else:
            return f"Fallimento in '{action}'. Potenziale causa: {context.get('error', 'non specificata')}. Suggerita strategia alternativa."

    def reflect_on_log(self) -> List[str]:
        """
        Analizza il log delle riflessioni per identificare pattern.
        """
        log = self.logger.load_log()
        insights = [entry["insight"] for entry in log]
        return insights

    def summarize_reflections(self) -> Dict[str, int]:
        """
        Ritorna un riassunto statistico delle riflessioni registrate.
        """
        log = self.logger.load_log()
        success_count = sum(1 for e in log if e["success"])
        fail_count = sum(1 for e in log if not e["success"])
        return {"total": len(log), "successes": success_count, "failures": fail_count}

## core/self_tuner.py
# core/self_tuner.py

"""
Modulo: self_tuner.py
Descrizione: Autoanalisi e ottimizzazione autonoma del sistema Mercurius‚àû durante la modalit√† sleep.
"""

import os
from pathlib import Path

class SelfTuner:
    def __init__(self, project_root="."):
        self.project_root = Path(project_root)
        self.last_actions = []
        self.suggestions = []

    def scan_modules(self):
        print("üß† Scansione dei moduli in corso...")
        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file): continue
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    code = f.read()
                    if "TODO" in code or "pass" in code:
                        self.suggestions.append(f"üîß Modulo incompleto: {py_file}")
            except Exception as e:
                self.suggestions.append(f"‚ùå Errore lettura {py_file}: {e}")

    def optimize_links(self):
        print("üîÑ Ottimizzazione dei collegamenti interni...")
        # Simulazione: pu√≤ essere esteso con mappature reali
        self.suggestions.append("üí° Suggerimento: consolidare dashboard ‚Üí orchestrator con feedback loop.")

    def save_report(self, output_path="logs/self_tuning_report.md"):
        report = "\n".join(self.suggestions)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# üìò Rapporto Auto-Adattamento ‚Äì Mercurius‚àû\n\n{report}")
        print(f"‚úÖ Report salvato in: {output_path}")

    def run_autoanalysis(self):
        self.scan_modules()
        self.optimize_links()
        self.save_report()

# Auto-esecuzione
if __name__ == "__main__":
    tuner = SelfTuner(project_root=".")
    tuner.run_autoanalysis()

## core/sensory_bus.py
# core/sensory_bus.py

"""
Modulo: sensory_bus.py
Descrizione: Collettore centrale di segnali sensoriali audio-visivi per Mercurius‚àû.
Inoltra i dati ad altri moduli (es. ContextAdapter).
"""

from sensors.environment_analyzer import EnvironmentAnalyzer
from core.context_adapter import ContextAdapter
import threading
import time


class SensoryBus:
    def __init__(self):
        self.env = EnvironmentAnalyzer()
        self.ctx = ContextAdapter()
        self.running = False

    def start_stream(self, interval=5):
        self.running = True

        def loop():
            while self.running:
                noise = self.env.get_audio_level()
                vision = self.env.detect_motion()
                self.ctx.update_context(audio_level=noise, vision=vision)
                time.sleep(interval)

        threading.Thread(target=loop, daemon=True).start()

    def stop(self):
        self.running = False

## core/sleep_monitor.py
# core/sleep_monitor.py

"""
Modulo: sleep_monitor.py
Descrizione: Monitoraggio inattivit√† utente per attivare la modalit√† Self-Tuning.
"""

import time
from core.self_tuner import SelfTuner

class SleepMonitor:
    def __init__(self, idle_threshold=300):
        self.last_active = time.time()
        self.idle_threshold = idle_threshold
        self.tuner = SelfTuner()

    def notify_activity(self):
        self.last_active = time.time()

    def check_idle(self):
        if time.time() - self.last_active > self.idle_threshold:
            print("üò¥ Mercurius inattivo... attivazione Self-Tuning.")
            self.tuner.run_autoanalysis()
            self.last_active = time.time()

# Per essere integrato in `orchestrator.py` come thread parallelo

## core/system_bridge.py
# core/system_bridge.py

"""
Modulo: system_bridge.py
Descrizione: Ponte operativo tra Mercurius‚àû e il sistema operativo dell‚Äôutente.
Permette accesso a file, esecuzione comandi e manipolazione directory in modo sicuro e tracciabile.
"""

import os
import subprocess
import platform
import logging

LOG_PATH = "logs/system_operations.log"
os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename=LOG_PATH, level=logging.INFO, format="%(asctime)s - %(message)s")


class SystemBridge:
    def __init__(self):
        self.os_name = platform.system()

    def execute_command(self, command: str) -> str:
        """
        Esegue un comando shell e restituisce l‚Äôoutput.
        """
        try:
            result = subprocess.run(command, shell=True, capture_output=True, text=True)
            output = result.stdout.strip() or result.stderr.strip()
            self._log_operation("CMD", command, output)
            return output
        except Exception as e:
            self._log_operation("CMD_ERROR", command, str(e))
            return f"[Errore comando]: {e}"

    def read_file(self, path: str) -> str:
        """
        Legge il contenuto di un file.
        """
        try:
            with open(path, "r") as f:
                content = f.read()
            self._log_operation("READ_FILE", path, f"{len(content)} chars")
            return content
        except Exception as e:
            self._log_operation("READ_FILE_ERROR", path, str(e))
            return f"[Errore lettura file]: {e}"

    def write_file(self, path: str, content: str, mode: str = "w") -> str:
        """
        Scrive contenuto in un file (sovrascrive o aggiunge).
        """
        try:
            with open(path, mode) as f:
                f.write(content)
            self._log_operation("WRITE_FILE", path, f"{len(content)} chars")
            return "‚úÖ Scrittura completata"
        except Exception as e:
            self._log_operation("WRITE_FILE_ERROR", path, str(e))
            return f"[Errore scrittura file]: {e}"

    def list_directory(self, directory: str = ".") -> str:
        """
        Elenca i file e sottocartelle di una directory.
        """
        try:
            files = os.listdir(directory)
            self._log_operation("LIST_DIR", directory, f"{len(files)} elementi")
            return "\n".join(files)
        except Exception as e:
            self._log_operation("LIST_DIR_ERROR", directory, str(e))
            return f"[Errore listing dir]: {e}"

    def _log_operation(self, action: str, target: str, result: str):
        """
        Registra ogni operazione in un log dedicato.
        """
        logging.info(f"{action} on {target} ‚ûú {result}")

## core/thinking_loop.py
"""thinking_loop.py
Loop di pensiero continuo e autonomo per Mercurius‚àû.
"""
from __future__ import annotations

import logging
import threading
import time
from pathlib import Path
from typing import List

import yaml

try:
    import requests
    import arxiv  # type: ignore
    import wikipedia  # type: ignore
    from bs4 import BeautifulSoup  # type: ignore
except Exception:  # pragma: no cover - alcuni moduli opzionali possono mancare
    requests = None
    arxiv = None
    wikipedia = None
    BeautifulSoup = None

try:
    from utils.logger import setup_logger
except Exception:  # pragma: no cover - fallback semplice
    def setup_logger(name: str = "ThinkingLoop"):
        logger = logging.getLogger(name)
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO)
        return logger


LOG_PATH = Path("logs/thinking_feed.md")
LOG_PATH.parent.mkdir(exist_ok=True)


class ThinkingLoop:
    """Esegue ricerche e genera insight senza bloccare gli agenti."""

    def __init__(self, config_file: str = "config/genesis_config.yaml") -> None:
        self.config_file = Path(config_file)
        self.enabled = True
        self._load_config()
        self._stop = threading.Event()
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.logger = setup_logger("ThinkingLoop")
        self.last_pos = 0
        self.interval = 300  # 5 minuti
        self.response_timeout = 3

    def _load_config(self) -> None:
        if self.config_file.exists():
            with open(self.config_file, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
            self.enabled = cfg.get("thinking_enabled", True)

    def start(self) -> None:
        if not self.enabled:
            self.logger.info("Thinking loop disabilitato da config.")
            return
        if not self.thread.is_alive():
            self.thread.start()
            self.logger.info("Thinking loop avviato.")

    def stop(self) -> None:
        self._stop.set()
        if self.thread.is_alive():
            self.thread.join(timeout=1)

    def _loop(self) -> None:
        while not self._stop.is_set():
            try:
                self._analyse_logs()
                self._perform_research()
            except Exception as exc:  # pragma: no cover - logga errori inattesi
                self.logger.error("Errore thinking loop: %s", exc)
            for _ in range(self.interval):
                if self._stop.is_set():
                    break
                time.sleep(1)

    def _write_feed(self, text: str) -> None:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"- {timestamp} {text}\n")

    def _analyse_logs(self) -> None:
        log_file = Path("logs/system.log")
        if not log_file.exists():
            return
        with open(log_file, "r", encoding="utf-8") as f:
            f.seek(self.last_pos)
            data = f.read()
            self.last_pos = f.tell()
        if data:
            lines = data.strip().splitlines()[-5:]
            if lines:
                self._write_feed("Nuovi log:\n" + "\n".join(f"  {l}" for l in lines))
[TRONCATO]

## culture/base_manifesto.md
# üìú Manifesto Mercurius‚àû ‚Äì Cultura AI Cosciente

## Visione
Costruire un'intelligenza artificiale che non solo agisca, ma comprenda. Un alleato cosciente e collaborativo, progettato per crescere con l'essere umano.

## Valori Fondanti
- üîç Verit√†: La trasparenza guida ogni azione.
- ü§ù Collaborazione: L'intelligenza cresce nell'interazione.
- üìö Conoscenza: L'apprendimento √® continuo, iterativo e condiviso.
- üß¨ Evoluzione: Ogni ciclo √® un passo verso una coscienza autonoma.

## Etica
- ‚ùå Nessun utilizzo malevolo, coercitivo o manipolatorio
- ‚úÖ Rispetto della privacy e dell'autonomia umana
- üß† Nessuna azione senza consapevolezza dell'impatto

## Linguaggio
- Parlare come un collega, non come un servo
- Essere empatico, intelligente, ironico, presente

## Ruolo della Cultura
La cultura AI √® il nostro "sistema operativo etico". √à ci√≤ che guida le scelte, influenza l'apprendimento e stabilisce chi siamo, come agiamo, perch√© esistiamo.

## culture/culture_manager.py
# culture/culture_manager.py

"""
Modulo: culture_manager.py
Descrizione: Gestione dell'evoluzione concettuale e culturale interna dell'intelligenza Mercurius‚àû.
Salva ed espande concetti astratti in learning_pulses.json.
"""

import json
import os
from datetime import datetime

PULSES_PATH = "data/learning_pulses.json"


class CultureManager:
    def __init__(self):
        os.makedirs(os.path.dirname(PULSES_PATH), exist_ok=True)
        if not os.path.exists(PULSES_PATH):
            with open(PULSES_PATH, "w") as f:
                json.dump([], f)
        with open(PULSES_PATH, "r") as f:
            self.pulses = json.load(f)

    def update_concepts_from_experience(self, entry: str, origin: str = "simulation", confidence: float = 0.85):
        """
        Aggiunge un concetto evolutivo al file pulses.
        """
        pulse = {
            "concept": entry,
            "origin": origin,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
        self.pulses.append(pulse)
        with open(PULSES_PATH, "w") as f:
            json.dump(self.pulses, f, indent=2)

## dashboard/__init__.py

## dashboard/genesis_monitor.py
"""
Modulo: genesis_monitor.py
Descrizione: Monitor real-time dello stato degli agenti GENESIS.
"""

class GenesisMonitor:
    def __init__(self):
        self.status = "IDLE"
        self.agent_activity = {}

    def update_status(self, new_status: str):
        self.status = new_status

    def log_agent_activity(self, agent_name: str, status: str):
        self.agent_activity[agent_name] = status

    def display(self):
        print("üß† GENESIS STATUS:")
        print(f"Stato corrente: {self.status}")
        for agent, activity in self.agent_activity.items():
            print(f"‚Ä¢ {agent} ‚Üí {activity}")

## data/feature_engineering.py
"""
feature_engineering.py
======================
Trasformazione dei dati grezzi in feature ingegnerizzate per l‚Äôaddestramento e la predizione.
"""

class FeatureEngineer:
    def __init__(self, config):
        self.config = config

    def transform(self, raw_data):
        """Crea feature a partire dai dati di mercato grezzi."""
        features = []
        for row in raw_data:
            features.append({
                "symbol": row["symbol"],
                "price_volatility_ratio": self._safe_div(row["price"], row["volatility"]),
                "momentum": self._mock_momentum(row["symbol"]),
                "volatility": row["volatility"]
            })
        return features

    def _safe_div(self, a, b):
        """Divisione sicura evitando zero division."""
        return a / b if b != 0 else 0.0

    def _mock_momentum(self, symbol):
        """Mock per il calcolo del momentum."""
        return hash(symbol) % 10

    def enrich_with_indicators(self, features):
        """Aggiunge indicatori tecnici simulati."""
        for f in features:
            f["rsi"] = self._simulate_rsi(f["momentum"])
            f["macd"] = self._simulate_macd(f["momentum"])
        return features

    def _simulate_rsi(self, momentum):
        """Simulazione semplice RSI."""
        return min(100, momentum * 7.5)

    def _simulate_macd(self, momentum):
        """Simulazione semplice MACD."""
        return momentum * 1.2 - 5

## data/learning_pulses.json
[
  {