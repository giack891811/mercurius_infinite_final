Questa è la parte 53 di project_tree. Continua da quella precedente.

        """
        Placeholder per estensione con modello YOLO/Vision per rilevamento oggetti.
        """
        return ["[analisi visiva non implementata]"]

## vision/ip_webcam_vision.py
"""YOLO based detection from IP webcam stream."""
import cv2
from vision.object_vision import ObjectVision

class IPWebcamVision(ObjectVision):
    def start_stream(self, ip_url: str):
        cap = cv2.VideoCapture(ip_url)
        if not cap.isOpened():
            raise RuntimeError("Cannot open IP camera")
        print("📡 Streaming IP webcam... press 'q' to quit")
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            results = self.model(frame, verbose=False)[0]
            annotated = self.box_annotator.annotate(
                scene=frame,
                detections=results.boxes
            )
            cv2.imshow("Mercurius∞ IP Cam", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

## vision/object_vision.py
# vision/object_vision.py

"""
Modulo: object_vision.py
Descrizione: Riconoscimento oggetti in tempo reale da webcam con YOLOv8.
"""

import cv2
import supervision as sv
from ultralytics import YOLO


class ObjectVision:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)
        self.box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=1, text_scale=0.5)

    def start_detection(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            raise RuntimeError("Camera non accessibile.")

        print("🎥 Avvio rilevamento oggetti YOLOv8... Premi 'q' per uscire.")

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = self.model(frame, verbose=False)[0]
            detections = sv.Detections.from_ultralytics(results)
            labels = [f"{c} {s:.2f}" for c, s in zip(results.names.values(), results.boxes.conf.cpu().numpy())]

            annotated = self.box_annotator.annotate(scene=frame, detections=detections, labels=labels)
            cv2.imshow("Mercurius∞ Vision", annotated)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

    def contextual_reaction(self, detected_items: list) -> str:
        if "person" in detected_items:
            return "👁️ Persona rilevata. Inizio monitoraggio ambientale."
        elif "keyboard" in detected_items:
            return "⌨️ Attività utente rilevata. Modalità lavoro attiva."
        else:
            return "🔍 Nessun oggetto prioritario rilevato."

## vision/ocr_module.py
"""
Modulo: ocr_module.py
Descrizione: Estrae testo da immagini tramite OCR (Tesseract o alternativa).
"""

try:
    import pytesseract
    from PIL import Image
except ImportError:
    raise ImportError("Modulo OCR non installato: usa `pip install pytesseract pillow`")

def extract_text_from_image(image_path: str) -> str:
    """
    Estrae il testo da un'immagine (jpg, png) usando OCR.
    """
    try:
        img = Image.open(image_path)
        text = pytesseract.image_to_string(img, lang='ita')  # o 'eng' se preferisci
        return text.strip()
    except Exception as e:
        return f"[❌ Errore OCR]: {str(e)}"

## vision/ocr_reader.py
# vision/ocr_reader.py

"""
Modulo: ocr_reader.py
Descrizione: Estrazione testi da immagini o webcam tramite OCR (Tesseract).
Supporta JPEG, PNG, flussi video.
"""

import pytesseract
import cv2


class OCRReader:
    def __init__(self):
        pass

    def read_text_from_image(self, path: str) -> str:
        img = cv2.imread(path)
        return pytesseract.image_to_string(img, lang="ita+eng")

    def read_from_camera(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            text = pytesseract.image_to_string(frame)
            print(f"[OCR] {text.strip()}")
            cv2.imshow("OCR Live", frame)
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
        cap.release()
        cv2.destroyAllWindows()

## vision/voice_trigger.py
# voice/voice_trigger.py

"""
Modulo: voice_trigger.py
Descrizione: Attivazione vocale tramite parola chiave "Hey Mercurius" utilizzando STT.
"""

import speech_recognition as sr


def listen_for_trigger(trigger_word: str = "hey mercurius") -> bool:
    """
    Ascolta il microfono per attivazione vocale.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎙️ Ascolto in corso... (parola chiave: 'Hey Mercurius')")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio).lower()
        print(f"🗣️ Rilevato: {text}")
        return trigger_word in text
    except sr.UnknownValueError:
        print("⚠️ Audio non riconosciuto.")
    except sr.RequestError:
        print("❌ Errore nel servizio di riconoscimento.")

    return False

## vision/yolo_handler.py
# vision/yolo_handler.py

"""
Modulo: yolo_handler.py
Descrizione: Riconoscimento oggetti con YOLOv5/YOLOv8 tramite OpenCV per Mercurius∞.
"""

from typing import List
import torch
import numpy as np

# Caricamento modello YOLO (richiede modello pre-addestrato disponibile localmente)
try:
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', trust_repo=True)
except Exception as e:
    print("⚠️ Errore nel caricamento del modello YOLO:", e)
    model = None


def detect_objects(image: np.ndarray) -> List[str]:
    """
    Rileva oggetti in un'immagine e restituisce le etichette.
    """
    if model is None:
        return []

    results = model(image)
    labels = results.pandas().xyxy[0]['name'].tolist()
    return labels

## voice/README.md
# 🎙️ Modulo Vocale – Attivazione

Gestisce input vocali e hotword per l'attivazione GENESIS.

## Componenti

- `activation_hook.py`: listener per "Hey Mercurius, attiva GENESIS"

## voice/__init__.py

## voice/coqui_tts.py
# voice/coqui_tts.py

"""
Modulo: coqui_tts.py
Descrizione: Sintesi vocale offline con Coqui TTS.
"""





## voice/elevenlabs_tts.py
# voice/elevenlabs_tts.py

"""
Modulo: elevenlabs_tts.py
Descrizione: Voce naturale con API ElevenLabs – stile Jarvis.
"""

import requests
import os

class ElevenLabsTTS:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("ELEVENLABS_API_KEY")

    def speak(self, text: str, voice_id="EXAVITQu4vr4xnSDxMaL"):
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
        headers = {
            "xi-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        json_data = {
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": {"stability": 0.5, "similarity_boost": 0.75}
        }
        response = requests.post(url, json=json_data, headers=headers)
        with open("output_11labs.wav", "wb") as f:
            f.write(response.content)

## voice/nari_tts.py
# voice/nari_tts.py

"""
Modulo: nari_tts.py
Descrizione: Sintesi vocale con il modello Nari Dia TTS.
"""

import soundfile as sf
from dia.model import Dia

class NariDiaTTS:
    def __init__(self, model_name="nari-labs/Dia-1.6B"):
        self.model = Dia.from_pretrained(model_name)

    def speak(self, text: str, output_path="output.wav"):
        output = self.model.generate(text)
        sf.write(output_path, output, 44100)

## voice/stt.py
# voice/stt.py

"""
Modulo: stt.py
Descrizione: Riconoscimento vocale da microfono in testo utilizzando SpeechRecognition (Google STT).
"""

import speech_recognition as sr


def transcribe_audio() -> str:
    """
    Converte l'audio acquisito da microfono in testo.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎧 In ascolto...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio, language="it-IT")
        print(f"📝 Riconosciuto: {text}")
        return text
    except sr.UnknownValueError:
        return "[Voce non riconosciuta]"
    except sr.RequestError:
        return "[Errore nel riconoscimento vocale]"

## voice/tts.py
# voice/tts.py (aggiornato)

"""
Modulo: tts.py
Descrizione: Sintesi vocale con fallback a gTTS se pyttsx3 non disponibile.
"""

try:
    import pyttsx3
    ENGINE = pyttsx3.init()
    USE_PYTTS = True
except ImportError:
    from gtts import gTTS
    import os
    USE_PYTTS = False


def speak(text: str):
    if USE_PYTTS:
        ENGINE.say(text)
        ENGINE.runAndWait()
    else:
        tts = gTTS(text=text, lang="it")
        file_path = "temp_audio.mp3"
        tts.save(file_path)
        os.system(f"start {file_path}" if os.name == "nt" else f"xdg-open {file_path}")

## voice/voice_bridge.py
"""voice_bridge.py
Output vocale tramite engine TTS locale.
"""

from __future__ import annotations

import pyttsx3

_engine = pyttsx3.init()


def speak(text: str) -> None:
    """Riproduce testo tramite sintesi vocale."""
    _engine.say(text)
    _engine.runAndWait()

## voice/voice_identity.py
# voice/voice_identity.py

"""
Modulo: voice_identity.py
Descrizione: Riconoscimento vocale degli speaker e saluti personalizzati.
"""

import os
import speech_recognition as sr
import json
from datetime import datetime
import hashlib


class VoiceIdentityManager:
    def __init__(self, db_path="logs/voice_profiles.json"):
        self.db_path = db_path
        if not os.path.exists(self.db_path):
            with open(self.db_path, "w") as f:
                json.dump({}, f)
        self.db = self._load_db()

    def _load_db(self):
        with open(self.db_path, "r") as f:
            return json.load(f)

    def identify_speaker(self, audio: sr.AudioData, recognizer: sr.Recognizer) -> str:
        try:
            text = recognizer.recognize_google(audio, language="it-IT")
            voice_id = self._voice_hash(audio)
            if voice_id in self.db:
                return f"🎙️ Bentornato {self.db[voice_id]['titolo']} {self.db[voice_id]['nome']}!"
            else:
                print("Voce non riconosciuta. Chi sei?")
                return self.register_new_voice(voice_id, text)
        except Exception:
            return "❌ Voce non comprensibile."

    def register_new_voice(self, voice_id: str, input_text: str) -> str:
        name = input_text.strip().split()[-1].capitalize()
        titolo = "Signor" if name[-1] not in "aeiou" else "Signora"
        self.db[voice_id] = {"nome": name, "titolo": titolo, "registrato": datetime.now().isoformat()}
        with open(self.db_path, "w") as f:
            json.dump(self.db, f, indent=2)
        return f"🎙️ Piacere {titolo} {name}, registrazione completata."

    def _voice_hash(self, audio: sr.AudioData) -> str:
        return hashlib.sha256(audio.get_raw_data()).hexdigest()[:16]

## voice/vosk_stt.py
# voice/vosk_stt.py

"""
Modulo: vosk_stt.py
Descrizione: Riconoscimento vocale locale con Vosk.
"""

import sounddevice as sd
import queue
import vosk
import json

class VoskSTT:
    def __init__(self, model_path="model"):
        self.model = vosk.Model(model_path)
        self.q = queue.Queue()

    def listen(self, duration=5, fs=16000):
        def callback(indata, frames, time, status):
            self.q.put(bytes(indata))
        with sd.RawInputStream(samplerate=fs, blocksize=8000, dtype="int16", channels=1, callback=callback):
            rec = vosk.KaldiRecognizer(self.model, fs)
            for _ in range(int(duration * fs / 8000)):
                data = self.q.get()
                if rec.AcceptWaveform(data):
                    res = json.loads(rec.Result())
                    return res.get("text", "")
            return ""

## voice/whisper_engine.py
# voice/whisper_engine.py

"""
Modulo: whisper_engine.py
Descrizione: Sintesi vocale inversa (STT) ad alta precisione con Whisper v3.
Supporta più lingue e trascrizione offline tramite modelli locali o OpenAI API.
"""

import os
import tempfile
import whisper


class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def transcribe_audio_file(self, audio_path: str, language: str = "it") -> str:
        result = self.model.transcribe(audio_path, language=language)
        return result.get("text", "[Nessun testo estratto]")

    def transcribe_microphone(self, duration=5, tmp_format="micro_input.wav") -> str:
        import sounddevice as sd
        import scipy.io.wavfile

        samplerate = 16000
        recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
        sd.wait()

        tmp_path = os.path.join(tempfile.gettempdir(), tmp_format)
        scipy.io.wavfile.write(tmp_path, samplerate, recording)
        return self.transcribe_audio_file(tmp_path)

## voice/whisper_stt.py
# voice/whisper_stt.py

"""
Modulo: whisper_stt.py
Descrizione: Trascrizione vocale avanzata multilingua tramite Whisper Large-V3.
"""

import whisper
import sounddevice as sd
import numpy as np
import tempfile
import wave

class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def record_audio(self, duration=5, fs=16000, device_index=None) -> str:
        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, device=device_index)
        sd.wait()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            wav_file = f.name
            with wave.open(wav_file, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(fs)
                wf.writeframes((audio * 32767).astype(np.int16).tobytes())
        return wav_file

    def transcribe_live_audio(self, duration=5, device_index=None) -> str:
        file_path = self.record_audio(duration, device_index=device_index)
        return self.transcribe_file(file_path)

    def transcribe_file(self, file_path: str) -> str:
        result = self.model.transcribe(file_path)
        return result["text"]

## voice/yolov8_engine.py
# vision/yolov8_engine.py

"""
Modulo: yolov8_engine.py
Descrizione: Riconoscimento in tempo reale di oggetti, volti e gesti con YOLOv8.
Supporta flussi da webcam o video file.
"""

import cv2
from ultralytics import YOLO


class VisionAI:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)

    def detect_from_image(self, image_path: str) -> list:
        results = self.model(image_path)
        return results[0].names

    def detect_from_webcam(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            results = self.model(frame)
            annotated = results[0].plot()
            cv2.imshow("Mercurius Vision", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

## voice/engine/coqui_tts.py
class CoquiTTS:
    def __init__(self):
        self.name = "CoquiTTS"

    def speak(self, phrase: str) -> str:
        return f"[{self.name}] Audio generato per: {phrase}"

## voice/engine/elevenlabs_tts.py
class ElevenLabsTTS:
    def __init__(self):
        self.name = "ElevenLabs"

    def synthesize(self, text: str, voice: str = "Jarvis") -> str:
        return f"[{self.name}] Sintesi vocale: '{text}' con voce {voice}"

## voice/engine/whisper_stt.py
class WhisperSTT:
    def __init__(self):
        self.name = "Whisper"

    def transcribe(self, audio_path: str) -> str:
        return f"[{self.name}] Trascrizione simulata del file: {audio_path}"

**ISTRUZIONI OPERATIVE:**
# Aggiungi qui le istruzioni operative per GPT-Engineer

### --- prompt_commands.txt --- ###
# Aggiungi qui le istruzioni operative per GPT-Engineer

### --- pyproject.toml --- ###
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

### --- rag/insight_rag.py --- ###
# rag/insight_rag.py

"""
Modulo: insight_rag.py
Descrizione: Sistema di archiviazione e recupero semantico (RAG) per concetti estratti da fonti multimodali.
"""

import os
import json
import uuid
from datetime import datetime
from sentence_transformers import SentenceTransformer, util

class InsightRAG:
    def __init__(self, db_path="logs/insight_memory.json"):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.db_path = db_path
        self.embeddings = []
        self.memory = []
        self.load_memory()

    def load_memory(self):
        if os.path.exists(self.db_path):
            with open(self.db_path, "r") as f:
                self.memory = json.load(f)
                self.embeddings = [item["embedding"] for item in self.memory]

    def save_memory(self):
        with open(self.db_path, "w") as f:
            json.dump(self.memory, f, indent=2)

    def embed_insight(self, content: str):
        embedding = self.model.encode(content).tolist()
        entry = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "text": content,
            "embedding": embedding
        }
        self.memory.append(entry)
        self.embeddings.append(embedding)
        self.save_memory()

    def query_concepts(self, question: str, top_k=3) -> list:
        query_emb = self.model.encode(question)
        scores = util.cos_sim(query_emb, self.embeddings)[0]
        top_indices = scores.argsort(descending=True)[:top_k]
        return [self.memory[idx] for idx in top_indices]

    def rank_relevance(self):
        return sorted(self.memory, key=lambda x: x["timestamp"], reverse=True)[:10]

### --- requirements.txt --- ###
absl-py==2.3.0
aiofiles==24.1.0
altair==5.5.0
altgraph==0.17.4
annotated-types==0.7.0
anyio==4.9.0
argbind==0.3.9
asttokens==3.0.0
attrs==25.3.0
audioread==3.0.1
bcrypt==4.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
cachetools==5.5.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
click==8.2.1
colorama==0.4.6
comtypes==1.4.11
contourpy==1.3.2
cryptography==45.0.3
cssselect==1.3.0
cycler==0.12.1
decorator==5.2.1
defusedxml==0.7.1
descript-audio-codec==1.0.0
descript-audiotools==0.7.2
diskcache==5.6.3
distro==1.9.0
docstring_parser==0.16
einops==0.8.1
executing==2.2.0
fastapi==0.115.12
ffmpy==0.5.0
filelock==3.18.0
fire==0.7.0
flatten-dict==0.4.2
fonttools==4.58.1
fsspec==2025.5.1
future==1.0.0
gitdb==4.0.12
GitPython==3.1.44
gradio==5.32.0
gradio_client==1.10.2
greenlet==3.2.2
groovy==0.1.2
grpcio==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.32.3
idna==3.10
imageio==2.37.0
imageio-ffmpeg==0.6.0
importlib_resources==6.5.2
iniconfig==2.1.0
ipython==9.3.0
ipython_pygments_lexers==1.1.1
jedi==0.19.2
Jinja2==3.1.6
jiter==0.10.0
joblib==1.5.1
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
julius==0.2.7
kiwisolver==1.4.8
langchain==0.3.25
langchain-core==0.3.63
langchain-text-splitters==0.3.8
langsmith==0.3.43
lazy_loader==0.4
librosa==0.11.0
llama_cpp_python==0.3.9
llvmlite==0.44.0
lxml==5.4.0
lxml_html_clean==0.4.2
Markdown==3.8
markdown-it-py==3.0.0
markdown2==2.5.3
MarkupSafe==3.0.2
matplotlib==3.10.3
matplotlib-inline==0.1.7
mdurl==0.1.2
-e git+https://github.com/giack891811/mercurius_infinite_final.git@92670b6422489a439fa7e6e9c4a47cdaa4aef3e1#egg=mercurius_infinite
more-itertools==10.7.0
moviepy==2.2.1
mpmath==1.3.0
msgpack==1.1.0
-e git+https://github.com/nari-labs/dia.git@2811af1c5f476b1f49f4744fabf56cf352be21e5#egg=nari_tts
narwhals==1.41.0
networkx==3.5
numba==0.61.2
numpy==1.26.4
openai==1.82.1
openai-whisper==20240930
opencv-python==4.11.0.86
orjson==3.10.18
packaging==24.2
pandas==2.2.3
paramiko==3.5.1
parso==0.8.4
pefile==2023.2.7
pillow==11.2.1
platformdirs==4.3.8
pluggy==1.6.0
pooch==1.8.2
proglog==0.1.12
prometheus_client==0.22.0
prompt_toolkit==3.0.51
protobuf==6.31.1
psutil==7.0.0
pure_eval==0.2.3
py-cpuinfo==9.0.0
pyarrow==20.0.0
pycparser==2.22
pydantic==2.11.5
pydantic_core==2.33.2
pydeck==0.9.1
pydub==0.25.1
Pygments==2.19.1
pyinstaller==6.13.0
pyinstaller-hooks-contrib==2025.4
PyJWT==2.10.1
pyloudnorm==0.1.1
PyNaCl==1.5.0
pyparsing==3.2.3
pypiwin32==223
PyQt5==5.15.11
PyQt5-Qt5==5.15.2
PyQt5_sip==12.17.0
pystoi==0.4.1
pytesseract==0.3.13
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
pyttsx3==2.98
pytube==15.0.0
pytz==2025.2
pywin32==310
pywin32-ctypes==0.2.3
PyYAML==6.0.2
randomname==0.2.1
readability-lxml==0.8.4.1
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-toolbelt==1.0.0
rich==14.0.0
rpds-py==0.25.1
ruff==0.11.12
safehttpx==0.1.6
safetensors==0.5.3
scikit-learn==1.6.1
scipy==1.15.3
semantic-version==2.10.0
sentence-transformers==4.1.0
setuptools==80.9.0
shellingham==1.5.4
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
sounddevice==0.5.2
soundfile==0.13.1
soupsieve==2.7
soxr==0.5.0.post1
SpeechRecognition==3.14.3
SQLAlchemy==2.0.41
srt==3.5.3
stack-data==0.6.3
starlette==0.46.2
streamlit==1.45.1
supervision==0.25.1
sympy==1.13.1
tenacity==9.1.2
tensorboard==2.19.0
tensorboard-data-server==0.7.2
termcolor==3.1.0
threadpoolctl==3.6.0
tiktoken==0.9.0
tokenizers==0.21.1
toml==0.10.2
tomlkit==0.13.2
torch==2.2.2+cu121
torch-stoi==0.2.3
torchaudio==2.2.2+cu121
torchvision==0.17.2+cu121
tornado==6.5.1
tqdm==4.67.1
traitlets==5.14.3
transformers==4.52.4
triton-windows==3.2.0.post18
typer==0.16.0
typing-inspection==0.4.1
typing_extensions==4.13.2
tzdata==2025.2
ultralytics==8.3.146
ultralytics-thop==2.0.14
urllib3==2.4.0
uvicorn==0.34.2
vosk==0.3.45
watchdog==6.0.0
wcwidth==0.2.13
websockets==15.0.1
Werkzeug==3.1.3
wheel==0.45.1
whisper==1.1.10
zstandard==0.23.0

scapy
pybluez
pywhatkit
pika==1.3.2

### --- safety/__init__.py --- ###


### --- safety/audit_logger.py --- ###
# safety/audit_logger.py
"""
Modulo: audit_logger
Descrizione: Log di audit immutabile per registrare azioni, decisioni e override.
"""

from pathlib import Path
from datetime import datetime
import json

AUDIT_FILE = Path("logs/audit_log.jsonl")
AUDIT_FILE.parent.mkdir(parents=True, exist_ok=True)


def audit(event_type: str, details: dict):
    entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "type": event_type,
        "details": details,
    }
    with open(AUDIT_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry) + "\n")

### --- safety/human_override.py --- ###
# safety/human_override.py
"""
Modulo: human_override
Descrizione: Consente all'operatore umano di confermare/bloccare azioni critiche.
"""
