Questa è la parte 25 di project_tree. Continua da quella precedente.

        output = sum(x * w for x, w in zip(inputs, self.weights)) + self.bias
        return [self._sigmoid(output)]

    def _sigmoid(self, x):
        """Funzione di attivazione sigmoid."""
        try:
            return 1 / (1 + pow(2.718, -x))
        except OverflowError:
            return 0.0 if x < 0 else 1.0

    def train(self, data):
        """Mock training: registra i dati per debugging."""
        print("Training data ricevuti:", data)

    def update_weights(self, new_weights):
        """Aggiorna i pesi della rete."""
        if len(new_weights) == self.input_dim:
            self.weights = new_weights

    def summary(self):
        """Restituisce un riassunto del modello."""
        return {
            "weights": self.weights,
            "bias": self.bias,
            "input_dim": self.input_dim
        }

## models/metrics/performance_metrics.py
"""
performance_metrics.py
=======================
Calcolo di metriche da esperienze Mercurius∞: accuracy, distribuzione profitti, ecc.
"""

from statistics import mean, stdev


class PerformanceMetrics:
    def __init__(self, experiences):
        self.experiences = experiences

    def profit_stats(self):
        profits = [e["result"].get("profit", 0) for e in self.experiences]
        return {
            "mean": mean(profits) if profits else 0,
            "stddev": stdev(profits) if len(profits) > 1 else 0,
            "count": len(profits)
        }

    def accuracy(self):
        correct = 0
        for e in self.experiences:
            profit = e["result"].get("profit", 0)
            action = e["trade"]["action"]
            if (profit > 0 and action == "BUY") or (profit < 0 and action == "SELL"):
                correct += 1
        return correct / len(self.experiences) if self.experiences else 0

    def summary(self):
        stats = self.profit_stats()
        return {
            **stats,
            "accuracy": self.accuracy()
        }

## modules/__init__.py
# modules/__init__.py
# rende importabili i sotto‐pacchetti

## modules/autogen_chat.py
# modules/autogen_chat.py

"""
Modulo: autogen_chat.py
Descrizione: Implementazione simulata di chat cooperativa multi-agente tramite Microsoft Autogen.
"""

class AutoGenChat:
    def __init__(self, agents=None):
        if agents is None:
            agents = ["Coder", "Planner", "Validator"]
        self.agents = agents

    def simulate_chat(self, topic: str):
        return "\n".join([f"{agent}: Partecipo alla discussione su '{topic}'." for agent in self.agents])


# Test
if __name__ == "__main__":
    chat = AutoGenChat()
    print(chat.simulate_chat("Sviluppo modulo di visione AI"))

## modules/chatgpt_interface.py
"""
Modulo: chatgpt_interface.py
Descrizione: Interfaccia tra Mercurius∞ e ChatGPT-4 per ragionamento, validazione e supporto decisionale.
"""

import openai
import os


class ChatGPTInterface:
    def __init__(self, model="gpt-4", temperature=0.4):
        self.model = model
        self.temperature = temperature
        self.api_key = os.getenv("OPENAI_API_KEY", "")

        if not self.api_key:
            raise ValueError("❌ OPENAI_API_KEY non definita nell'ambiente.")

        openai.api_key = self.api_key

    def ask(self, prompt: str, system: str = "Agisci come supervisore AI avanzato.") -> str:
        """
        Invia un messaggio a ChatGPT e restituisce la risposta.
        """
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                temperature=self.temperature,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": prompt}
                ]
            )
            reply = response.choices[0].message.content.strip()
            return reply
        except Exception as e:
            return f"⚠️ Errore nella richiesta a ChatGPT: {e}"

    def validate_code(self, code_snippet: str) -> str:
        """
        Chiede a ChatGPT di validare un frammento di codice.
        """
        prompt = f"Valuta se il seguente codice è valido e migliorabile:\n\n{code_snippet}"
        return self.ask(prompt, system="Sei un validatore di codice Python altamente esperto.")


# Uso diretto
if __name__ == "__main__":
    gpt = ChatGPTInterface()
    print(gpt.ask("Qual è il significato della vita secondo l'informatica?"))

## modules/crewai_team.py
# modules/crewai_team.py

"""
Modulo: crewai_team.py
Descrizione: Simulazione di un team AI con ruoli definiti (coder, manager, validator) basato su CrewAI.
"""

class CrewAI:
    def __init__(self):
        self.members = {
            "Project Manager": [],
            "Senior Coder": [],
            "Validator": []
        }

    def assign_task(self, role: str, task: str):
        if role in self.members:
            self.members[role].append(task)
            return f"📌 Task assegnato a {role}: {task}"
        else:
            return f"❌ Ruolo non valido: {role}"

    def team_report(self):
        return "\n".join([f"{role}: {', '.join(tasks)}" if tasks else f"{role}: 🔕 Nessun task" for role, tasks in self.members.items()])


# Test
if __name__ == "__main__":
    crew = CrewAI()
    print(crew.assign_task("Senior Coder", "Sviluppare interfaccia OCR"))
    print(crew.assign_task("Validator", "Verifica modulo vocale"))
    print(crew.team_report())

## modules/feedback_loop.py
# modules/feedback_loop.py

"""
Modulo: feedback_loop.py
Descrizione: Coordina il feedback tra moduli AI in Mercurius∞ per auto-apprendimento, miglioramento codice e decisioni collettive.
"""

class FeedbackLoop:
    def __init__(self):
        self.logs = []

    def collect_feedback(self, source: str, message: str):
        entry = f"🔁 [{source}] → {message}"
        self.logs.append(entry)
        print(entry)

    def last_feedback(self, n=5):
        return "\n".join(self.logs[-n:])


# Test rapido
if __name__ == "__main__":
    fb = FeedbackLoop()
    fb.collect_feedback("AZR", "Codice semanticamente corretto.")
    fb.collect_feedback("GPT-4o", "Suggerita ottimizzazione per compatibilità.")
    print(fb.last_feedback())

## modules/fingpt_analyzer.py
# modules/fingpt_analyzer.py

"""
Modulo: fingpt_analyzer.py
Descrizione: Modulo di analisi NLP con FinGPT per generare segnali operativi da notizie e sentiment.
"""

class FinGPTAnalyzer:
    def __init__(self):
        self.last_signal = None

    def analyze_text(self, text: str) -> str:
        if "inflation" in text.lower():
            self.last_signal = "SELL"
        else:
            self.last_signal = "BUY"
        return f"📉 Segnale da news: {self.last_signal}"


# Test rapido
if __name__ == "__main__":
    bot = FinGPTAnalyzer()
    print(bot.analyze_text("US Inflation data released - very high"))

## modules/finrl_agent.py
# modules/finrl_agent.py

"""
Modulo: finrl_agent.py
Descrizione: Wrapper per l’utilizzo di FinRL all’interno di Mercurius∞. Consente addestramento e deploy di agenti RL per trading.
"""

class FinRLAgent:
    def __init__(self):
        self.model = None

    def train(self, data_path: str, model_type="ppo"):
        print(f"📈 Addestramento agente {model_type} su {data_path}")
        # Qui si collegherà al training FinRL in futuro

    def predict(self, state):
        return "🧠 Predizione (stub): buy/sell/hold"

    def evaluate(self):
        return "📊 Performance dell’agente: +4.2% (simulata)"


# Test
if __name__ == "__main__":
    agent = FinRLAgent()
    agent.train("data/btc.csv")
    print(agent.predict("BTC_state"))

## modules/freqtrade_bot.py
# modules/freqtrade_bot.py

"""
Modulo: freqtrade_bot.py
Descrizione: Struttura base per l'integrazione di strategie ML con Freqtrade.
"""

class FreqtradeBot:
    def __init__(self, strategy_name="MLBaseline"):
        self.strategy = strategy_name

    def run(self):
        return f"🤖 Esecuzione bot crypto con strategia: {self.strategy}"

    def update_strategy(self, new_name):
        self.strategy = new_name
        return f"🔁 Strategia aggiornata a: {self.strategy}"


# Test
if __name__ == "__main__":
    bot = FreqtradeBot()
    print(bot.run())
    print(bot.update_strategy("SuperAI_MACD"))

## modules/gesture.py
"""
Modulo: gesture.py
Responsabilità: Stub logico per riconoscimento gesti e azioni gestuali
Autore: Mercurius∞ Engineer Mode
"""

from typing import Dict


class GestureRecognizer:
    """
    Placeholder per riconoscimento gesti. Può essere integrato con visione artificiale.
    """

    def recognize(self, input_frame) -> Dict:
        """
        Analizza un frame video e ritorna un gesto identificato (stub logico).
        """
        # In un sistema reale si integrerebbe OpenCV + ML per gesture spotting
        return {
            "gesture": "saluto",
            "action": "inizia_conversazione"
        }

    def interpret_gesture(self, gesture: str) -> Dict:
        """
        Converte un gesto in comando.
        """
        if gesture == "saluto":
            return {"action": "interagisci_utente"}
        elif gesture == "indicazione":
            return {"action": "raggiungi_destinazione", "context": {"destinazione": "indicata"}}
        else:
            return {"action": "ignora"}

## modules/goal_manager.py
"""
Modulo: goal_manager.py
Descrizione: Gestione di obiettivi (Goal) con priorità, stato e contesto.
Autore: Mercurius∞ AI Engineer
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

@dataclass(order=False)  # disattiva ordinamento automatico
class Goal:
    name: str
    priority: int = 1
    context: Dict[str, Any] = field(default_factory=dict)
    done: bool = False
    status: str = "pending"  # possible values: pending, active, completed

class GoalManager:
    def __init__(self):
        self._goals: List[Goal] = []

    # --- API principale ---
    def add_goal(self, name: str, priority: int = 1, context: Optional[Dict[str, Any]] = None):
        self._goals.append(Goal(name=name, priority=priority, context=context or {}))
        # ordina per PRIORITÀ decrescente (priorità alta prima)
        self._goals.sort(key=lambda g: g.priority, reverse=True)

    def get_next_goal(self) -> Optional[Goal]:
        for g in self._goals:
            if not g.done:
                g.status = "active"  # aggiorna lo stato a active
                return g
        return None

    def complete_goal(self, name: str):
        for g in self._goals:
            if g.name == name:
                g.done = True
                g.status = "completed"
                break

    def list_goals(self) -> List[Goal]:
        return self._goals

    def active_goals(self) -> List[Goal]:
        """Restituisce la lista di goal attivi (status active e non done)."""
        return [g for g in self._goals if g.status == "active" and not g.done]

    def pending_goals(self) -> List[Goal]:
        """Ritorna i goal ancora in attesa di essere attivati."""
        return [g for g in self._goals if g.status == "pending" and not g.done]

    def all_goals(self) -> List[Dict[str, Any]]:
        """Rappresentazione serializzabile di tutti i goal."""
        return [
            {
                "name": g.name,
                "priority": g.priority,
                "context": g.context,
                "status": g.status,
                "done": g.done,
            }
            for g in self._goals
        ]

# Esempio di utilizzo
if __name__ == "__main__":
    gm = GoalManager()
    gm.add_goal("Scrivere report", priority=5)
    gm.add_goal("Debug sistema", priority=10)
    next_goal = gm.get_next_goal()
    print("Goal attivo:", next_goal)
    gm.complete_goal(next_goal.name)
    print("Lista goal:", gm.list_goals())

## modules/gpt4o_interface.py
"""
Modulo: gpt4o_interface.py
Descrizione: Comunicazione diretta con GPT-4o per validazione, riflessione e finalizzazione dei task AI.
"""

import openai
import os

class GPT4oInterface:
    def __init__(self, api_key: str = None, model: str = "gpt-4o"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        openai.api_key = self.api_key

    def ask(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1024) -> str:
        """
        Invia un prompt a GPT-4o e restituisce la risposta testuale.
        """
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            return f"❌ Errore GPT-4o: {str(e)}"


# Test locale
if __name__ == "__main__":
    gpt = GPT4oInterface()
    reply = gpt.ask("Validami questa funzione Python: def somma(a, b): return a + b")
    print(reply)

## modules/gpt_engineer_wrapper.py
# modules/gpt_engineer_wrapper.py

"""
Modulo: gpt_engineer_wrapper.py
Descrizione: Interfaccia per pilotare GPT-Engineer via CLI o API, permettendo generazione autonoma di progetti e moduli.
"""

import subprocess
import os

class GPTEngineerWrapper:
    def __init__(self, project_path="generated_projects/", config_file=None):
        self.project_path = project_path
        self.config_file = config_file or "gpt_config.yaml"

    def generate_project(self, prompt: str) -> str:
        """
        Avvia GPT-Engineer per generare un progetto in base al prompt.
        """
        try:
            os.makedirs(self.project_path, exist_ok=True)
            with open("prompt.txt", "w") as f:
                f.write(prompt)

            result = subprocess.run(
                ["gpt-engineer", "."],
                cwd=self.project_path,
                capture_output=True,
                text=True
            )
            return result.stdout or "✅ Generazione completata."
        except Exception as e:
            return f"❌ Errore GPT-Engineer: {e}"


# Test
if __name__ == "__main__":
    wrapper = GPTEngineerWrapper()
    print(wrapper.generate_project("Crea un'applicazione per il tracciamento del sonno"))

## modules/gpt_task_router.py

## modules/hf_tools_manager.py
# modules/hf_tools_manager.py

"""
Modulo: hf_tools_manager.py
Descrizione: Integrazione con HuggingFace Transformers Agents per interagire con strumenti locali.
"""

from transformers import HfAgent

class HFToolsManager:
    def __init__(self):
        self.agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")

    def use_tool(self, query: str) -> str:
        try:
            return self.agent.run(query)
        except Exception as e:
            return f"❌ Errore HuggingFace Tools: {str(e)}"

## modules/leonai_bridge.py
# modules/leonai_bridge.py

"""
Modulo: leonai_bridge.py
Descrizione: Integrazione Leon AI per il controllo testuale/vocale del sistema operativo.
"""

import os

class LeonAI:
    def execute_command(self, command: str) -> str:
        try:
            result = os.popen(command).read()
            return f"✅ Comando eseguito:\n{result}"
        except Exception as e:
            return f"❌ Errore: {str(e)}"


# Test
if __name__ == "__main__":
    leon = LeonAI()
    print(leon.execute_command("echo 'Mercurius è operativo!'"))

## modules/localai_executor.py
# modules/localai_executor.py

"""
Modulo: localai_executor.py
Descrizione: Wrapper per gestire LocalAI in locale con modelli in formato GGUF.
Supporta: GPT, STT/TTS, SD.
"""

import subprocess

class LocalAIExecutor:
    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url

    def call_model(self, prompt: str, model="gpt4all"):
        try:
            # Simulazione chiamata locale (sostituibile con requests.post se installato)
            command = f'curl -X POST {self.base_url}/chat -d \'{{"prompt": "{prompt}", "model": "{model}"}}\''
            output = subprocess.getoutput(command)
            return output
        except Exception as e:
            return f"❌ Errore durante l'esecuzione: {str(e)}"

## modules/meta_team_agent.py
# modules/meta_team_agent.py

"""
Modulo: meta_team_agent.py
Descrizione: Simula un team AI composto da PM, Developer e QA utilizzando MetaGPT o logica equivalente. Coordina task evolutivi.
"""

class MetaTeamAgent:
    def __init__(self):
        self.roles = {
            "PM": self.project_manager,
            "DEV": self.developer,
            "QA": self.quality_assurance
        }

    def assign_task(self, task: str) -> str:
        pm_result = self.roles["PM"](task)
        dev_result = self.roles["DEV"](pm_result)
        return self.roles["QA"](dev_result)

    def project_manager(self, task: str) -> str:
        return f"[PM] Definizione requisiti per: {task}"

    def developer(self, spec: str) -> str:
        return f"[DEV] Implementazione codice basata su: {spec}"

    def quality_assurance(self, code: str) -> str:
        return f"[QA] Validazione e test eseguiti su: {code}"


# Test locale
if __name__ == "__main__":
    meta = MetaTeamAgent()
    print(meta.assign_task("Crea un modulo per la gestione vocale"))

## modules/n8n_connector.py
# modules/n8n_connector.py

"""
Modulo: n8n_connector.py
Descrizione: Invio e ricezione webhook da n8n per orchestrare flussi AI → PC locale.
"""

import requests

class N8NConnector:
    def __init__(self, webhook_url="http://localhost:5678/webhook/test"):
        self.webhook_url = webhook_url

    def trigger_flow(self, payload: dict) -> str:
        try:
            res = requests.post(self.webhook_url, json=payload)
            return f"📡 Webhook n8n attivato: {res.status_code}"
        except Exception as e:
            return f"❌ Errore n8n: {str(e)}"

## modules/network_analyzer.py
"""network_analyzer.py
Analizza dispositivi sulla rete locale e categoriza le ricerche web.
"""

from __future__ import annotations

import os
from pathlib import Path
from collections import defaultdict
from datetime import datetime

try:
    from scapy.all import ARP, Ether, srp, sniff, DNSQR
except Exception:  # pragma: no cover - scapy may not be installed
    ARP = Ether = srp = sniff = DNSQR = None  # type: ignore

try:
    import bluetooth
except Exception:  # pragma: no cover - bluetooth may not be installed
    bluetooth = None  # type: ignore

try:
    import pywhatkit
except Exception:  # pragma: no cover - pywhatkit may not be installed
    pywhatkit = None  # type: ignore

# Categoria di parole chiave per le ricerche
CATEGORY_PATTERNS = {
    "salute": ["salute", "medic", "ospedale", "dieta", "farmac"],
    "politica": ["politic", "governo", "elezion"],
    "gossip": ["gossip", "vip", "celebrity"],
    "economia": ["econom", "borsa", "finanza"],
    "viaggi": ["viagg", "hotel", "voli"],
    "religione": ["chiesa", "papa", "religion"],
    "social": ["facebook", "instagram", "tiktok", "twitter"],
}

# Eventuale mappatura IP/MAC -> nome utente
KNOWN_DEVICES = {
    "AA:BB:CC:DD:EE:FF": "PAPA",
}


def scan_wifi_network(network_range: str = "192.168.1.0/24") -> list[dict]:
    """Rileva i dispositivi Wi-Fi sulla rete locale."""
    if ARP is None:
        return []
    arp = ARP(pdst=network_range)
    ether = Ether(dst="ff:ff:ff:ff:ff:ff")
    packet = ether / arp
    result = srp(packet, timeout=3, verbose=0)[0]
    devices = []
    for sent, received in result:
        devices.append({"ip": received.psrc, "mac": received.hwsrc})
    return devices


def scan_bluetooth_devices() -> list[dict]:
    """Scansione dei dispositivi Bluetooth vicini."""
    if bluetooth is None:
        return []
    devices = []
    try:
        nearby = bluetooth.discover_devices(duration=5, lookup_names=True)
        for addr, name in nearby:
            devices.append({"mac": addr, "name": name})
    except Exception:
        pass
    return devices


def _extract_domain(packet) -> str | None:
    if DNSQR and packet.haslayer(DNSQR):
        try:
            return packet[DNSQR].qname.decode().rstrip('.')
        except Exception:
            return None
    return None


def capture_dns_queries(duration: int = 30) -> list[str]:
    """Sniffa il traffico DNS per un certo periodo."""
    if sniff is None:
        return []
    queries = []
    packets = sniff(filter="udp port 53", timeout=duration)
    for p in packets:
        d = _extract_domain(p)
        if d:
            queries.append(d)
    return queries


def categorize_domain(domain: str) -> str:
    lower = domain.lower()
    for cat, keywords in CATEGORY_PATTERNS.items():
        for kw in keywords:
            if kw in lower:
                return cat
    return "altro"
[TRONCATO]

## modules/nlp.py
"""
Modulo: nlp.py
Responsabilità: Interpretazione semantica dei comandi vocali/testuali
Autore: Mercurius∞ Engineer Mode
"""

from typing import Dict


class CommandInterpreter:
    """
    Interpreta frasi e comandi naturali in azioni simboliche.
    """

    def __init__(self):
        self.known_commands = {
            "analizza l'ambiente": {"action": "analizza_ambiente"},
            "vai alla base": {"action": "raggiungi_destinazione", "context": {"destinazione": "base"}},
            "parla con me": {"action": "interagisci_utente"},
        }

    def interpret(self, text: str) -> Dict:
        """
        Converte una frase in comando semantico.
        """
        text = text.lower().strip()
        if text in self.known_commands:
            return self.known_commands[text]
        elif "ambiente" in text:
            return {"action": "analizza_ambiente"}
        elif "base" in text:
            return {"action": "raggiungi_destinazione", "context": {"destinazione": "base"}}
        elif "parla" in text or "conversazione" in text:
            return {"action": "interagisci_utente"}
        else:
            return {"action": "ignora", "context": {"frase": text}}

## modules/ollama3_interface.py
"""
Modulo: ollama3_interface.py
Descrizione: Interfaccia per comunicare con il server locale di Ollama 3 e ottenere risposte da modelli LLM open source.
"""

import requests
import json


class Ollama3Interface:
    def __init__(self, base_url="http://localhost:11434/api/generate", model="llama3"):
        self.base_url = base_url
        self.model = model

    def ask(self, prompt: str, stream: bool = False) -> str:
        """
        Invia un prompt al modello Ollama 3 e restituisce la risposta.
        """
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream
        }

        try:
            response = requests.post(self.base_url, headers=headers, data=json.dumps(data))
            response.raise_for_status()

            if stream:
                return self._handle_stream_response(response)
            else:
                output = response.json().get("response", "").strip()
                return output
        except Exception as e:
            return f"⚠️ Errore comunicazione con Ollama: {e}"

    def _handle_stream_response(self, response) -> str:
        output = ""
        for line in response.iter_lines():
            if line:
                try:
                    decoded = json.loads(line.decode("utf-8"))
                    chunk = decoded.get("response", "")
                    output += chunk
                except json.JSONDecodeError:
                    continue
        return output


# Test del modulo
if __name__ == "__main__":
    ollama = Ollama3Interface()
    reply = ollama.ask("Scrivi una funzione Python che calcola il fattoriale.")
    print(reply)

## modules/openbb_terminal.py
# modules/openbb_terminal.py

"""
Modulo: openbb_terminal.py
Descrizione: Wrapper per OpenBB Terminal. Supporta richieste CLI per dati e strategie via comando.
"""

import subprocess

class OpenBBWrapper:
    def run_command(self, command: str) -> str:
        try:
            result = subprocess.run(
                command, shell=True, capture_output=True, text=True
            )
            return result.stdout or "✅ Comando eseguito"
        except Exception as e:
            return f"❌ Errore: {e}"


# Test
if __name__ == "__main__":
    obb = OpenBBWrapper()
    print(obb.run_command("echo 'Simulazione OpenBB'"))

## modules/planner.py
"""
Modulo: planner.py
Responsabilità: Pianificazione strategica delle azioni in base a obiettivi e contesto
Autore: Mercurius∞ Engineer Mode
"""

from typing import List, Dict, Any


class ActionPlanner:
    """
    Planner strategico per sequenziare azioni sulla base di obiettivi e contesto.
    """

    def __init__(self):
        self.last_plan: List[Dict[str, Any]] = []

    def generate_plan(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Genera una sequenza di azioni per raggiungere un obiettivo dato il contesto.
        """
        # Placeholder semplice. In futuro: agenti LLM o regole fuzzy.
        plan = []

        if goal == "analizza_ambiente":
            plan.append({"action": "attiva_sensori", "params": {"tipo": "ambientali"}})
            plan.append({"action": "acquisisci_dati"})
            plan.append({"action": "valuta_rischi"})

        elif goal == "raggiungi_destinazione":
            plan.append({"action": "calcola_percorso", "params": {"destinazione": context.get("destinazione")}})
            plan.append({"action": "avvia_navigazione"})
            plan.append({"action": "monitoraggio_progresso"})

        elif goal == "interagisci_utente":
            plan.append({"action": "saluta"})
            plan.append({"action": "richiedi_input"})
            plan.append({"action": "rispondi"})

        else:
            plan.append({"action": "log", "params": {"messaggio": f"Nessun piano noto per '{goal}'"}})

        self.last_plan = plan
        return plan

    def describe_plan(self, plan: List[Dict[str, Any]]) -> str:
        """
        Descrive verbalmente un piano d'azione.
        """
        description = "Piano d'azione:\n"
        for step in plan:
            description += f" - {step['action']}"
            if "params" in step:
                description += f" con parametri {step['params']}"
            description += "\n"
        return description

    def validate_plan(self, plan: List[Dict[str, Any]]) -> bool:
        """
        Verifica che il piano contenga azioni ben definite.
        """
        for step in plan:
            if not isinstance(step.get("action"), str):
                return False
        return True

    def plan_summary(self) -> Dict[str, Any]:
        """
        Riepilogo dell'ultimo piano generato.
        """
        return {
            "step_count": len(self.last_plan),
            "actions": [step["action"] for step in self.last_plan]
        }

## modules/qlib_quant.py
# modules/qlib_quant.py
"""
Modulo: qlib_quant.py
Descrizione: Integrazione con Qlib per predizione di prezzi, analisi dati storici e backtest quantitativi.
"""
import random

class QlibQuant:
    def __init__(self):
        # Mantiene ultime previsioni per simulare continuità (es. prezzo ultimo conosciuto per ticker)
        self.last_price = {}

    def predict(self, ticker: str) -> float:
        """Restituisce una previsione simulata del prezzo per il ticker dato."""
        base_price = self.last_price.get(ticker, random.uniform(50, 150))  # prezzo base casuale se sconosciuto