Questa è la parte 14 di project_tree. Continua da quella precedente.

            self.model = None
            self.ready = False

    def is_available(self):
        return self.ready

    def transcribe(self, file_path: str) -> str:
        """
        Trascrive un file audio offline con Vosk (stub demo).
        """
        if not self.ready:
            return "[❌ Vosk non disponibile]"
        try:
            import wave
            wf = wave.open(file_path, "rb")
            rec = self.vosk.KaldiRecognizer(self.model, wf.getframerate())
            results = []
            while True:
                data = wf.readframes(4000)
                if len(data) == 0:
                    break
                if rec.AcceptWaveform(data):
                    results.append(rec.Result())
            results.append(rec.FinalResult())
            return "\n".join(results)
        except Exception as e:
            return f"[❌ Errore Vosk]: {e}"

### --- modules/planner.py --- ###
"""
Modulo: planner.py
Responsabilità: Pianificazione strategica delle azioni in base a obiettivi e contesto
Autore: Mercurius∞ Engineer Mode
"""

from typing import List, Dict, Any


class ActionPlanner:
    """
    Planner strategico per sequenziare azioni sulla base di obiettivi e contesto.
    """

    def __init__(self):
        self.last_plan: List[Dict[str, Any]] = []

    def generate_plan(self, goal: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Genera una sequenza di azioni per raggiungere un obiettivo dato il contesto.
        """
        # Placeholder semplice. In futuro: agenti LLM o regole fuzzy.
        plan = []

        if goal == "analizza_ambiente":
            plan.append({"action": "attiva_sensori", "params": {"tipo": "ambientali"}})
            plan.append({"action": "acquisisci_dati"})
            plan.append({"action": "valuta_rischi"})

        elif goal == "raggiungi_destinazione":
            plan.append({"action": "calcola_percorso", "params": {"destinazione": context.get("destinazione")}})
            plan.append({"action": "avvia_navigazione"})
            plan.append({"action": "monitoraggio_progresso"})

        elif goal == "interagisci_utente":
            plan.append({"action": "saluta"})
            plan.append({"action": "richiedi_input"})
            plan.append({"action": "rispondi"})

        else:
            plan.append({"action": "log", "params": {"messaggio": f"Nessun piano noto per '{goal}'"}})

        self.last_plan = plan
        return plan

    def describe_plan(self, plan: List[Dict[str, Any]]) -> str:
        """
        Descrive verbalmente un piano d'azione.
        """
        description = "Piano d'azione:\n"
        for step in plan:
            description += f" - {step['action']}"
            if "params" in step:
                description += f" con parametri {step['params']}"
            description += "\n"
        return description

    def validate_plan(self, plan: List[Dict[str, Any]]) -> bool:
        """
        Verifica che il piano contenga azioni ben definite.
        """
        for step in plan:
            if not isinstance(step.get("action"), str):
                return False
        return True

    def plan_summary(self) -> Dict[str, Any]:
        """
        Riepilogo dell'ultimo piano generato.
        """
        return {
            "step_count": len(self.last_plan),
            "actions": [step["action"] for step in self.last_plan]
        }

### --- modules/qlib_quant.py --- ###
# modules/qlib_quant.py
"""
Modulo: qlib_quant.py
Descrizione: Integrazione con Qlib per predizione di prezzi, analisi dati storici e backtest quantitativi.
"""
import random

class QlibQuant:
    def __init__(self):
        # Mantiene ultime previsioni per simulare continuità (es. prezzo ultimo conosciuto per ticker)
        self.last_price = {}

    def predict(self, ticker: str) -> float:
        """Restituisce una previsione simulata del prezzo per il ticker dato."""
        base_price = self.last_price.get(ticker, random.uniform(50, 150))  # prezzo base casuale se sconosciuto
        # Simula variazione percentuale casuale tra -1% e +1%
        change = random.uniform(-0.01, 0.01)
        predicted_price = base_price * (1 + change)
        # Aggiorna lo storico del prezzo
        self.last_price[ticker] = predicted_price
        print(f"📈 QlibQuant: Predizione per {ticker} = {predicted_price:.2f}")
        return predicted_price

    def backtest(self):
        """Esegue un backtest simulato e restituisce un report sintetico."""
        # Simula calcolo di uno Sharpe Ratio basato su dati casuali
        sharpe_ratio = round(random.uniform(0.5, 2.0), 2)
        return f"✅ Backtest completato: Sharpe Ratio {sharpe_ratio}"

### --- modules/reasoner_dispatcher.py --- ###
"""reasoner_dispatcher.py
=======================
Dispatcher multi-agent che instrada i prompt ai vari Reasoner (GPT-4o, Ollama3, AZR, ecc.)
Seleziona e fonde le risposte, gestendo fallback ed errori.
"""

from __future__ import annotations

from typing import Dict
import json

from utils.logger import setup_logger
from modules.llm.chatgpt_interface import ChatGPTAgent
from modules.llm.ollama3_interface import Ollama3Agent
from modules.llm.azr_reasoner import AZRAgent
from modules.llm.gpt4o_validator import GPT4oAgent

logger = setup_logger("ReasonerDispatcher")


class ReasonerDispatcher:
    """Gestisce l'inoltro dei prompt ai vari reasoner e ne combina le risposte."""

    def __init__(self) -> None:
        self.reasoners = {
            "chatgpt4": ChatGPTAgent(),
            "ollama3": Ollama3Agent(),
            "azr": AZRAgent(),
            "gpt4o": GPT4oAgent(),
        }

    def dispatch(self, prompt: str) -> str:
        """Invia il prompt a tutti i reasoner disponibili e sintetizza la risposta migliore."""
        logger.info(f"[DISPATCH] Prompt ricevuto: {prompt}")
        responses: Dict[str, str] = {}
        for name, agent in self.reasoners.items():
            try:
                if name == "chatgpt4":
                    responses[name] = agent.elaborate(prompt)
                elif name == "ollama3":
                    responses[name] = agent.generate(prompt)
                elif name == "azr":
                    responses[name] = agent.analyze(prompt)
                elif name == "gpt4o":
                    responses[name] = agent.validate(prompt)
            except Exception as exc:
                responses[name] = f"Errore {name}: {exc}"
                logger.error(f"Errore nel reasoner {name}: {exc}")

        # Sintesi finale con GPT4o se disponibile
        synth_prompt = "Sintetizza in una risposta unica e coerente le seguenti risposte:\n" + json.dumps(responses, ensure_ascii=False, indent=2)
        try:
            final_resp = self.reasoners["gpt4o"].validate(synth_prompt)
        except Exception as exc:  # fallback se GPT4o fallisce
            logger.error(f"Fallback GPT4o: {exc}")
            # Selezione semplice: risposta più lunga senza errore
            valid = [r for r in responses.values() if not r.lower().startswith("errore") and r]
            final_resp = max(valid, key=len) if valid else "Nessuna risposta disponibile."
        logger.info("[DISPATCH] Risposta finale generata")
        return final_resp


def dispatch_to_reasoner(prompt: str) -> str:
    """Funzione helper per utilizzo rapido del dispatcher."""
    dispatcher = ReasonerDispatcher()
    return dispatcher.dispatch(prompt)


# Test rapido
if __name__ == "__main__":
    test_prompt = "Spiega la teoria della relativit\u00e0 in breve"
    print(dispatch_to_reasoner(test_prompt))

### --- modules/sandbox_executor/secure_executor.py --- ###
"""
Modulo: secure_executor.py
Descrizione: Esecuzione sicura di codice Python in sandbox controllata con timeout.
Autore: Mercurius∞ AI Engineer
"""

import sys
import io
import multiprocessing
import traceback
import contextlib

class SecureExecutor:
    def __init__(self, timeout: int = 5):
        """
        timeout: tempo massimo di esecuzione in secondi
        """
        self.timeout = timeout

    def _run_code(self, code: str, return_dict):
        """Esegue codice Python in ambiente isolato e cattura output e errori."""
        stdout = io.StringIO()
        stderr = io.StringIO()
        try:
            sys.stdout = stdout
            sys.stderr = stderr
            exec(code, {})
        except Exception:
            return_dict['error'] = traceback.format_exc()
        finally:
            return_dict['output'] = stdout.getvalue()
            return_dict['stderr'] = stderr.getvalue()

    def execute(self, code: str) -> dict:
        """
        Esegue codice Python con timeout e isolamento tramite multiprocessing.
        Ritorna un dict con chiavi: output, stderr, error.
        """
        manager = multiprocessing.Manager()
        return_dict = manager.dict()

        proc = multiprocessing.Process(target=self._run_code, args=(code, return_dict))
        proc.start()
        proc.join(self.timeout)

        if proc.is_alive():
            proc.terminate()
            return {
                "output": "",
                "stderr": "",
                "error": "Execution timed out."
            }

        # Se l'errore non è stato catturato da exec, assegna stringa vuota
        if 'error' not in return_dict:
            return_dict['error'] = ""

        return dict(return_dict)

# Test esecuzione diretta
if __name__ == "__main__":
    executor = SecureExecutor(timeout=3)
    code_snippet = """
print('Hello from sandbox!')
for i in range(3):
    print(i)
"""
    result = executor.execute(code_snippet)
    print("Output:", result['output'])
    print("Error:", result['error'])

### --- modules/speech.py --- ###
"""
Modulo: speech.py
Responsabilità: Gestione input vocale (ASR) e output vocale (TTS)
Autore: Mercurius∞ Engineer Mode
"""

try:
    import pyttsx3
except Exception:  # pragma: no cover - optional engine
    pyttsx3 = None
import speech_recognition as sr


class TextToSpeech:
    """
    Sintesi vocale basata su pyttsx3.
    """
    def __init__(self, voice_id=None):
        self.engine = None
        if pyttsx3 is not None:
            try:
                self.engine = pyttsx3.init()
                self.set_voice(voice_id)
            except Exception:
                self.engine = None

    def set_voice(self, voice_id):
        if not self.engine:
            return
        if voice_id is not None:
            self.engine.setProperty('voice', voice_id)
        else:
            voices = self.engine.getProperty('voices')
            if voices:
                self.engine.setProperty('voice', voices[0].id)

    def speak(self, text: str):
        if self.engine:
            self.engine.say(text)
            self.engine.runAndWait()
        else:
            print(f"[TTS] {text}")


class SpeechToText:
    """
    Riconoscimento vocale basato su speech_recognition.
    """
    def __init__(self, language: str = "it-IT"):
        self.recognizer = sr.Recognizer()
        self.language = language

    def listen(self, timeout: int = 5) -> str:
        with sr.Microphone() as source:
            print("🎙️ In ascolto...")
            audio = self.recognizer.listen(source, timeout=timeout)
            try:
                text = self.recognizer.recognize_google(audio, language=self.language)
                print("🗣️ Riconosciuto:", text)
                return text
            except sr.UnknownValueError:
                return "[ERROR] Non ho capito."
            except sr.RequestError as e:
                return f"[ERROR] Errore di connessione: {e}"

### --- modules/start_fullmode/initializer.py --- ###
"""
Modulo: initializer
Descrizione: Avvio completo del sistema Mercurius∞ in modalità autonoma.
Autore: Mercurius∞ AI Engineer
"""

import os
import time
from modules.ai_kernel.agent_core import AgentCore
from modules.voice_bridge.audio_interface import AudioInterface
from modules.stream_vision.video_pipeline import VideoPipeline

class SystemInitializer:
    def __init__(self):
        self.agent = AgentCore()
        self.audio = AudioInterface()
        self.vision = VideoPipeline()

    def initialize_environment(self):
        """Setup iniziale del sistema."""
        print("[INIT] Configurazione ambiente...")
        os.environ['MERCURIUS_MODE'] = 'full'
        time.sleep(1)

    def start_components(self):
        """Avvia tutti i moduli principali."""
        print("[INIT] Avvio moduli principali...")
        self.vision.start()
        self.audio.initialize()
        self.agent.boot()

    def start_fullmode(self):
        """Avvia il sistema in modalità autonoma completa."""
        self.initialize_environment()
        self.start_components()
        print("[INIT] Mercurius∞ avviato in modalità FULLMODE.")

# Avvio diretto
if __name__ == "__main__":
    system = SystemInitializer()
    system.start_fullmode()

### --- modules/strategic/__init__.py --- ###


### --- modules/strategic/strategic_brain.py --- ###
"""Strategic Brain module integrating gpt_engineer with Mercurius∞.
"""
from pathlib import Path
from typing import List

from modules.goal_manager import GoalManager, Goal
from modules.gpt_engineer_wrapper import GPTEngineerWrapper
from modules.llm.azr_reasoner import AZRAgent
from orchestrator.genesis_orchestrator import GenesisOrchestrator


class StrategicBrain:
    """High level manager that routes goals to LLMs and falls back to GPT-Engineer."""

    def __init__(self, workspace: str = "strategic_projects") -> None:
        self.goal_manager = GoalManager()
        self.orchestrator = GenesisOrchestrator()
        self.validator = AZRAgent()
        self.builder = GPTEngineerWrapper(project_path=workspace)
        Path(workspace).mkdir(exist_ok=True)

    def load_goals(self, goals_path: str) -> None:
        """Load goals from a text file."""
        path = Path(goals_path)
        if not path.exists():
            return
        for line in path.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if line:
                self.goal_manager.add_goal(line)

    def execute_goal(self, goal: Goal) -> str:
        """Process a single goal using the orchestrator and fallback with GPT-Engineer."""
        result = self.orchestrator.route_task(goal.name)
        analysis = self.validator.analyze(result.get("response", ""))
        if isinstance(analysis, str) and analysis.startswith("❌"):
            # invalid response: trigger GPT-Engineer
            return self.builder.generate_project(goal.name)
        return result.get("response", "")

    def run(self) -> List[str]:
        """Run through all pending goals and return list of outputs."""
        outputs = []
        while True:
            next_goal = self.goal_manager.get_next_goal()
            if not next_goal:
                break
            output = self.execute_goal(next_goal)
            outputs.append(output)
            self.goal_manager.complete_goal(next_goal.name)
        return outputs

### --- modules/strategic/strategic_runner.py --- ###
"""CLI per eseguire il modulo StrategicBrain."""
import argparse
from pathlib import Path

from .strategic_brain import StrategicBrain


def main() -> None:
    parser = argparse.ArgumentParser(description="Run Strategic Brain goals")
    parser.add_argument("goals", nargs="?", default="goals.txt", help="File dei goal o singolo obiettivo")
    args = parser.parse_args()

    brain = StrategicBrain()

    if Path(args.goals).is_file():
        brain.load_goals(args.goals)
    else:
        brain.goal_manager.add_goal(args.goals)

    results = brain.run()
    for res in results:
        print(res)


if __name__ == "__main__":
    main()

### --- modules/stream_vision/__init__.py --- ###
"""
Package stream_vision
Contiene pipeline di elaborazione video (placeholder minimale).
"""

### --- modules/stream_vision/video_pipeline.py --- ###
"""
Modulo: video_pipeline.py
Descrizione: Gestione realistica della pipeline video con placeholder di fallback.
Autore: Mercurius∞ AI Engineer
"""

import cv2
import threading

class VideoPipeline:
    def __init__(self, source=0, use_placeholder=False):
        """
        source: indice webcam o percorso file
        use_placeholder: se True usa il placeholder semplice senza OpenCV
        """
        self.source = source
        self.use_placeholder = use_placeholder
        self.active = False
        self.capture_thread = None

    def _process_frame(self, frame):
        """Elabora il frame video (placeholder per elaborazioni future)."""
        print("[VISION] Frame catturato.")
        return frame

    def _capture_loop(self):
        """Ciclo continuo di cattura video con OpenCV."""
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            print("[VISION] Impossibile aprire la sorgente video.")
            return

        while self.active:
            ret, frame = cap.read()
            if not ret:
                break
            self._process_frame(frame)
        cap.release()
        print("[VISION] Video terminato.")

    def start(self):
        """Avvia la pipeline video (reale o placeholder)."""
        if self.active:
            return
        print(f"[VISION] Avvio pipeline video su '{self.source}' " + 
              ("(placeholder)" if self.use_placeholder else "(reale)"))
        self.active = True
        if self.use_placeholder:
            print(f"📹 VideoPipeline avviata su '{self.source}' (placeholder)")
        else:
            self.capture_thread = threading.Thread(target=self._capture_loop)
            self.capture_thread.start()

    def stop(self):
        """Arresta la pipeline video."""
        if not self.active:
            return
        self.active = False
        if self.capture_thread:
            self.capture_thread.join()
        print("🛑 Pipeline video fermata")

# Esempio di esecuzione diretta
if __name__ == "__main__":
    vp = VideoPipeline(source=0, use_placeholder=False)
    vp.start()
    import time
    time.sleep(5)
    vp.stop()

### --- modules/stream_voice/__init__.py --- ###


### --- modules/superagi_agent.py --- ###
# modules/superagi_agent.py

"""
Modulo: superagi_agent.py
Descrizione: Framework per task evolutivi autonomi multi-step. Simula workflow AI dinamici tramite SuperAGI.
"""

class SuperAGIAgent:
    def __init__(self, name="MercuriusExecutor"):
        self.name = name
        self.steps = []

    def assign_task(self, task: str):
        self.steps = [f"Step {i+1}: {subtask}" for i, subtask in enumerate(task.split("."))]
        return f"🧠 {self.name} ha pianificato {len(self.steps)} subtask."

    def execute(self):
        results = [f"✅ {step} completato." for step in self.steps]
        return "\n".join(results)


# Test
if __name__ == "__main__":
    agent = SuperAGIAgent()
    print(agent.assign_task("Analizza i dati. Genera il report. Invia l’output."))
    print(agent.execute())

### --- modules/supervisor.py --- ###
"""
Modulo: supervisor.py
Responsabilità: Monitoraggio comportamentale e strategico del sistema
Autore: Mercurius∞ Engineer Mode
"""

import time
import datetime
from typing import Dict, List


class ActionLog:
    """
    Rappresenta un'azione osservata dal supervisore.
    """

    def __init__(self, action: str, outcome: str, success: bool, context: Dict):
        self.timestamp = datetime.datetime.now().isoformat()
        self.action = action
        self.outcome = outcome
        self.success = success
        self.context = context

    def to_dict(self) -> Dict:
        return {
            "timestamp": self.timestamp,
            "action": self.action,
            "outcome": self.outcome,
            "success": self.success,
            "context": self.context
        }


class Supervisor:
    """
    Sistema di supervisione del comportamento cognitivo e operativo.
    """

    def __init__(self):
        self.logs: List[Dict] = []
        self.total_actions = 0
        self.total_success = 0
        self.total_failures = 0
        self.start_time = time.time()

    def observe(self, action: str, outcome: str, success: bool, context: Dict):
        """
        Registra un evento/azione osservato.
        """
        self.total_actions += 1
        if success:
            self.total_success += 1
        else:
            self.total_failures += 1

        log_entry = ActionLog(action, outcome, success, context)
        self.logs.append(log_entry.to_dict())

    def performance_report(self) -> Dict:
        """
        Fornisce un report generale delle prestazioni osservate.
        """
        uptime = time.time() - self.start_time
        return {
            "uptime_sec": int(uptime),
            "actions_total": self.total_actions,
            "successes": self.total_success,
            "failures": self.total_failures,
            "success_rate": round((self.total_success / self.total_actions) * 100, 2) if self.total_actions else 0.0
        }

    def last_actions(self, count: int = 5) -> List[Dict]:
        return self.logs[-count:]

### --- modules/task_manager_cli.py --- ###
import argparse
from modules.Neo.trainer_orchestrator import bootstrap_agents
from modules.Localai.local_ai import LocalAI
from modules.Leonai.leon_ai import LeonAI

class TaskManagerCLI:
    def __init__(self):
        self.localai = LocalAI()
        self.leonai = LeonAI()
        print("🕹️ TaskManager CLI interattivo pronto! Scrivi 'ai: ...' per LLM offline, 'pc: ...' per comandi PC, 'exit' per uscire.")

    def run(self):
        while True:
            try:
                cmd = input("Task> ").strip()
                if not cmd:
                    continue
                if cmd.lower() == "exit":
                    print("Bye Jarvis!")
                    break
                if cmd.startswith("ai:"):
                    prompt = cmd[3:].strip()
                    response = self.localai.rispondi(prompt)
                    print(f"\n🤖 LocalAI: {response}\n")
                elif cmd.startswith("pc:"):
                    sys_cmd = cmd[3:].strip()
                    try:
                        out = self.leonai.esegui_comando(sys_cmd)
                        print(f"\n🦾 LeonAI Output:\n{out}\n")
                    except PermissionError as e:
                        print(f"[SECURITY]: {e}")
                else:
                    print("❓ Comando non riconosciuto. Usa 'ai:' o 'pc:' davanti.")
            except KeyboardInterrupt:
                print("\nInterrotto. Exit.")
                break

def create_agent(nome):
    print(f"🧬 Creo nuovo agente: {nome}")
    # Se bootstrap_agents non supporta argomenti, chiamalo senza parametri.
    bootstrap_agents()



def elenco_task():
    print("📜 Task disponibili:")
    print(" - crea_agente --nome <NomeAgente>")
    print(" - avvia_bootstrap")
    print(" - interactive (modalità CLI interattiva)")
    print(" - help")

def main():
    parser = argparse.ArgumentParser(description="Mercurius∞ TaskManager CLI – Modalità Jarvis+ Ultra")
    parser.add_argument("--task", type=str, help="Task da eseguire (es: crea_agente, avvia_bootstrap, interactive, help)")
    parser.add_argument("--nome", type=str, help="Nome agente, modulo o oggetto")
    args = parser.parse_args()

    if not args.task or args.task == "interactive":
        TaskManagerCLI().run()
    elif args.task == "crea_agente" and args.nome:
        create_agent(args.nome)
    elif args.task == "avvia_bootstrap":
        print("🟢 Avvio sequenza di bootstrap completa!")
        bootstrap_agents()
    elif args.task == "help":
        elenco_task()
    else:
        print("❌ Comando non riconosciuto. Usa '--task help' per la lista.")

### --- modules/url_learner.py --- ###
# modules/knowledge/url_learner.py
"""
Scarica pagine web, le riassume con GPT e salva in Long-Term Memory.
"""

import requests
import os
import openai
import readability
from bs4 import BeautifulSoup
from memory.long_term_memory import LongTermMemory

openai.api_key = os.getenv("OPENAI_API_KEY")
mem = LongTermMemory()                           # usa backend JSON

def _clean_html(html: str) -> str:
    # readability-lxml per estrarre solo il main <article>
    doc = readability.Document(html)
    soup = BeautifulSoup(doc.summary(), "html.parser")
    return soup.get_text(separator="\n")

def summarize(text: str, url: str) -> str:
    prompt = (
        f"Riassumi in 10 bullet il seguente articolo ({url}). "
        "Evidenzia i concetti chiave e gli eventuali numeri importanti.\n\n"
        f"{text[:8000]}"    # taglio altrimenti supero token
    )
    resp = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}],
        temperature=0.4, max_tokens=500
    )
    return resp["choices"][0]["message"]["content"]

def ingest_url(url: str):
    r = requests.get(url, timeout=15)
    r.raise_for_status()
    text = _clean_html(r.text)
    summary = summarize(text, url)
    mem.save_experience({
        "tags": ["url_knowledge"],
        "source": url,
        "summary": summary
    })
    print(f"✅ Ingested {url}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python -m modules.knowledge.url_learner <url1> <url2> …")
        raise SystemExit
    for u in sys.argv[1:]:
        ingest_url(u)

### --- modules/vision_audio/__init__.py --- ###


### --- modules/vision_audio/note10_jarvis_bridge.py --- ###
"""note10_jarvis_bridge.py
Modulo: note10_jarvis_bridge
Descrizione: trasforma un Note10+ in assistente vocale continuo in stile Jarvis.
"""

from __future__ import annotations

import logging
import queue
import threading
import time
from typing import Callable, List, Optional

try:
    import sounddevice as sd
except Exception:  # pragma: no cover - sounddevice may not be available
    sd = None

try:
    import whisper
except Exception:  # pragma: no cover - whisper may not be installed
    whisper = None

try:
    import vosk
except Exception:  # pragma: no cover - vosk may not be installed
    vosk = None

try:
    import requests
except Exception:  # pragma: no cover
    requests = None

HOTWORDS = [
    "tu che ne pensi aion",
    "analizzami questo aion",
    "tu che dici aion",
    "vero aion",
    "giusto aion?",
]

logger = logging.getLogger(__name__)


class VoiceListener:
    """Microfono sempre attivo con hotword detection."""

    def __init__(self, hotwords: Optional[List[str]] = None, model: str = "base"):
        self.hotwords = [h.lower() for h in (hotwords or HOTWORDS)]
        self.model_name = model
        self._queue: queue.Queue[bytes] = queue.Queue()
        self._stop = threading.Event()
        self._callback: Optional[Callable[[str], None]] = None
        self.use_whisper = False
        self.use_vosk = False
        self._init_models()

    def _init_models(self) -> None:
        if whisper:
            try:
                self.whisper_model = whisper.load_model(self.model_name)
                self.use_whisper = True
                logger.info("Whisper model ready")
            except Exception as exc:  # pragma: no cover
                logger.warning("Whisper load failed: %s", exc)
        if not self.use_whisper and vosk:
            try:
                self.vosk_model = vosk.Model("model")
                self.use_vosk = True
                logger.info("Vosk model ready")
            except Exception as exc:  # pragma: no cover
                logger.warning("Vosk load failed: %s", exc)

    def start(self, on_trigger: Callable[[str], None]) -> None:
        self._callback = on_trigger
        threading.Thread(target=self._listen_loop, daemon=True).start()

    def stop(self) -> None:
        self._stop.set()

    def _audio_cb(self, indata, frames, time_info, status) -> None:
        self._queue.put(bytes(indata))

    def _listen_loop(self) -> None:
        if not sd:
            logger.error("sounddevice non disponibile")
            return
        with sd.InputStream(channels=1, samplerate=16000, callback=self._audio_cb):
            while not self._stop.is_set():
                time.sleep(0.1)
                if not self._queue.empty():
                    data = b"".join([self._queue.get() for _ in range(self._queue.qsize())])
                    text = self._transcribe(data)
                    if text:
                        lowered = text.lower().strip()
                        if any(h in lowered for h in self.hotwords):
                            logger.info("Hotword detected: %s", lowered)
                            if self._callback:
                                self._callback(lowered)

    def _transcribe(self, audio: bytes) -> str:
        if self.use_whisper:
            try:
                import numpy as np
                waveform = np.frombuffer(audio, dtype="int16").astype(np.float32) / 32768.0
                result = self.whisper_model.transcribe(waveform, language="it")

                return result.get("text", "")
            except Exception as exc:  # pragma: no cover
                logger.error("Whisper error: %s", exc)
        if self.use_vosk:
            try:
                rec = vosk.KaldiRecognizer(self.vosk_model, 16000)
                if rec.AcceptWaveform(audio):
                    import json
                    return json.loads(rec.Result()).get("text", "")
            except Exception as exc:  # pragma: no cover
                logger.error("Vosk error: %s", exc)
        return ""


class PermissionHandler:
    """Gestisce l'autorizzazione dell'assistente."""

    def __init__(self) -> None:
        self.authorized = True