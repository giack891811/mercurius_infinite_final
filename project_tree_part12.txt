Questa √® la parte 12 di project_tree. Continua da quella precedente.

    AZRAnalyzer unisce due livelli di analisi:
      1. Analisi statistica su profitti (mean, volatility, ed eventuale suggerimento di riduzione rischio).
      2. Creazione di un prompt testuale per AZRAgent, che analizza il batch recente e suggerisce adattamenti.
    """

    def __init__(self, exp_memory: ExperienceMemory, config: Dict[str, Any]):
        """
        - exp_memory: istanza di ExperienceMemory gi√† inizializzata (backend JSON).
        - config: dizionario di configurazione, con possibili chiavi:
            - "azr_profit_floor": soglia minima media dei profitti (float), default 0.5
            - "base_trade_qty": quantit√† base del trade (int), default 100
            - "use_azr": booleano che abilita l‚Äôanalisi LLM (default True)
        """
        self.exp_memory = exp_memory
        self.config = config
        self.azr = AZRAgent() if config.get("use_azr", True) else None

    def analyze_recent_performance(
        self, limit: int = 20
    ) -> Dict[str, Union[str, float, Dict[str, Any]]]:
        """
        Esegue un‚Äôanalisi statistica sui profitti delle ultime `limit` esperienze.
        Restituisce un dizionario contenente:
          - "status": "no_data" o "ok"
          - "mean_profit": media dei profitti (float)
          - "volatility": deviazione standard (float)
          - "decision": suggerimento generico basato sul confronto con "azr_profit_floor"
        """
        recent = self.exp_memory.get_recent_experiences(limit)
        # Estrae la lista dei profitti, default a 0 se mancante
        profits: List[float] = [
            e.get("result", {}).get("profit", 0.0) for e in recent
        ]

        if not profits:
            return {"status": "no_data"}

        avg_profit = mean(profits)
        vol = stdev(profits) if len(profits) > 1 else 0.0
        decision = self._suggest_statistical(avg_profit)

        return {
            "status": "ok",
            "mean_profit": avg_profit,
            "volatility": vol,
            "decision": decision,
        }

    def _suggest_statistical(self, avg_profit: float) -> Dict[str, Any]:
        """
        Suggerisce un‚Äôazione di tuning se la media dei profitti √® inferiore a "azr_profit_floor".
        Se avg_profit < soglia, restituisce:
          {
            "action": "decrease_risk",
            "new_qty": <met√† di base_trade_qty o 10, se inferiore>
          }
        Altrimenti restituisce {"action": "maintain"}.
        """
        threshold = float(self.config.get("azr_profit_floor", 0.5))
        base_qty = int(self.config.get("base_trade_qty", 100))

        if avg_profit < threshold:
            new_qty = max(10, int(base_qty * 0.5))
            return {"action": "decrease_risk", "new_qty": new_qty}

        return {"action": "maintain"}

    def apply_statistical_adaptation(self, limit: int = 20) -> Dict[str, Any]:
        """
        Esegue l‚Äôanalisi statistica e, se viene suggerita una riduzione del rischio,
        aggiorna `config["base_trade_qty"]`. Restituisce il risultato dell'analisi.
        """
        result = self.analyze_recent_performance(limit)
        decision = result.get("decision", {})
        if decision.get("action") == "decrease_risk":
            self.config["base_trade_qty"] = decision["new_qty"]
        return result

    def analyze_with_azr(self, limit: int = 10) -> Optional[Dict[str, Any]]:
        """
        Crea un prompt testuale basato sulle ultime `limit` esperienze e lo invia ad AZRAgent,
        che restituisce un‚Äôanalisi in linguaggio naturale. Se AZRAgent suggerisce un cambiamento
        di rischio, viene applicato a config["base_trade_qty"] e viene restituito il dizionario
        {"action": ..., "new_qty": ...}. Altrimenti restituisce None.
        """
        if not self.azr:
            return None

        recent = self.exp_memory.get_recent_experiences(limit)
        if not recent:
            return None

        # Costruzione del prompt
        prompt_lines = ["Analizza queste esperienze di trading e suggerisci adattamento del rischio:"]
        for e in recent:
            profit = e.get("result", {}).get("profit", 0.0)
            qty = e.get("trade", {}).get("quantity", self.config.get("base_trade_qty", 100))
            prompt_lines.append(f"- Profit: {profit}, Quantity: {qty}")
        prompt = "\n".join(prompt_lines)

        # Chiamata ad AZRAgent
        analysis_text = self.azr.analyze(prompt)

        # Se AZRAgent parla di 'rischio' (o 'risk'), applichiamo la modifica
        if "rischio" in analysis_text.lower() or "risk" in analysis_text.lower():
            base_qty = int(self.config.get("base_trade_qty", 100))
            new_qty = max(1, int(base_qty * 0.5))
            self.config["base_trade_qty"] = new_qty
            return {"action": "decrease_risk", "new_qty": new_qty, "analysis": analysis_text}

        return None

    def apply_adaptation(self, use_azr: bool = True, limit: int = 20) -> Dict[str, Any]:
        """
        Combina le due modalit√† di adattamento:
          - Se use_azr=True e AZRAgent √® disponibile, prova prima l‚Äôadattamento LLM:
            se AZR suggerisce un cambiamento, lo applica e lo restituisce.
          - Altrimenti, (o se AZR non suggerisce nulla) esegue l‚Äôadattamento statistico
            tramite `apply_statistical_adaptation(limit)`.
        Restituisce sempre un dizionario con la decisione effettiva.
        """
        # Tentativo con AZRAgent
        if use_azr and self.azr:
            azr_result = self.analyze_with_azr(limit // 2)
            if azr_result is not None:
                return {"method": "azr", "decision": azr_result}

        # Altrimenti, adattamento statistico
        stat_result = self.apply_statistical_adaptation(limit)
        return {"method": "statistical", "decision": stat_result.get("decision", {})}


# ========== ESEMPIO DI UTILIZZO ==========
if __name__ == "__main__":
    # Simulazione di integrazione con ExperienceMemory e config
    from modules.experience.experience_memory import ExperienceMemory

    config = {
        "azr_profit_floor": 0.5,
        "base_trade_qty": 100,
        "use_azr": True
    }

    # Inizializza la memoria esperienziale
    em = ExperienceMemory({"experience_file": "memory/experience_log.json", "max_recent": 50})

    # Registra alcune esperienze di test
    em.record_experience(
        signal={"symbol": "EURUSD", "type": "BUY", "price": 1.1000},
        trade={"quantity": 100, "entry": 1.1000, "exit": 1.1020},
        result={"profit": 20, "currency": "USD"},
        feedback="Esecuzione corretta"
    )
    em.record_experience(
        signal={"symbol": "EURUSD", "type": "SELL", "price": 1.1050},
        trade={"quantity": 100, "entry": 1.1050, "exit": 1.1030},
        result={"profit": -20, "currency": "USD"},
        feedback="Stop troppo stretto"
    )

    # Inizializza l‚Äôanalizzatore
    analyzer = AZRAnalyzer(exp_memory=em, config=config)

    # Applichiamo l‚Äôadattamento (AZR o statistico)
    adaptation = analyzer.apply_adaptation(use_azr=True, limit=20)
    print("Adattamento applicato:", json.dumps(adaptation, indent=2))

### --- modules/experience/experience_memory.py --- ###
# modules/experience/experience_memory.py

"""
Modulo: experience_memory.py
Descrizione: Memoria evolutiva esperienziale per il sistema Mercurius‚àû.
Registra segnali, trade e risultati; usa internamente il backend JSON di LongTermMemory.
"""

import os
import json
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from memory.long_term_memory import LongTermMemory


class ExperienceMemory:
    """
    Strato di astrazione sopra LongTermMemory (backend JSON) con API di alto livello per il trading.

    - Se config contiene la chiave "experience_file", user√† quel file JSON (es. "memory/experience_log.json").
    - Mantiene, in aggiunta, una lista self.recent in memoria con le ultime N esperienze.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        - config: dizionario di configurazione. Se config["experience_file"] √® presente, verr√† usato come nome del file JSON.
        - In assenza di config, viene creato/riutilizzato ‚Äúmemory/experience_log.json‚Äù.
        """
        if config is None:
            config = {}

        # Decidiamo il percorso del file JSON: o quello fornito in config, oppure il default
        default_path = "memory/experience_log.json"
        self.storage_path: str = config.get("experience_file", default_path)

        # Creiamo la cartella se non esiste
        os.makedirs(os.path.dirname(self.storage_path), exist_ok=True)

        # üî• Patch: inizializziamo il limite PRIMA di caricare la storia!
        self._max_recent: int = config.get("max_recent", 50)

        # Inizializziamo LongTermMemory in modalit√† JSON, usando il file indicato
        self.store = LongTermMemory(
            backend="json",
            json_filename=self.storage_path
        )

        # Carichiamo la storia esistente dal file JSON, se esiste
        self.recent: List[Dict[str, Any]] = self._load_existing_history()

    def _load_existing_history(self) -> List[Dict[str, Any]]:
        """
        Legge il file JSON e restituisce la lista completa delle esperienze salvate.
        Popola self.recent con gli ultimi _max_recent elementi (o meno, se il file contiene di meno).
        """
        try:
            with open(self.storage_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if not isinstance(data, list):
                    return []
                # Manteniamo in recent solo gli ultimi max_recent elementi
                return data[-self._max_recent :]
        except FileNotFoundError:
            return []
        except json.JSONDecodeError:
            return []

    def record_experience(
        self,
        signal: Any,
        trade: Any,
        result: Any,
        feedback: Any,
        tags: Optional[List[str]] = None
    ) -> None:
        """
        Registra una nuova esperienza di trading, composta da:
         - signal: informazioni sul segnale (es. "BUY EURUSD a 1.1000")
         - trade: dettagli del trade (es. numero di lotti, entry, exit)
         - result: risultato (es. profitto/perdita)
         - feedback: eventuali commenti o valutazioni
         - tags: lista opzionale di stringhe per categorizzare l‚Äôesperienza (di default ["trading"])
        """
        if tags is None:
            tags = ["trading"]

        # Costruiamo il dizionario dell‚Äôesperienza, aggiungendo timestamp UTC
        exp: Dict[str, Any] = {
            "timestamp": datetime.utcnow().isoformat(),
            "signal": signal,
            "trade": trade,
            "result": result,
            "feedback": feedback,
            "tags": tags,
        }

        # Salviamo l‚Äôesperienza nel backend JSON di LongTermMemory
        self.store.save_experience(exp)

        # Aggiungiamo in cache
        self.recent.append(exp)
        # Se superiamo _max_recent, eliminiamo i pi√π vecchi
        if len(self.recent) > self._max_recent:
            self.recent = self.recent[-self._max_recent :]

    def get_recent_experiences(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Restituisce le ultime 'limit' esperienze direttamente dalla cache self.recent.
        Se limit > len(recent), restituisce tutta la cache.
        """
        return self.recent[-limit:]

    def get_all_experiences(self) -> List[Dict[str, Any]]:
        """
        Legge tutte le esperienze dal file JSON (tramite LongTermMemory.get_all).
        Se si vuole lavorare con la lista completa, anche quelle non in cache.
        """
        return self.store.get_all()

    def reset(self) -> None:
        """
        Svuota completamente la memoria delle esperienze:
         - Cancella la cache self.recent
         - Sovrascrive il file JSON con lista vuota
        """
        self.recent.clear()
        try:
            with open(self.storage_path, "w", encoding="utf-8") as f:
                json.dump([], f, indent=2)
        except Exception as e:
            raise RuntimeError(f"Impossibile resettare il file '{self.storage_path}': {e}")

    def summarize(self) -> Dict[str, Any]:
        """
        Restituisce un dizionario riepilogativo della memoria:
         - 'total': numero totale di esperienze salvate
         - 'last_timestamp': timestamp dell‚Äôultima esperienza (se esiste)
         - 'cached_recent': numero di esperienze in cache
        """
        all_exps = self.get_all_experiences()
        total = len(all_exps)
        last_ts = all_exps[-1]["timestamp"] if total > 0 else None
        return {
            "total": total,
            "last_timestamp": last_ts,
            "cached_recent": len(self.recent),
        }


# ===================== ESEMPIO DI UTILIZZO =====================
if __name__ == "__main__":
    # Esempio di configurazione: memorizza in 'memory/experience_log.json' e tiene 50 esperienze in cache
    config = {
        "experience_file": "memory/experience_log.json",
        "max_recent": 50
    }
    em = ExperienceMemory(config)

    # Registra un‚Äôesperienza di prova
    em.record_experience(
        signal={"symbol": "EURUSD", "type": "BUY", "price": 1.1000},
        trade={"lots": 0.1, "entry": 1.1000, "exit": 1.1020},
        result={"pnl": 20, "currency": "USD"},
        feedback="Buon segnale, gestione corretta dello stop."
    )

    # Ottieni le ultime 5 esperienze
    recenti = em.get_recent_experiences(limit=5)
    print("Ultime esperienze:", recenti)

    # Riassunto della memoria
    riassunto = em.summarize()
    print("Riepilogo:", riassunto)

    # Se vuoi resettare tutto, scommenta la riga seguente:
    # em.reset()

### --- modules/feedback_loop.py --- ###
# modules/feedback_loop.py

"""
Modulo: feedback_loop.py
Descrizione: Coordina il feedback tra moduli AI in Mercurius‚àû per auto-apprendimento, miglioramento codice e decisioni collettive.
"""

class FeedbackLoop:
    def __init__(self):
        self.logs = []

    def collect_feedback(self, source: str, message: str):
        entry = f"üîÅ [{source}] ‚Üí {message}"
        self.logs.append(entry)
        print(entry)

    def last_feedback(self, n=5):
        return "\n".join(self.logs[-n:])


# Test rapido
if __name__ == "__main__":
    fb = FeedbackLoop()
    fb.collect_feedback("AZR", "Codice semanticamente corretto.")
    fb.collect_feedback("GPT-4o", "Suggerita ottimizzazione per compatibilit√†.")
    print(fb.last_feedback())

### --- modules/fingpt_analyzer.py --- ###
# modules/fingpt_analyzer.py

"""
Modulo: fingpt_analyzer.py
Descrizione: Modulo di analisi NLP con FinGPT per generare segnali operativi da notizie e sentiment.
"""

class FinGPTAnalyzer:
    def __init__(self):
        self.last_signal = None

    def analyze_text(self, text: str) -> str:
        if "inflation" in text.lower():
            self.last_signal = "SELL"
        else:
            self.last_signal = "BUY"
        return f"üìâ Segnale da news: {self.last_signal}"


# Test rapido
if __name__ == "__main__":
    bot = FinGPTAnalyzer()
    print(bot.analyze_text("US Inflation data released - very high"))

### --- modules/finrl_agent.py --- ###
# modules/finrl_agent.py

"""
Modulo: finrl_agent.py
Descrizione: Wrapper per l‚Äôutilizzo di FinRL all‚Äôinterno di Mercurius‚àû. Consente addestramento e deploy di agenti RL per trading.
"""

class FinRLAgent:
    def __init__(self):
        self.model = None

    def train(self, data_path: str, model_type="ppo"):
        print(f"üìà Addestramento agente {model_type} su {data_path}")
        # Qui si collegher√† al training FinRL in futuro

    def predict(self, state):
        return "üß† Predizione (stub): buy/sell/hold"

    def evaluate(self):
        return "üìä Performance dell‚Äôagente: +4.2% (simulata)"


# Test
if __name__ == "__main__":
    agent = FinRLAgent()
    agent.train("data/btc.csv")
    print(agent.predict("BTC_state"))

### --- modules/freqtrade_bot.py --- ###
# modules/freqtrade_bot.py

"""
Modulo: freqtrade_bot.py
Descrizione: Struttura base per l'integrazione di strategie ML con Freqtrade.
"""

class FreqtradeBot:
    def __init__(self, strategy_name="MLBaseline"):
        self.strategy = strategy_name

    def run(self):
        return f"ü§ñ Esecuzione bot crypto con strategia: {self.strategy}"

    def update_strategy(self, new_name):
        self.strategy = new_name
        return f"üîÅ Strategia aggiornata a: {self.strategy}"


# Test
if __name__ == "__main__":
    bot = FreqtradeBot()
    print(bot.run())
    print(bot.update_strategy("SuperAI_MACD"))

### --- modules/gesture.py --- ###
"""
Modulo: gesture.py
Responsabilit√†: Stub logico per riconoscimento gesti e azioni gestuali
Autore: Mercurius‚àû Engineer Mode
"""

from typing import Dict


class GestureRecognizer:
    """
    Placeholder per riconoscimento gesti. Pu√≤ essere integrato con visione artificiale.
    """

    def recognize(self, input_frame) -> Dict:
        """
        Analizza un frame video e ritorna un gesto identificato (stub logico).
        """
        # In un sistema reale si integrerebbe OpenCV + ML per gesture spotting
        return {
            "gesture": "saluto",
            "action": "inizia_conversazione"
        }

    def interpret_gesture(self, gesture: str) -> Dict:
        """
        Converte un gesto in comando.
        """
        if gesture == "saluto":
            return {"action": "interagisci_utente"}
        elif gesture == "indicazione":
            return {"action": "raggiungi_destinazione", "context": {"destinazione": "indicata"}}
        else:
            return {"action": "ignora"}

### --- modules/goal_manager.py --- ###
"""
Modulo: goal_manager.py
Descrizione: Gestione di obiettivi (Goal) con priorit√†, stato e contesto.
Autore: Mercurius‚àû AI Engineer
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

@dataclass(order=False)  # disattiva ordinamento automatico
class Goal:
    name: str
    priority: int = 1
    context: Dict[str, Any] = field(default_factory=dict)
    done: bool = False
    status: str = "pending"  # possible values: pending, active, completed

class GoalManager:
    def __init__(self):
        self._goals: List[Goal] = []

    # --- API principale ---
    def add_goal(self, name: str, priority: int = 1, context: Optional[Dict[str, Any]] = None):
        self._goals.append(Goal(name=name, priority=priority, context=context or {}))
        # ordina per PRIORIT√Ä decrescente (priorit√† alta prima)
        self._goals.sort(key=lambda g: g.priority, reverse=True)

    def get_next_goal(self) -> Optional[Goal]:
        for g in self._goals:
            if not g.done:
                g.status = "active"  # aggiorna lo stato a active
                return g
        return None

    def complete_goal(self, name: str):
        for g in self._goals:
            if g.name == name:
                g.done = True
                g.status = "completed"
                break

    def list_goals(self) -> List[Goal]:
        return self._goals

    def active_goals(self) -> List[Goal]:
        """Restituisce la lista di goal attivi (status active e non done)."""
        return [g for g in self._goals if g.status == "active" and not g.done]

    def pending_goals(self) -> List[Goal]:
        """Ritorna i goal ancora in attesa di essere attivati."""
        return [g for g in self._goals if g.status == "pending" and not g.done]

    def all_goals(self) -> List[Dict[str, Any]]:
        """Rappresentazione serializzabile di tutti i goal."""
        return [
            {
                "name": g.name,
                "priority": g.priority,
                "context": g.context,
                "status": g.status,
                "done": g.done,
            }
            for g in self._goals
        ]

# Esempio di utilizzo
if __name__ == "__main__":
    gm = GoalManager()
    gm.add_goal("Scrivere report", priority=5)
    gm.add_goal("Debug sistema", priority=10)
    next_goal = gm.get_next_goal()
    print("Goal attivo:", next_goal)
    gm.complete_goal(next_goal.name)
    print("Lista goal:", gm.list_goals())

### --- modules/gpt4o_interface.py --- ###
"""
Modulo: gpt4o_interface.py
Descrizione: Comunicazione diretta con GPT-4o per validazione, riflessione e finalizzazione dei task AI.
"""

import openai
import os

class GPT4oInterface:
    def __init__(self, api_key: str = None, model: str = "gpt-4o"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        openai.api_key = self.api_key

    def ask(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1024) -> str:
        """
        Invia un prompt a GPT-4o e restituisce la risposta testuale.
        """
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response["choices"][0]["message"]["content"].strip()
        except Exception as e:
            return f"‚ùå Errore GPT-4o: {str(e)}"


# Test locale
if __name__ == "__main__":
    gpt = GPT4oInterface()
    reply = gpt.ask("Validami questa funzione Python: def somma(a, b): return a + b")
    print(reply)

### --- modules/gpt_engineer_wrapper.py --- ###
# modules/gpt_engineer_wrapper.py

"""
Modulo: gpt_engineer_wrapper.py
Descrizione: Interfaccia per pilotare GPT-Engineer via CLI o API, permettendo generazione autonoma di progetti e moduli.
"""

import subprocess
import os

class GPTEngineerWrapper:
    def __init__(self, project_path="generated_projects/", config_file=None):
        self.project_path = project_path
        self.config_file = config_file or "gpt_config.yaml"

    def generate_project(self, prompt: str) -> str:
        """
        Avvia GPT-Engineer per generare un progetto in base al prompt.
        """
        try:
            os.makedirs(self.project_path, exist_ok=True)
            with open("prompt.txt", "w") as f:
                f.write(prompt)

            result = subprocess.run(
                ["gpt-engineer", "."],
                cwd=self.project_path,
                capture_output=True,
                text=True
            )
            return result.stdout or "‚úÖ Generazione completata."
        except Exception as e:
            return f"‚ùå Errore GPT-Engineer: {e}"


# Test
if __name__ == "__main__":
    wrapper = GPTEngineerWrapper()
    print(wrapper.generate_project("Crea un'applicazione per il tracciamento del sonno"))

### --- modules/gpt_task_router.py --- ###


### --- modules/hf_tools_manager.py --- ###
# modules/hf_tools_manager.py

"""
Modulo: hf_tools_manager.py
Descrizione: Integrazione con HuggingFace Transformers Agents per interagire con strumenti locali.
"""

from transformers import HfAgent

class HFToolsManager:
    def __init__(self):
        self.agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")

    def use_tool(self, query: str) -> str:
        try:
            return self.agent.run(query)
        except Exception as e:
            return f"‚ùå Errore HuggingFace Tools: {str(e)}"

### --- modules/io_modules/mobile_connect.py --- ###
import cv2

def start_note10_sync(ip_camera_url="http://192.168.0.10:8080/video",
                      use_hotword=True,
                      request_qr_if_fail=True):
    print("üì≤ Avvio sincronizzazione con Note10+...")

    try:
        cap = cv2.VideoCapture(ip_camera_url)
        if not cap.isOpened():
            raise ConnectionError("‚ö†Ô∏è Impossibile connettersi alla camera IP")

        print("‚úÖ Stream ricevuto. Avvio visione artificiale...")

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            cv2.imshow("Note10+ Camera Feed", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

    except Exception as e:
        print(f"‚ùå Errore connessione: {e}")
        if request_qr_if_fail:
            print("üì∏ Generazione QR code per pairing alternativo (simulazione)")

### --- modules/leonai_bridge.py --- ###
# modules/leonai_bridge.py

"""
Modulo: leonai_bridge.py
Descrizione: Integrazione Leon AI per il controllo testuale/vocale del sistema operativo.
"""

import os

class LeonAI:
    def execute_command(self, command: str) -> str:
        try:
            result = os.popen(command).read()
            return f"‚úÖ Comando eseguito:\n{result}"
        except Exception as e:
            return f"‚ùå Errore: {str(e)}"


# Test
if __name__ == "__main__":
    leon = LeonAI()
    print(leon.execute_command("echo 'Mercurius √® operativo!'"))

### --- modules/llm/azr_reasoner.py --- ###
# modules/llm/azr_reasoner.py
"""
Modulo: azr_reasoner.py
Descrizione: Sistema di validazione logica del codice e dei pensieri AI secondo la logica AZR.
Utilizza analisi sintattica ed esecuzione controllata per determinare la validit√† di frammenti di codice.
"""
import ast
import traceback
from typing import Any

class AZRReasoning:
    def __init__(self):
        self.log = []

    def validate_with_azr(self, code: str) -> bool:
        """
        Analizza il codice ricevuto e ne valuta la coerenza logica e l'eseguibilit√†.
        """
        self.log.append(f"üîç Validating code:\n{code}")
        try:
            tree = ast.parse(code)
            self.log.append("‚úÖ AST parsing succeeded.")
        except SyntaxError as e:
            self.log.append(f"‚ùå Syntax Error: {e}")
            return False
        try:
            compiled = compile(tree, filename="<azr_check>", mode="exec")
            test_env: dict[str, Any] = {}
            exec(compiled, test_env)
            self.log.append("‚úÖ Execution succeeded.")
            return True
        except Exception as e:
            self.log.append(f"‚ö†Ô∏è Execution Error: {traceback.format_exc()}")
            return False

    def last_validation_log(self) -> str:
        """Restituisce le ultime voci di log della validazione."""
        return "\n".join(self.log[-5:])

# Funzione diretta per uso esterno
def validate_with_azr(code: str) -> bool:
    azr = AZRReasoning()
    return azr.validate_with_azr(code)

# Agente AZR: utilizza AZRReasoning per analizzare task di debug/logica
class AZRAgent:
    def __init__(self):
        self.azr = AZRReasoning()

    def analyze(self, text: str, context: dict = None) -> str:
        """
        Analizza il testo (es. codice) con la logica AZR e restituisce un responso.
        """
        code_to_check = text if context is None else context.get("code", text)
        success = self.azr.validate_with_azr(code_to_check)
        if success:
            return "‚úÖ AZR: codice valido e logica consistente."
        else:
            # Include dettagli di errore nel risultato
            error_log = self.azr.last_validation_log()
            return f"‚ùå AZR: rilevate criticit√† logiche.\nLog: {error_log}"

### --- modules/llm/chatgpt_interface.py --- ###
"""
Modulo: chatgpt_interface
Descrizione: Interfaccia con ChatGPT-4 per ragionamento linguistico e conversazione.
"""

import openai
import os

class ChatGPTAgent:
    def __init__(self, model_name="gpt-4"):
        self.model = model_name