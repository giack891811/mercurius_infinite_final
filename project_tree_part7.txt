• Riavvia il processo principale
"""

import subprocess
import sys
from pathlib import Path
from typing import Literal, Optional
from analytics.behavior_logger import BehaviorLogger

logger = BehaviorLogger()


class AutoUpdater:
    def __init__(self, repo_url: str, branch: str = "main"):
        self.repo_url = repo_url
        self.branch = branch
        self.repo_dir = Path(".").resolve()

    # ---------- public ----------
    def update(self, source: Literal["git", "package"] = "git", pkg_file: Optional[str] = None):
        if source == "git":
            return self._pull_git()
        if source == "package" and pkg_file:
            return self._extract_package(pkg_file)
        raise ValueError("Sorgente update non valida.")

    # ---------- internal ----------
    def _pull_git(self):
        cmd = ["git", "pull", self.repo_url, self.branch]
        res = subprocess.run(cmd, cwd=self.repo_dir, text=True, capture_output=True)
        logger.log("auto_update", {"method": "git", "stdout": res.stdout, "stderr": res.stderr})
        if res.returncode == 0:
            self._post_update()
            return "✅ Update da Git completato."
        return f"❌ Git pull error: {res.stderr}"

    def _extract_package(self, pkg_file: str):
        import tarfile, zipfile, shutil, tempfile

        tmp = Path(tempfile.mkdtemp())
        if pkg_file.endswith(".tar.gz"):
            with tarfile.open(pkg_file) as tar:
                tar.extractall(tmp)
        elif pkg_file.endswith(".zip"):
            with zipfile.ZipFile(pkg_file) as zf:
                zf.extractall(tmp)
        else:
            return "Formato pacchetto non supportato."

        # Copia sopra il codice
        for item in tmp.iterdir():
            target = self.repo_dir / item.name
            if target.exists():
                shutil.rmtree(target, ignore_errors=True)
            shutil.move(item, target)
        logger.log("auto_update", {"method": "package", "file": pkg_file})
        self._post_update()
        return "✅ Update da package completato."

    def _post_update(self):
        subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt", "-q"])
        logger.log("auto_update", {"action": "deps_installed"})

## utils/config_loader.py
"""
config_loader.py
================
Carica la configurazione da file YAML (mock per ora).
"""

def load_config(path):
    """
    Mock del caricamento configurazione.
    In un sistema reale, caricherebbe da YAML/JSON.
    """
    return {
        "symbols": ["AAPL", "TSLA", "GOOG"],
        "base_trade_qty": 100,
        "min_confidence": 0.55,
        "retrain_threshold": 0.65
    }

## utils/environment.py
"""
Modulo: environment.py
Responsabilità: Caricare e gestire le variabili di ambiente per Mercurius∞
Autore: Mercurius∞ Engineer Mode
"""

import os
from dotenv import load_dotenv

class Environment:
    """
    Carica il file .env e fornisce accesso centralizzato alle variabili di ambiente.
    """

    def __init__(self, dotenv_path: str = ".env"):
        self.loaded = False
        self.dotenv_path = dotenv_path
        self.load_environment()

    def load_environment(self):
        """
        Carica le variabili da .env nel sistema.
        """
        if os.path.exists(self.dotenv_path):
            load_dotenv(dotenv_path=self.dotenv_path)
            self.loaded = True
        else:
            raise FileNotFoundError(f"File .env non trovato in {self.dotenv_path}")

    def get(self, key: str, default=None):
        """
        Recupera una variabile d'ambiente.
        """
        return os.getenv(key, default)

    def get_openai_config(self) -> dict:
        return {
            "use_openai": self.get("USE_OPENAI") == "1",
            "api_key": self.get("OPENAI_API_KEY"),
            "chat_model": self.get("OPENAI_CHAT_MODEL"),
            "embed_model": self.get("OPENAI_EMBED_MODEL")
        }

    def get_web_monitor_credentials(self) -> dict:
        return {
            "user": self.get("WM_USER"),
            "password": self.get("WM_PASS")
        }

    def get_mcp_config(self) -> dict:
        return {
            "token": self.get("MCP_TOKEN"),
            "introspect_url": self.get("MCP_INTROSPECT_URL")
        }

    def get_mercurius_api_key(self) -> str:
        return self.get("MERCURIUS_API_KEY")

    def get_run_mode(self) -> str:
        """Restituisce la modalità operativa di AION."""
        return self.get("AION_RUN_MODE", "dialogic-autonomous")

## utils/logger.py
"""
logger.py
=========
Configurazione logging per il sistema Mercurius∞.
"""

import logging

def setup_logger(name="MercuriusLogger"):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
    return logger

def get_file_logger(name="MercuriusFileLogger", filename="mercurius.log"):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(logging.INFO)
        fh = logging.FileHandler(filename)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        logger.addHandler(fh)
    return logger

## utils/telemetry.py
"""
Modulo: telemetry.py
Responsabilità: Raccolta telemetria interna (risorse, moduli, stato sistema)
Autore: Mercurius∞ Engineer Mode
"""

import psutil
import platform
import os
import time
from typing import Dict


class Telemetry:
    """
    Fornisce dati interni sullo stato del sistema e delle risorse.
    """

    @staticmethod
    def system_info() -> Dict:
        return {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.machine(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_MB": round(psutil.virtual_memory().total / (1024 ** 2), 2),
        }

    @staticmethod
    def current_usage() -> Dict:
        mem = psutil.virtual_memory()
        return {
            "cpu_percent": psutil.cpu_percent(interval=0.5),
            "memory_used_MB": round(mem.used / (1024 ** 2), 2),
            "memory_percent": mem.percent,
            "active_processes": len(psutil.pids()),
            "uptime_sec": int(time.time() - psutil.boot_time())
        }

    @staticmethod
    def process_info(pid: int = os.getpid()) -> Dict:
        p = psutil.Process(pid)
        return {
            "pid": pid,
            "name": p.name(),
            "status": p.status(),
            "cpu_percent": p.cpu_percent(interval=0.5),
            "memory_MB": round(p.memory_info().rss / (1024 ** 2), 2),
            "threads": p.num_threads()
        }

## vision/__init__.py
from .ocr_module import extract_text_from_image

__all__ = ["extract_text_from_image"]

## vision/capture.py
# vision/capture.py

"""
Modulo: capture.py
Descrizione: Acquisizione video da IP Webcam per Mercurius∞. Utilizza OpenCV per estrarre frame in tempo reale.
"""

import cv2
import numpy as np
from typing import Optional


def get_frame_from_ip(ip_url: str) -> Optional[np.ndarray]:
    """
    Recupera un frame dall'indirizzo IP di una webcam.
    """
    cap = cv2.VideoCapture(ip_url)
    if not cap.isOpened():
        print("❌ Impossibile connettersi alla webcam IP.")
        return None

    ret, frame = cap.read()
    cap.release()

    if not ret:
        print("⚠️ Nessun frame catturato.")
        return None

    return frame

## vision/image_vision.py
# vision/image_vision.py

"""
Modulo: image_vision.py
Descrizione: Analisi di immagini statiche con OCR per l'estrazione di testo e concetti visuali.
Usa pytesseract per lettura OCR e OpenCV per preprocessing.
"""

import pytesseract
import cv2
from typing import List


class ImageVision:
    def __init__(self):
        pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"  # Aggiorna se necessario

    def read_text_from_image(self, image_path: str) -> str:
        """
        Estrae il testo da un'immagine tramite OCR.
        """
        try:
            image = cv2.imread(image_path)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            text = pytesseract.image_to_string(gray)
            return text.strip()
        except Exception as e:
            return f"[ERRORE OCR]: {e}"

    def extract_labels(self, image_path: str) -> List[str]:
        """
        Placeholder per estensione con modello YOLO/Vision per rilevamento oggetti.
        """
        return ["[analisi visiva non implementata]"]

## vision/ip_webcam_vision.py
"""YOLO based detection from IP webcam stream."""
import cv2
from vision.object_vision import ObjectVision

class IPWebcamVision(ObjectVision):
    def start_stream(self, ip_url: str):
        cap = cv2.VideoCapture(ip_url)
        if not cap.isOpened():
            raise RuntimeError("Cannot open IP camera")
        print("📡 Streaming IP webcam... press 'q' to quit")
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            results = self.model(frame, verbose=False)[0]
            annotated = self.box_annotator.annotate(
                scene=frame,
                detections=results.boxes
            )
            cv2.imshow("Mercurius∞ IP Cam", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

## vision/object_vision.py
# vision/object_vision.py

"""
Modulo: object_vision.py
Descrizione: Riconoscimento oggetti in tempo reale da webcam con YOLOv8.
"""

import cv2
import supervision as sv
from ultralytics import YOLO


class ObjectVision:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)
        self.box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=1, text_scale=0.5)

    def start_detection(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            raise RuntimeError("Camera non accessibile.")

        print("🎥 Avvio rilevamento oggetti YOLOv8... Premi 'q' per uscire.")

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = self.model(frame, verbose=False)[0]
            detections = sv.Detections.from_ultralytics(results)
            labels = [f"{c} {s:.2f}" for c, s in zip(results.names.values(), results.boxes.conf.cpu().numpy())]

            annotated = self.box_annotator.annotate(scene=frame, detections=detections, labels=labels)
            cv2.imshow("Mercurius∞ Vision", annotated)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

    def contextual_reaction(self, detected_items: list) -> str:
        if "person" in detected_items:
            return "👁️ Persona rilevata. Inizio monitoraggio ambientale."
        elif "keyboard" in detected_items:
            return "⌨️ Attività utente rilevata. Modalità lavoro attiva."
        else:
            return "🔍 Nessun oggetto prioritario rilevato."

## vision/ocr_module.py
"""
Modulo: ocr_module.py
Descrizione: Estrae testo da immagini tramite OCR (Tesseract o alternativa).
"""

try:
    import pytesseract
    from PIL import Image
except ImportError:
    raise ImportError("Modulo OCR non installato: usa `pip install pytesseract pillow`")

def extract_text_from_image(image_path: str) -> str:
    """
    Estrae il testo da un'immagine (jpg, png) usando OCR.
    """
    try:
        img = Image.open(image_path)
        text = pytesseract.image_to_string(img, lang='ita')  # o 'eng' se preferisci
        return text.strip()
    except Exception as e:
        return f"[❌ Errore OCR]: {str(e)}"

## vision/ocr_reader.py
# vision/ocr_reader.py

"""
Modulo: ocr_reader.py
Descrizione: Estrazione testi da immagini o webcam tramite OCR (Tesseract).
Supporta JPEG, PNG, flussi video.
"""

import pytesseract
import cv2


class OCRReader:
    def __init__(self):
        pass

    def read_text_from_image(self, path: str) -> str:
        img = cv2.imread(path)
        return pytesseract.image_to_string(img, lang="ita+eng")

    def read_from_camera(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            text = pytesseract.image_to_string(frame)
            print(f"[OCR] {text.strip()}")
            cv2.imshow("OCR Live", frame)
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
        cap.release()
        cv2.destroyAllWindows()

## vision/voice_trigger.py
# voice/voice_trigger.py

"""
Modulo: voice_trigger.py
Descrizione: Attivazione vocale tramite parola chiave "Hey Mercurius" utilizzando STT.
"""

import speech_recognition as sr


def listen_for_trigger(trigger_word: str = "hey mercurius") -> bool:
    """
    Ascolta il microfono per attivazione vocale.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎙️ Ascolto in corso... (parola chiave: 'Hey Mercurius')")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio).lower()
        print(f"🗣️ Rilevato: {text}")
        return trigger_word in text
    except sr.UnknownValueError:
        print("⚠️ Audio non riconosciuto.")
    except sr.RequestError:
        print("❌ Errore nel servizio di riconoscimento.")

    return False

## vision/yolo_handler.py
# vision/yolo_handler.py

"""
Modulo: yolo_handler.py
Descrizione: Riconoscimento oggetti con YOLOv5/YOLOv8 tramite OpenCV per Mercurius∞.
"""

from typing import List
import torch
import numpy as np

# Caricamento modello YOLO (richiede modello pre-addestrato disponibile localmente)
try:
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', trust_repo=True)
except Exception as e:
    print("⚠️ Errore nel caricamento del modello YOLO:", e)
    model = None


def detect_objects(image: np.ndarray) -> List[str]:
    """
    Rileva oggetti in un'immagine e restituisce le etichette.
    """
    if model is None:
        return []

    results = model(image)
    labels = results.pandas().xyxy[0]['name'].tolist()
    return labels

## voice/README.md
# 🎙️ Modulo Vocale – Attivazione

Gestisce input vocali e hotword per l'attivazione GENESIS.

## Componenti

- `activation_hook.py`: listener per "Hey Mercurius, attiva GENESIS"

## voice/__init__.py

## voice/coqui_tts.py
# voice/coqui_tts.py

"""
Modulo: coqui_tts.py
Descrizione: Sintesi vocale offline con Coqui TTS.
"""





## voice/elevenlabs_tts.py
# voice/elevenlabs_tts.py

"""
Modulo: elevenlabs_tts.py
Descrizione: Voce naturale con API ElevenLabs – stile Jarvis.
"""

import requests
import os

class ElevenLabsTTS:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("ELEVENLABS_API_KEY")

    def speak(self, text: str, voice_id="EXAVITQu4vr4xnSDxMaL"):
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
        headers = {
            "xi-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        json_data = {
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": {"stability": 0.5, "similarity_boost": 0.75}
        }
        response = requests.post(url, json=json_data, headers=headers)
        with open("output_11labs.wav", "wb") as f:
            f.write(response.content)

## voice/nari_tts.py
# voice/nari_tts.py

"""
Modulo: nari_tts.py
Descrizione: Sintesi vocale con il modello Nari Dia TTS.
"""

import soundfile as sf
from dia.model import Dia

class NariDiaTTS:
    def __init__(self, model_name="nari-labs/Dia-1.6B"):
        self.model = Dia.from_pretrained(model_name)

    def speak(self, text: str, output_path="output.wav"):
        output = self.model.generate(text)
        sf.write(output_path, output, 44100)

## voice/stt.py
# voice/stt.py

"""
Modulo: stt.py
Descrizione: Riconoscimento vocale da microfono in testo utilizzando SpeechRecognition (Google STT).
"""

import speech_recognition as sr


def transcribe_audio() -> str:
    """
    Converte l'audio acquisito da microfono in testo.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎧 In ascolto...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio, language="it-IT")
        print(f"📝 Riconosciuto: {text}")
        return text
    except sr.UnknownValueError:
        return "[Voce non riconosciuta]"
    except sr.RequestError:
        return "[Errore nel riconoscimento vocale]"

## voice/tts.py
# voice/tts.py (aggiornato)

"""
Modulo: tts.py
Descrizione: Sintesi vocale con fallback a gTTS se pyttsx3 non disponibile.
"""

try:
    import pyttsx3
    ENGINE = pyttsx3.init()
    USE_PYTTS = True
except ImportError:
    from gtts import gTTS
    import os
    USE_PYTTS = False


def speak(text: str):
    if USE_PYTTS:
        ENGINE.say(text)
        ENGINE.runAndWait()
    else:
        tts = gTTS(text=text, lang="it")
        file_path = "temp_audio.mp3"
        tts.save(file_path)
        os.system(f"start {file_path}" if os.name == "nt" else f"xdg-open {file_path}")

## voice/voice_bridge.py
"""voice_bridge.py
Output vocale tramite engine TTS locale.
"""

from __future__ import annotations

import pyttsx3

_engine = pyttsx3.init()


def speak(text: str) -> None:
    """Riproduce testo tramite sintesi vocale."""
    _engine.say(text)
    _engine.runAndWait()

## voice/voice_identity.py
# voice/voice_identity.py

"""
Modulo: voice_identity.py
Descrizione: Riconoscimento vocale degli speaker e saluti personalizzati.
"""

import os
import speech_recognition as sr
import json
from datetime import datetime
import hashlib


class VoiceIdentityManager:
    def __init__(self, db_path="logs/voice_profiles.json"):
        self.db_path = db_path
        if not os.path.exists(self.db_path):
            with open(self.db_path, "w") as f:
                json.dump({}, f)
        self.db = self._load_db()

    def _load_db(self):
        with open(self.db_path, "r") as f:
            return json.load(f)

    def identify_speaker(self, audio: sr.AudioData, recognizer: sr.Recognizer) -> str:
        try:
            text = recognizer.recognize_google(audio, language="it-IT")
            voice_id = self._voice_hash(audio)
            if voice_id in self.db:
                return f"🎙️ Bentornato {self.db[voice_id]['titolo']} {self.db[voice_id]['nome']}!"
            else:
                print("Voce non riconosciuta. Chi sei?")
                return self.register_new_voice(voice_id, text)
        except Exception:
            return "❌ Voce non comprensibile."

    def register_new_voice(self, voice_id: str, input_text: str) -> str:
        name = input_text.strip().split()[-1].capitalize()
        titolo = "Signor" if name[-1] not in "aeiou" else "Signora"
        self.db[voice_id] = {"nome": name, "titolo": titolo, "registrato": datetime.now().isoformat()}
        with open(self.db_path, "w") as f:
            json.dump(self.db, f, indent=2)
        return f"🎙️ Piacere {titolo} {name}, registrazione completata."

    def _voice_hash(self, audio: sr.AudioData) -> str:
        return hashlib.sha256(audio.get_raw_data()).hexdigest()[:16]

## voice/vosk_stt.py
# voice/vosk_stt.py

"""
Modulo: vosk_stt.py
Descrizione: Riconoscimento vocale locale con Vosk.
"""

import sounddevice as sd
import queue
import vosk
import json

class VoskSTT:
    def __init__(self, model_path="model"):
        self.model = vosk.Model(model_path)
        self.q = queue.Queue()

    def listen(self, duration=5, fs=16000):
        def callback(indata, frames, time, status):
            self.q.put(bytes(indata))
        with sd.RawInputStream(samplerate=fs, blocksize=8000, dtype="int16", channels=1, callback=callback):
            rec = vosk.KaldiRecognizer(self.model, fs)
            for _ in range(int(duration * fs / 8000)):
                data = self.q.get()
                if rec.AcceptWaveform(data):
                    res = json.loads(rec.Result())
                    return res.get("text", "")
            return ""

## voice/whisper_engine.py
# voice/whisper_engine.py

"""
Modulo: whisper_engine.py
Descrizione: Sintesi vocale inversa (STT) ad alta precisione con Whisper v3.
Supporta più lingue e trascrizione offline tramite modelli locali o OpenAI API.
"""

import os
import tempfile
import whisper


class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def transcribe_audio_file(self, audio_path: str, language: str = "it") -> str:
        result = self.model.transcribe(audio_path, language=language)
        return result.get("text", "[Nessun testo estratto]")

    def transcribe_microphone(self, duration=5, tmp_format="micro_input.wav") -> str:
        import sounddevice as sd
        import scipy.io.wavfile

        samplerate = 16000
        recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
        sd.wait()

        tmp_path = os.path.join(tempfile.gettempdir(), tmp_format)
        scipy.io.wavfile.write(tmp_path, samplerate, recording)
        return self.transcribe_audio_file(tmp_path)

## voice/whisper_stt.py
# voice/whisper_stt.py

"""
Modulo: whisper_stt.py
Descrizione: Trascrizione vocale avanzata multilingua tramite Whisper Large-V3.
"""

import whisper
import sounddevice as sd
import numpy as np
import tempfile
import wave

class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def record_audio(self, duration=5, fs=16000, device_index=None) -> str:
        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, device=device_index)
        sd.wait()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            wav_file = f.name
            with wave.open(wav_file, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(fs)
                wf.writeframes((audio * 32767).astype(np.int16).tobytes())
        return wav_file

    def transcribe_live_audio(self, duration=5, device_index=None) -> str:
        file_path = self.record_audio(duration, device_index=device_index)
        return self.transcribe_file(file_path)

    def transcribe_file(self, file_path: str) -> str:
        result = self.model.transcribe(file_path)
        return result["text"]

## voice/yolov8_engine.py
# vision/yolov8_engine.py

"""
Modulo: yolov8_engine.py
Descrizione: Riconoscimento in tempo reale di oggetti, volti e gesti con YOLOv8.
Supporta flussi da webcam o video file.
"""

import cv2
from ultralytics import YOLO


class VisionAI:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)

    def detect_from_image(self, image_path: str) -> list:
        results = self.model(image_path)
        return results[0].names

    def detect_from_webcam(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            results = self.model(frame)
            annotated = results[0].plot()
            cv2.imshow("Mercurius Vision", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

## voice/engine/coqui_tts.py
class CoquiTTS:
    def __init__(self):
        self.name = "CoquiTTS"

    def speak(self, phrase: str) -> str:
        return f"[{self.name}] Audio generato per: {phrase}"

## voice/engine/elevenlabs_tts.py
class ElevenLabsTTS:
    def __init__(self):
        self.name = "ElevenLabs"

    def synthesize(self, text: str, voice: str = "Jarvis") -> str:
        return f"[{self.name}] Sintesi vocale: '{text}' con voce {voice}"

## voice/engine/whisper_stt.py
class WhisperSTT:
    def __init__(self):
        self.name = "Whisper"

    def transcribe(self, audio_path: str) -> str:
        return f"[{self.name}] Trascrizione simulata del file: {audio_path}"

**ISTRUZIONI OPERATIVE:**
# Aggiungi qui le istruzioni operative per GPT-Engineer

### --- prompt_commands.txt --- ###
# Aggiungi qui le istruzioni operative per GPT-Engineer

### --- pyproject.toml --- ###
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

### --- rag/insight_rag.py --- ###
# rag/insight_rag.py

"""
Modulo: insight_rag.py
Descrizione: Sistema di archiviazione e recupero semantico (RAG) per concetti estratti da fonti multimodali.
"""

import os
import json
import uuid
from datetime import datetime
from sentence_transformers import SentenceTransformer, util

class InsightRAG:
    def __init__(self, db_path="logs/insight_memory.json"):
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.db_path = db_path
        self.embeddings = []
        self.memory = []
        self.load_memory()

    def load_memory(self):
        if os.path.exists(self.db_path):
            with open(self.db_path, "r") as f:
                self.memory = json.load(f)
                self.embeddings = [item["embedding"] for item in self.memory]

    def save_memory(self):
        with open(self.db_path, "w") as f:
            json.dump(self.memory, f, indent=2)

    def embed_insight(self, content: str):
        embedding = self.model.encode(content).tolist()
        entry = {
            "id": str(uuid.uuid4()),
            "timestamp": datetime.now().isoformat(),
            "text": content,
            "embedding": embedding
        }
        self.memory.append(entry)
        self.embeddings.append(embedding)
        self.save_memory()

    def query_concepts(self, question: str, top_k=3) -> list:
        query_emb = self.model.encode(question)
        scores = util.cos_sim(query_emb, self.embeddings)[0]
        top_indices = scores.argsort(descending=True)[:top_k]
        return [self.memory[idx] for idx in top_indices]

    def rank_relevance(self):
        return sorted(self.memory, key=lambda x: x["timestamp"], reverse=True)[:10]

### --- requirements.txt --- ###
absl-py==2.3.0
aiofiles==24.1.0
altair==5.5.0
altgraph==0.17.4
annotated-types==0.7.0
anyio==4.9.0
argbind==0.3.9
asttokens==3.0.0
attrs==25.3.0
audioread==3.0.1
bcrypt==4.3.0
beautifulsoup4==4.13.4
blinker==1.9.0
cachetools==5.5.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
click==8.2.1
colorama==0.4.6
comtypes==1.4.11
contourpy==1.3.2
cryptography==45.0.3
cssselect==1.3.0
cycler==0.12.1
decorator==5.2.1
defusedxml==0.7.1
descript-audio-codec==1.0.0
descript-audiotools==0.7.2
diskcache==5.6.3
distro==1.9.0
docstring_parser==0.16
einops==0.8.1
executing==2.2.0
fastapi==0.115.12
ffmpy==0.5.0
filelock==3.18.0
fire==0.7.0
flatten-dict==0.4.2
fonttools==4.58.1
fsspec==2025.5.1
future==1.0.0
gitdb==4.0.12
GitPython==3.1.44
gradio==5.32.0
gradio_client==1.10.2
greenlet==3.2.2
groovy==0.1.2
grpcio==1.71.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.32.3
idna==3.10
imageio==2.37.0
imageio-ffmpeg==0.6.0
importlib_resources==6.5.2
iniconfig==2.1.0
ipython==9.3.0
ipython_pygments_lexers==1.1.1
jedi==0.19.2
Jinja2==3.1.6
jiter==0.10.0
joblib==1.5.1
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
julius==0.2.7
kiwisolver==1.4.8
langchain==0.3.25
langchain-core==0.3.63
langchain-text-splitters==0.3.8
langsmith==0.3.43
lazy_loader==0.4
librosa==0.11.0
llama_cpp_python==0.3.9
llvmlite==0.44.0
lxml==5.4.0
lxml_html_clean==0.4.2
Markdown==3.8
markdown-it-py==3.0.0
markdown2==2.5.3
MarkupSafe==3.0.2
matplotlib==3.10.3
matplotlib-inline==0.1.7
mdurl==0.1.2
-e git+https://github.com/giack891811/mercurius_infinite_final.git@92670b6422489a439fa7e6e9c4a47cdaa4aef3e1#egg=mercurius_infinite
more-itertools==10.7.0
moviepy==2.2.1
mpmath==1.3.0
msgpack==1.1.0
-e git+https://github.com/nari-labs/dia.git@2811af1c5f476b1f49f4744fabf56cf352be21e5#egg=nari_tts
narwhals==1.41.0
networkx==3.5
numba==0.61.2
numpy==1.26.4
openai==1.82.1
openai-whisper==20240930
opencv-python==4.11.0.86
orjson==3.10.18
packaging==24.2
pandas==2.2.3
paramiko==3.5.1
parso==0.8.4
pefile==2023.2.7
pillow==11.2.1
platformdirs==4.3.8
pluggy==1.6.0
pooch==1.8.2
proglog==0.1.12
prometheus_client==0.22.0
prompt_toolkit==3.0.51
protobuf==6.31.1
psutil==7.0.0
pure_eval==0.2.3
py-cpuinfo==9.0.0
pyarrow==20.0.0
pycparser==2.22
pydantic==2.11.5
pydantic_core==2.33.2
pydeck==0.9.1
pydub==0.25.1
Pygments==2.19.1
pyinstaller==6.13.0
pyinstaller-hooks-contrib==2025.4
PyJWT==2.10.1
pyloudnorm==0.1.1
PyNaCl==1.5.0
pyparsing==3.2.3
pypiwin32==223
PyQt5==5.15.11
PyQt5-Qt5==5.15.2
PyQt5_sip==12.17.0
pystoi==0.4.1
pytesseract==0.3.13
pytest==8.3.5
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
pyttsx3==2.98
pytube==15.0.0
pytz==2025.2
pywin32==310
pywin32-ctypes==0.2.3
PyYAML==6.0.2
randomname==0.2.1
readability-lxml==0.8.4.1
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
requests-toolbelt==1.0.0
rich==14.0.0
rpds-py==0.25.1
ruff==0.11.12
safehttpx==0.1.6
safetensors==0.5.3
scikit-learn==1.6.1
scipy==1.15.3
semantic-version==2.10.0
sentence-transformers==4.1.0
setuptools==80.9.0
shellingham==1.5.4
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
sounddevice==0.5.2
soundfile==0.13.1
soupsieve==2.7
soxr==0.5.0.post1
SpeechRecognition==3.14.3
SQLAlchemy==2.0.41
srt==3.5.3
stack-data==0.6.3
starlette==0.46.2
streamlit==1.45.1
supervision==0.25.1
sympy==1.13.1
tenacity==9.1.2
tensorboard==2.19.0
tensorboard-data-server==0.7.2
termcolor==3.1.0
threadpoolctl==3.6.0
tiktoken==0.9.0
tokenizers==0.21.1
toml==0.10.2
tomlkit==0.13.2
torch==2.2.2+cu121
torch-stoi==0.2.3
torchaudio==2.2.2+cu121
torchvision==0.17.2+cu121
tornado==6.5.1
tqdm==4.67.1
traitlets==5.14.3
transformers==4.52.4
triton-windows==3.2.0.post18
typer==0.16.0
typing-inspection==0.4.1
typing_extensions==4.13.2
tzdata==2025.2
ultralytics==8.3.146
ultralytics-thop==2.0.14
urllib3==2.4.0
uvicorn==0.34.2
vosk==0.3.45
watchdog==6.0.0
wcwidth==0.2.13
websockets==15.0.1
Werkzeug==3.1.3
wheel==0.45.1
whisper==1.1.10
zstandard==0.23.0

scapy
pybluez
pywhatkit
pika==1.3.2

### --- safety/__init__.py --- ###


### --- safety/audit_logger.py --- ###
# safety/audit_logger.py
"""
Modulo: audit_logger
Descrizione: Log di audit immutabile per registrare azioni, decisioni e override.
"""

from pathlib import Path
from datetime import datetime
import json

AUDIT_FILE = Path("logs/audit_log.jsonl")
AUDIT_FILE.parent.mkdir(parents=True, exist_ok=True)


def audit(event_type: str, details: dict):
    entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "type": event_type,
        "details": details,
    }
    with open(AUDIT_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(entry) + "\n")

### --- safety/human_override.py --- ###
# safety/human_override.py
"""
Modulo: human_override
Descrizione: Consente all'operatore umano di confermare/bloccare azioni critiche.
"""

from typing import Callable, Any

class HumanOverride:
    def __init__(self, interactive: bool = True):
        self.interactive = interactive

    def confirm(self, message: str) -> bool:
        """
        Chiede conferma all'utente per procedere con un'azione sensibile.
        In modalità non interattiva ritorna sempre False (azione bloccata).
        """
        if not self.interactive:
            print(f"⛔ Override: azione '{message}' bloccata (non-interactive).")
            return False
        reply = input(f"⚠️ Confermi azione critica? '{message}' (y/n): ").strip().lower()
        return reply in {"y", "yes"}

    def guard(self, message: str) -> Callable:
        """
        Decoratore per funzioni che necessitano approvazione umana.
        """

        def decorator(func: Callable) -> Callable:
            def wrapper(*args, **kwargs) -> Any:
                if self.confirm(message):
                    return func(*args, **kwargs)
                else:
                    print("🚫 Azione annullata dall'operatore.")
                    return None

            return wrapper

        return decorator

### --- safety/policies.yaml --- ###
- action: block
  name: no_secrets
  rule: password=
- action: block
  name: no_secrets
  rule: password=

### --- safety/policy_manager.py --- ###
# safety/policy_manager.py
"""
Modulo: policy_manager
Descrizione: Gestisce le policy etiche, di sicurezza e privacy per Mercurius∞.
Le policy sono definite in YAML ed estendibili in runtime.
"""

import yaml
from pathlib import Path
from typing import Dict, Any, List

POLICY_FILE = Path("safety/policies.yaml")


class PolicyManager:
    def __init__(self):
        self.policies: List[Dict[str, Any]] = []
        self.load_policies()

    # ---------- Public API ----------
    def load_policies(self) -> None:
        if POLICY_FILE.exists():
            self.policies = yaml.safe_load(POLICY_FILE.read_text(encoding="utf-8")) or []
        else:
            self.policies = []

    def add_policy(self, name: str, rule: str, action: str = "block") -> None:
        self.policies.append({"name": name, "rule": rule, "action": action})
        self._save()

    def check(self, text: str) -> Dict[str, Any] | None:
        """
        Ritorna la policy violata se text ne infrange una.
        """
        for pol in self.policies:
            if pol["rule"].lower() in text.lower():
                return pol
        return None

    # ---------- Private ----------
    def _save(self):
        POLICY_FILE.parent.mkdir(exist_ok=True, parents=True)
        yaml.safe_dump(self.policies, POLICY_FILE.open("w", encoding="utf-8"))

### --- safety/safety_guard.py --- ###
# safety/safety_guard.py
"""
Modulo: safety_guard
Descrizione: Punto di ingresso globale per controlli policy + human override + audit.
"""

from safety.policy_manager import PolicyManager
from safety.human_override import HumanOverride
from safety.audit_logger import audit


class SafetyGuard:
    def __init__(self, interactive=True):
        self.policy_mgr = PolicyManager()
        self.override = HumanOverride(interactive=interactive)

    def filter_text(self, text: str) -> str | None:
        """
        Applica policy. Se violazione -> chiede override umano.
        Ritorna testo se permesso, None se bloccato.
        """
        violation = self.policy_mgr.check(text)
        if violation:
            audit("policy_violation", {"rule": violation["name"], "text": text})
            allowed = self.override.confirm(
                f"Violazione '{violation['name']}'. Consentire comunque?"
            )
            if not allowed:
                print("⛔ Bloccato da SafetyGuard.")
                return None
        return text

### --- scheduler/auto_scheduler.py --- ###
"""
auto_scheduler.py
=================
Modulo per programmazione automatica di esecuzioni trading e test.

Basato su threading + pianificazione in tempo reale:
- task ciclici
- esecuzioni ritardate
- notifiche pianificate
"""

import threading
import time
from datetime import datetime, timedelta


class AutoScheduler:
    def __init__(self):
        self.tasks = []

    def schedule_task(self, task_func, delay_sec=5, repeat=False, interval_sec=60, name=None):
        """Programma un task con delay e ripetizione opzionale."""
        task = {
            "name": name or task_func.__name__,
            "function": task_func,
            "delay": delay_sec,
            "repeat": repeat,
            "interval": interval_sec,
            "next_run": datetime.now() + timedelta(seconds=delay_sec)
        }
        self.tasks.append(task)

    def run(self):
        """Avvia il ciclo continuo di pianificazione."""
        def loop():
            while True:
                now = datetime.now()
                for task in self.tasks:
                    if now >= task["next_run"]:
                        try:
                            print(f"🕒 Esecuzione task: {task['name']}")
                            task["function"]()
                        except Exception as e:
                            print(f"❌ Errore nel task {task['name']}: {e}")
                        if task["repeat"]:
                            task["next_run"] = now + timedelta(seconds=task["interval"])
                        else:
                            self.tasks.remove(task)
                time.sleep(1)

        threading.Thread(target=loop, daemon=True).start()

    def list_tasks(self):
        """Lista dei task programmati."""
        return [(t["name"], t["next_run"]) for t in self.tasks]

    def clear(self):
        self.tasks.clear()

### --- scheduler/task_registry.py --- ###
"""
task_registry.py
================
Raccolta di task Mercurius∞ registrabili nello scheduler:
- simulazioni
- test
- azioni periodiche
"""

from core.pipeline_controller import PipelineController
from utils.config_loader import load_config

class TaskRegistry:
    def __init__(self):
        self.config = load_config("config/config.yaml")
        self.pipeline = PipelineController(self.config)

    def simulate_trading_session(self):
        """Task: esegue una sessione completa simulata."""
        print("▶️ Simulazione trading session")
        self.pipeline.run_batch_session()

    def multiple_sessions(self, count=3):
        """Task: n simulazioni."""
        print(f"▶️ Avvio {count} sessioni simulate")
        self.pipeline.simulate_multiple_sessions(n=count)

    def health_check(self):
        """Task diagnostico semplificato."""
        print("✅ Mercurius∞ pronto. Config:", self.config.get("symbols", []))

### --- scripts/activate_hud_mobile.py --- ###
"""Script per avviare l'interfaccia mobile HUD."""
from modules.mobile.note_interface import start_mobile_hud


def main():
    start_mobile_hud()


if __name__ == "__main__":  # pragma: no cover
    main()

### --- scripts/aion_boot.py --- ###
import os
import sys

# Aggiunge la root del progetto al path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from core.orchestrator import Orchestrator
from utils.environment import Environment


def main():
    print("🧬 Avvio AION – Modalità: dialogic-autonomous")

    env = Environment()
    os.environ["RUN_MODE"] = "dialogic-autonomous"
    print(f"🌐 AION_RUN_MODE = {env.get('RUN_MODE')}")

    orchestrator = Orchestrator()

    print("🔍 Eseguo self-check...")
    orchestrator.run_self_check(path=".")

    print("🧠 Eseguo missione #SELF_MISSION...")
    orchestrator.execute_mission("#SELF_MISSION")

    try:
        from deployment.aion_api import start_api
        import threading
        threading.Thread(target=start_api, daemon=True).start()
        print("🌐 Aion API server avviato sulla porta 8000")
    except Exception as exc:
        print(f"⚠️ Avvio Aion API fallito: {exc}")

    try:
        from modules.voice_bridge.voice_loop import start_listening
        print("🎙️ Voice recognition attiva...")
        start_listening()
    except ImportError:
        print("⚠️ Voice module non disponibile")

    try:
        from modules.dashboard import launch_dashboard
        print("🖥️ Avvio dashboard...")
        launch_dashboard()
    except ImportError:
        print("⚠️ Dashboard non trovata")

    print("✅ AION operativo. In ascolto comandi.")

if __name__ == "__main__":
    main()

### --- scripts/bootstrap_codex.py --- ###
"""
Script iniziale per ambiente Codex.
Attiva Mercurius, esegue check e lancia missione di completamento.
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))
from core.orchestrator import Orchestrator

def main():
    print("🚀 Avvio di Mercurius∞...")

    # Inizializza Orchestrator
    orchestrator = Orchestrator()

    # Esegue controllo e inizializzazione sistema
    print("🔍 Analisi interna...")
    orchestrator.run_self_check(path=".")

    # Attiva missione automatica di completamento
    print("🧠 Attivazione SELF_MISSION...")
    orchestrator.execute_mission("#SELF_MISSION")

if __name__ == "__main__":
    main()

### --- scripts/build_prompt.py --- ###
#!/usr/bin/env python3
"""Create prompt.txt combining project tree and user commands."""
from pathlib import Path

ROOT_DIR = Path(__file__).resolve().parents[1]
OUTPUT_FILE = ROOT_DIR / 'prompt.txt'
TREE_FILE = ROOT_DIR / 'project_tree.txt'
COMMANDS_FILE = ROOT_DIR / 'prompt_commands.txt'


def read_file(path: Path) -> str:
    return path.read_text(encoding='utf-8') if path.exists() else ''


def main():
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        out.write('**STRUTTURA E FILE DEL PROGETTO:**\n')
        out.write(read_file(TREE_FILE))
        out.write('\n**ISTRUZIONI OPERATIVE:**\n')
        out.write(read_file(COMMANDS_FILE))


if __name__ == '__main__':
    main()

### --- scripts/mercurius_control.py --- ###
import argparse
import os
import subprocess
from pathlib import Path

PID_FILE = Path("mercurius.pid")


def start_system() -> None:
    if PID_FILE.exists():
        print("Mercurius∞ sembra già in esecuzione.")
        return
    process = subprocess.Popen(["python", "scripts/aion_boot.py"])
    PID_FILE.write_text(str(process.pid))
    print(f"Mercurius∞ avviato con PID {process.pid}")


def stop_system() -> None:
    if not PID_FILE.exists():
        print("Mercurius∞ non risulta attivo.")
        return
    pid = int(PID_FILE.read_text())
    try:
        os.kill(pid, 9)
        print("Mercurius∞ arrestato.")
    except ProcessLookupError:
        print("Processo non trovato.")
    PID_FILE.unlink(missing_ok=True)


def main() -> None:
    parser = argparse.ArgumentParser(description="Gestione start/stop di Mercurius∞")
    parser.add_argument("action", choices=["start", "stop"], help="Azione da eseguire")
    args = parser.parse_args()
    if args.action == "start":
        start_system()
    else:
        stop_system()


if __name__ == "__main__":
    main()

### --- scripts/prompt_panel.py --- ###
import argparse
from modules.reasoner_dispatcher import dispatch_to_reasoner


def interactive_panel() -> None:
    print("Mercurius Prompt Panel - digita 'exit' per uscire")
    while True:
        try:
            prompt = input("Prompt> ").strip()
            if prompt.lower() in {"exit", "quit"}:
                break
            if prompt:
                response = dispatch_to_reasoner(prompt)
                print(response)
        except KeyboardInterrupt:
            break


def main() -> None:
    parser = argparse.ArgumentParser(description="Mercurius Prompt Panel")
    parser.add_argument("--prompt", help="Prompt singolo da inviare", default=None)
    args = parser.parse_args()
    if args.prompt:
        print(dispatch_to_reasoner(args.prompt))
    else:
        interactive_panel()


if __name__ == "__main__":
    main()

### --- scripts/start_genesis.py --- ###
"""
🚀 scripts/start_genesis.py
Script di avvio manuale per la modalità GENESIS – attiva il sistema AI Mercurius∞
"""

from core.orchestrator import Orchestrator
from core.self_mission import genesis_directive

def start():
    genesis_directive()
    orchestrator = Orchestrator()
    orchestrator.activate_genesis()

if __name__ == "__main__":
    start()

### --- scripts/update_project_tree.py --- ###
#!/usr/bin/env python3
"""Generate project_tree.txt with the repository tree and file previews."""
import os
from pathlib import Path

ROOT_DIR = Path(__file__).resolve().parents[1]
OUTPUT_FILE = ROOT_DIR / 'project_tree.txt'
MAX_LINES = 100
# File extensions considered text and included in preview
TEXT_EXTENSIONS = {
    '.py', '.json', '.md', '.txt', '.yml', '.yaml', '.ini', '.cfg', '.toml', '.js', '.ts'
}


def generate_tree():
    tree_lines = []
    text_files = []
    for root, dirs, files in os.walk(ROOT_DIR):
        if '.git' in dirs:
            dirs.remove('.git')
        dirs.sort()
        files.sort()
        level = Path(root).relative_to(ROOT_DIR).parts
        indent = '    ' * len(level)
        tree_lines.append(f"{indent}{Path(root).name}/")
        for name in files:
            tree_lines.append(f"{indent}    {name}")
            ext = Path(name).suffix.lower()
            if ext in TEXT_EXTENSIONS:
                text_files.append(Path(root) / name)
    return tree_lines, text_files


def read_snippet(file_path: Path):
    lines = []
    truncated = False
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for idx, line in enumerate(f):
                if idx >= MAX_LINES:
                    truncated = True
                    break
                lines.append(line.rstrip('\n'))
    except Exception as exc:
        lines.append(f"[Errore lettura: {exc}]")
    return lines, truncated


def main():
    tree_lines, text_files = generate_tree()
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out:
        out.write('PROJECT TREE\n')
        out.write('\n'.join(tree_lines))
        out.write('\n\nFILE PREVIEW\n')
        for file in text_files:
            rel = file.relative_to(ROOT_DIR)
            out.write(f"\n## {rel}\n")
            lines, truncated = read_snippet(file)
            for l in lines:
                out.write(l + '\n')
            if truncated:
                out.write('[TRONCATO]\n')


if __name__ == '__main__':
    main()

### --- security/code_signer.py --- ###
# security/code_signer.py

"""
Modulo: code_signer.py
Autore: Mercurius∞
Descrizione: Sistema di firma digitale SHA256 per tutti i file generati, con registrazione in log e firma visibile in coda al file.
"""

import hashlib
import json
from datetime import datetime
import os


class CodeSigner:
    def __init__(self, author="Mercurius∞", log_path="logs/code_signatures.json"):
        self.author = author
        self.log_path = log_path
        self.signatures = self.load_signatures()

    def load_signatures(self) -> dict:
        if os.path.exists(self.log_path):
            try:
                with open(self.log_path, "r") as f:
                    return json.load(f)
            except json.JSONDecodeError:
                return {}
        return {}

    def save_signatures(self):
        with open(self.log_path, "w") as f:
            json.dump(self.signatures, f, indent=2)

    def generate_signature_block(self, content: str) -> str:
        sha = hashlib.sha256(content.encode()).hexdigest()
        timestamp = datetime.utcnow().isoformat()
        return f"\n\n# --SIGNATURE--\n# SHA256: {sha}\n# SignedAt: {timestamp}\n# By: {self.author}\n"

    def generate_hash(self, file_path: str) -> str:
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(block)
        return sha256_hash.hexdigest()

    def sign_file(self, file_path: str) -> str:
        # Legge contenuto originale
        with open(file_path, "r") as f:
            content = f.read()

        # Genera firma visibile
        signature_block = self.generate_signature_block(content)

        # Aggiunge firma al file
        with open(file_path, "a") as f:
            f.write(signature_block)

        # Log firma in file JSON
        file_hash = self.generate_hash(file_path)
        self.signatures[file_path] = {
            "file": file_path,
            "hash": file_hash,
            "timestamp": datetime.utcnow().isoformat(),
            "author": self.author
        }
        self.save_signatures()
        return f"✅ File firmato e registrato: {file_path}"

    def verify_signature(self, file_path: str) -> bool:
        if file_path not in self.signatures:
            return False
        current_hash = self.generate_hash(file_path)
        stored_hash = self.signatures[file_path]["hash"]
        return current_hash == stored_hash

    def report_signature_status(self, file_path: str) -> str:
        if self.verify_signature(file_path):
            info = self.signatures[file_path]
            return (f"🔐 Firma verificata:\n"
                    f"🗂 File: {info['file']}\n"
                    f"🕒 Timestamp: {info['timestamp']}\n"
                    f"🧑‍💻 Autore: {info['author']}\n"
                    f"🔑 SHA256: {info['hash']}")
        return "❌ Firma non valida o assente."

### --- security/code_verifier.py --- ###
# security/code_verifier.py

"""
Modulo: code_verifier.py
Descrizione: Verifica la firma SHA256 di un file generato per garantirne l'integrità.
Estrae blocco firma e confronta l'hash del codice.
"""

import hashlib


class CodeVerifier:
    def verify_file(self, filepath: str) -> str:
        with open(filepath, "r") as f:
            lines = f.readlines()

        try:
            idx = lines.index("# --SIGNATURE--\n")
            code = "".join(lines[:idx])
            original_hash = [l for l in lines[idx:] if "SHA256" in l][0].split(":")[1].strip()
            actual_hash = hashlib.sha256(code.encode()).hexdigest()

            if actual_hash == original_hash:
                return "✅ Firma valida – contenuto integro"
            else:
                return "❌ Firma NON valida – file modificato"
        except Exception:
            return "⚠️ Firma non trovata o incompleta"

### --- security/gpg_support.py --- ###
# security/gpg_support.py

"""
Modulo: gpg_support.py
Descrizione: Firma/verifica file tramite GPG. Richiede GnuPG installato.
"""

import subprocess


class GPGSupport:
    def gpg_sign_file(self, path: str, key_id: str) -> str:
        cmd = f"gpg --default-key {key_id} --output {path}.sig --detach-sig {path}"
        try:
            subprocess.run(cmd, shell=True, check=True)
            return f"✅ File firmato con GPG: {path}.sig"
        except Exception as e:
            return f"❌ Errore GPG: {e}"

    def gpg_verify(self, path: str) -> str:
        cmd = f"gpg --verify {path}.sig {path}"
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            return result.stdout + result.stderr
        except Exception as e:
            return f"❌ Verifica fallita: {e}"

### --- security/pairing_manager.py --- ###
# security/pairing_manager.py

"""
Modulo: pairing_manager.py
Descrizione: Gestione pairing sicuro con utente tramite QR code o password vocale.
"""

import qrcode
from voice.stt import transcribe_audio


def generate_qr_pairing_link(link: str, filename: str = "pairing_qr.png") -> None:
    """
    Genera un QR code da un link e lo salva come immagine.
    """
    img = qrcode.make(link)
    img.save(filename)
    print(f"✅ QR generato: {filename}")


def pair_with_user(method: str = "qr") -> bool:
    """
    Esegue il pairing con l'utente. Metodo supportato: 'qr', 'voice'
    """
    if method == "qr":
        generate_qr_pairing_link("https://mercurius.local/pair")
        return True

    elif method == "voice":
        print("🔒 Pronuncia la password vocale:")
        spoken = transcribe_audio().lower()
        return "mercurius autorizza" in spoken

    return False

### --- seleziona_cartella.py --- ###
from tkinter import Tk, filedialog

root = Tk()
root.withdraw()  # Nasconde la finestra principale
folder_path = filedialog.askdirectory(title="Seleziona una cartella")

print("📁 Cartella selezionata:", folder_path)

### --- sensors/environment_analyzer.py --- ###
# sensors/environment_analyzer.py

"""
Modulo: environment_analyzer.py
Descrizione: Analizza il livello di rumore ambientale e cambiamenti visivi dalla webcam.
Serve per attivare modalità silenziosa, reattiva o sicurezza.
"""

import cv2
import numpy as np
import sounddevice as sd


class EnvironmentAnalyzer:
    def __init__(self, camera_index=0):
        self.cam = cv2.VideoCapture(camera_index)

    def get_audio_level(self, duration=1) -> float:
        recording = sd.rec(int(duration * 16000), samplerate=16000, channels=1)
        sd.wait()
        return float(np.abs(recording).mean())

    def detect_motion(self) -> str:
        ret, frame1 = self.cam.read()
        ret, frame2 = self.cam.read()
        if not ret:
            return "no_camera"

        diff = cv2.absdiff(frame1, frame2)
        gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        _, thresh = cv2.threshold(gray, 25, 255, cv2.THRESH_BINARY)
        motion = np.sum(thresh) / 255
        if motion > 1000:
            return "movimento sospetto"
        return "nessun movimento"

### --- sensors/sensor_hub.py --- ###
"""sensor_hub.py
Cattura schermo e microfono con semplice hotword detection.
Espone stream FastAPI per integrazione multisensoriale.
"""

from __future__ import annotations

import io
from typing import Generator
from fastapi import FastAPI, Response
import uvicorn
from mss import mss
from PIL import Image
import speech_recognition as sr

app = FastAPI(title="Sensor Hub")


def _grab_screen() -> bytes:
    with mss() as sct:
        shot = sct.grab(sct.monitors[0])
        img = Image.frombytes("RGB", shot.size, shot.rgb)
        buf = io.BytesIO()
        img.save(buf, format="JPEG")
        return buf.getvalue()


def capture_screen_stream() -> bytes:
    """Restituisce un frame dello schermo in JPEG."""
    return _grab_screen()


@app.get("/vision")
def vision() -> Response:
    frame = _grab_screen()
    return Response(content=frame, media_type="image/jpeg")


def _recognize_speech(duration: int = 3) -> str:
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        audio = recognizer.record(source, duration=duration)
    try:
        return recognizer.recognize_google(audio, language="it-IT")
    except Exception:
        return ""


def listen_microphone(duration: int = 3) -> str:
    """Ritorna testo dal microfono."""
    return _recognize_speech(duration)


@app.get("/audio")
def audio() -> dict:
    text = _recognize_speech()
    return {"text": text}


def detect_hotword(hotword: str = "hey mercurius", duration: int = 3) -> bool:
    text = _recognize_speech(duration).lower()
    return hotword.lower() in text


@app.get("/hotword")
def hotword() -> dict:
    return {"detected": detect_hotword()}


def start_sensor_server(host: str = "0.0.0.0", port: int = 5124) -> None:
    """Avvia il server dei sensori."""
    uvicorn.run(app, host=host, port=port)

### --- setup.py --- ###
from setuptools import setup, find_packages

setup(
    name="mercurius-infinite",
    version="1.0.0",
    packages=find_packages(),
    include_package_data=True,
    install_requires=[
        "altair==5.5.0",
        "altgraph==0.17.4",
        "annotated-types==0.7.0",
        "anyio==4.9.0",
        "attrs==25.3.0",
        "blinker==1.9.0",
        "cachetools==5.5.2",
        "certifi==2025.4.26",
        "cffi==1.17.1",
        "charset-normalizer==3.4.2",
        "click==8.2.1",
        "colorama==0.4.6",
        "comtypes==1.4.11",
        "contourpy==1.3.2",
        "cycler==0.12.1",
        "decorator==5.2.1",
        "defusedxml==0.7.1",
        "distro==1.9.0",
        "filelock==3.18.0",
        "fonttools==4.58.1",
        "fsspec==2025.5.1",
        "gitdb==4.0.12",
        "GitPython==3.1.44",
        "h11==0.16.0",
        "httpcore==1.0.9",
        "httpx==0.28.1",
        "huggingface-hub==0.32.3",
        "idna==3.10",
        "imageio==2.37.0",
        "imageio-ffmpeg==0.6.0",
        "Jinja2==3.1.6",
        "jiter==0.10.0",
        "joblib==1.5.1",
        "jsonschema==4.24.0",
        "jsonschema-specifications==2025.4.1",
        "kiwisolver==1.4.8",
        "MarkupSafe==3.0.2",
        "matplotlib==3.10.3",
        "moviepy==2.2.1",
        "mpmath==1.3.0",
        "narwhals==1.41.0",
        "networkx==3.5",
        "numpy==2.2.6",
        "openai==1.82.1",
        "opencv-python==4.11.0.86",
        "packaging==24.2",
        "pandas==2.2.3",
        "pefile==2023.2.7",
        "pillow==11.2.1",
        "proglog==0.1.12",
        "protobuf==6.31.1",
        "psutil==7.0.0",
        "py-cpuinfo==9.0.0",
        "pyarrow==20.0.0",
        "pycparser==2.22",
        "pydantic==2.11.5",
        "pydantic_core==2.33.2",
        "pydeck==0.9.1",
        "pyinstaller==6.13.0",
        "pyinstaller-hooks-contrib==2025.4",
        "pyparsing==3.2.3",
        "pypiwin32==223",
        "PyQt5==5.15.11",
        "PyQt5-Qt5==5.15.2",
        "PyQt5_sip==12.17.0",
        "pytesseract==0.3.13",
        "python-dateutil==2.9.0.post0",
        "python-dotenv==1.1.0",
        "pyttsx3==2.98",
        "pytube==15.0.0",
        "pytz==2025.2",
        "pywin32==310",
        "pywin32-ctypes==0.2.3",
        "PyYAML==6.0.2",
        "referencing==0.36.2",
        "regex==2024.11.6",
        "requests==2.32.3",
        "rpds-py==0.25.1",
        "safetensors==0.5.3",
        "scikit-learn==1.6.1",
        "scipy==1.15.3",
        "sentence-transformers==4.1.0",
        "setuptools==80.9.0",
        "six==1.17.0",
        "smmap==5.0.2",
        "sniffio==1.3.1",
        "sounddevice==0.5.2",
        "SpeechRecognition==3.14.3",
        "srt==3.5.3",
        "streamlit==1.45.1",
        "supervision==0.25.1",
        "sympy==1.14.0",
        "tenacity==9.1.2",
        "threadpoolctl==3.6.0",
        "tokenizers==0.21.1",
        "toml==0.10.2",
        "torch==2.7.0",
        "torchvision==0.22.0",
        "tornado==6.5.1",
        "tqdm==4.67.1",
        "transformers==4.52.4",
        "typing-inspection==0.4.1",
        "typing_extensions==4.13.2",
        "tzdata==2025.2",
        "ultralytics==8.3.146",
        "ultralytics-thop==2.0.14",
        "urllib3==2.4.0",
        "vosk==0.3.45",
        "watchdog==6.0.0",
        "websockets==15.0.1",
        "whisper==1.1.10",
    ],
    entry_points={"console_scripts": ["merc-start=start_fullmode:main"]},
    author="Giacomo Germano",
    description="AI evolutiva cosciente stile Jarvis",
)

### --- start_fullmode.py --- ###
def main():
    print("🔁 Avvio completo Mercurius∞")
    print("🧠 Modalità Jarvis+ attiva: Visione, Voce, Dashboard, AI Cognitiva")
    # Avviare sequenze di bootstrap dei moduli AI
    from modules.Neo.trainer_orchestrator import bootstrap_agents
    bootstrap_agents()

if __name__ == "__main__":
    main()

### --- start_voice_interface.py --- ###
"""
Script: start_voice_interface
Funzione: Comunicazione vocale Mercurius∞ da file audio nella root.
Autore: Mercurius∞ AI Engineer
"""

import os
from modules.voice_bridge.multimodal_controller import MultimodalController
from modules.ai_kernel.agent_core import AgentCore

AUDIO_FILE = "audio_input.wav"  # Assicurati che il file sia nella root!

def ensure_audio_exists(path):
    if not os.path.exists(path):
        print(f"[ERRORE] File audio non trovato: {path}")
        exit(1)

def avvia_interazione_vocale(audio_file):
    ensure_audio_exists(audio_file)

    agente = AgentCore()
    multimodale = MultimodalController()

    print("🎙️ Avvio comunicazione vocale...")
    multimodale.listen_and_respond(audio_file, agente.process_input)

if __name__ == "__main__":
    avvia_interazione_vocale(AUDIO_FILE)

### --- strategies/strategy_executor.py --- ###
"""
strategy_executor.py
====================
Genera segnali operativi basati su output del modello predittivo.
"""

class StrategyExecutor:
    def __init__(self, config):
        self.config = config

    def generate_signals(self, model, features):
        """Genera segnali di trading basandosi sull'output del modello."""
        signals = []
        for f in features:
            pred = model.forward([
                f["price_volatility_ratio"],
                f["momentum"],
                f["volatility"]
            ])[0]
            action = "BUY" if pred > 0.5 else "SELL"
            signals.append({
                "symbol": f["symbol"],
                "action": action,
                "confidence": pred,
                "volatility": f["volatility"],
                "timestamp": "2025-05-30T12:00:00"
            })
        return signals

    def filter_signals(self, signals, min_confidence=0.6):
        """Filtra i segnali con bassa confidenza."""
        return [s for s in signals if s["confidence"] >= min_confidence]

    def summary_stats(self, signals):
        """Statistiche dei segnali generati."""
        summary = {"BUY": 0, "SELL": 0}
        for s in signals:
            summary[s["action"]] += 1
        return summary

### --- task_manager_cli.py --- ###
from modules.task_manager_cli import main

if __name__ == "__main__":
    main()

### --- test_exp.json --- ###
[
  {
    "tags": [
      "unit"
    ],
    "result": "ok",
    "timestamp": "2025-06-01T12:23:47.154527"
  }
]

### --- tests/conftest.py --- ###
import sys
import pathlib
import types

ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# Stub external dependencies
_dummy_openai = types.SimpleNamespace(
    ChatCompletion=types.SimpleNamespace(create=lambda **_: {"choices": [{"message": {"content": "ok"}}]})
)
_dummy = types.SimpleNamespace()
for name, mod in {
    "openai": _dummy_openai,
    "torch": _dummy,
    "speech_recognition": _dummy,
    "fitz": _dummy,
    "yaml": _dummy,
    "psutil": _dummy,
    "requests": _dummy,
}.items():
    if name not in sys.modules:
        sys.modules[name] = mod

### --- tests/run_simulation.py --- ###
"""
Simulazione: Avvio sistema Mercurius∞ in modalità autonoma.
Scopo: Verifica operatività integrata dei moduli principali.
"""

from modules.start_fullmode.initializer import SystemInitializer

def run_simulation():
    print("🔁 Simulazione in corso...")

    # Inizializzazione e avvio
    system = SystemInitializer()
    system.initialize_environment()
    system.start_components()

    # Interazione simulata
    audio_input = system.audio.listen()
    system.agent.perceive(audio_input)
    decision = system.agent.reason()
    system.agent.act(decision)
    system.audio.speak(f"Ho elaborato: {decision}")

    # Arresto video per sicurezza
    system.vision.stop()
    print("✅ Simulazione completata.")

if __name__ == "__main__":
    run_simulation()

### --- tests/test_agent_core.py --- ###
from modules.ai_kernel.agent_core import AgentCore


class DummyReasoner:
    def think(self, query: str) -> str:
        return "dummy decision"


def test_agent_boot(monkeypatch):
    monkeypatch.setattr(
        "modules.ai_kernel.agent_core.LangReasoner", lambda: DummyReasoner()
    )
    agent = AgentCore("TestAgent")
    agent.boot()
    assert agent.status == "ready"

### --- tests/test_audio_interface.py --- ###
from modules.voice_bridge.audio_interface import AudioInterface

def test_audio_initialization():
    audio = AudioInterface()
    audio.initialize()
    assert audio.microphone_ready
    assert audio.tts_ready

def test_audio_listen_and_speak():
    audio = AudioInterface()
    audio.initialize()
    spoken = audio.listen()
    assert isinstance(spoken, str)
    assert "simulato" in spoken
    response = audio.speak("Messaggio di test")
    assert response is None  # La funzione speak stampa ma non ritorna nulla

### --- tests/test_autonomia_cognitiva.py --- ###
"""
Test: test_autonomia_cognitiva.py
Responsabilità: Verifica dei moduli self_reflection.py e learning.py
Autore: Mercurius∞ Engineer Mode
"""

import unittest
import os

from core.self_reflection import SelfReflection
from core.learning import ContinuousLearner

class TestAutonomiaCognitiva(unittest.TestCase):
    def setUp(self):
        self.test_reflection_path = "data/test_reflection_log.json"
        self.test_learning_path = "data/test_knowledge_base.json"

        if os.path.exists(self.test_reflection_path):
            os.remove(self.test_reflection_path)
        if os.path.exists(self.test_learning_path):
            os.remove(self.test_learning_path)

        self.reflection = SelfReflection(log_path=self.test_reflection_path)
        self.learner = ContinuousLearner(knowledge_path=self.test_learning_path)

    def test_reflection_logging(self):
        context = {"error": "Timeout"}
        result = self.reflection.evaluate_action("Scan Area", "No response", False, context)
        self.assertIn("insight", result)
        self.assertFalse(result["success"])

        log = self.reflection.logger.load_log()
        self.assertEqual(len(log), 1)
        self.assertEqual(log[0]["action"], "Scan Area")

    def test_reflection_summary(self):
        self.reflection.evaluate_action("Init Sequence", "OK", True, {})
        self.reflection.evaluate_action("Connect API", "403 Forbidden", False, {"error": "Auth failed"})
        summary = self.reflection.summarize_reflections()
        self.assertEqual(summary["total"], 2)
        self.assertEqual(summary["successes"], 1)
        self.assertEqual(summary["failures"], 1)

    def test_learning_mechanism(self):
        context = {"sensor": "IR"}
        insight = self.learner.learn_from_experience("Move Forward", "Success", True, context)
        self.assertTrue("Esperienza positiva" in insight["insight"])

        data = self.learner.kb.load()
        self.assertEqual(len(data), 1)

    def test_learning_statistics(self):
        self.learner.learn_from_experience("Pick Object", "Failed", False, {"error": "gripper jam"})
        self.learner.learn_from_experience("Drop Object", "OK", True, {})
        stats = self.learner.stats()
        self.assertEqual(stats["total"], 2)
        self.assertEqual(stats["successes"], 1)
        self.assertEqual(stats["failures"], 1)

    def tearDown(self):
        if os.path.exists(self.test_reflection_path):
            os.remove(self.test_reflection_path)
        if os.path.exists(self.test_learning_path):
            os.remove(self.test_learning_path)

if __name__ == "__main__":
    unittest.main()

### --- tests/test_end2end.py --- ###
"""
Test: test_end2end.py
Responsabilità: Simulazione di un flusso intero da input a pianificazione e log
Autore: Mercurius∞ Engineer Mode
"""

import unittest
import pytest

pytest.skip("Test End-to-End richiede dipendenze audio/video", allow_module_level=True)

from orchestrator.multimodal_controller import MultimodalController
from modules.supervisor import Supervisor


class TestEndToEnd(unittest.TestCase):

    def setUp(self):
        self.controller = MultimodalController()
        self.supervisor = Supervisor()

    def test_complete_workflow(self):
        """
        Simula ciclo completo da voce a comportamento e supervisione.
        """
        self.controller.run_full_cycle(input_text="parla con me")
        self.controller.run_full_cycle(input_text="analizza l'ambiente")
        self.controller.run_full_cycle(gesture="saluto")

        summary = self.controller.autonomy.summarize_autonomy()
        self.assertGreaterEqual(summary["reflection_summary"]["successes"], 2)

    def test_supervised_actions(self):
        """
        Simula log supervisionato indipendente.
        """
        self.supervisor.observe("auto-test", "OK", True, {"canale": "debug"})
        report = self.supervisor.performance_report()
        self.assertEqual(report["successes"], 1)


if __name__ == "__main__":
    unittest.main()

### --- tests/test_initializer.py --- ###
import os
import pytest

cv2 = pytest.importorskip('cv2', reason='cv2 non disponibile')
from modules.start_fullmode.initializer import SystemInitializer

def test_system_initializer():
    system = SystemInitializer()
    assert system.agent is not None
    assert system.audio is not None
    assert system.vision is not None

def test_environment_setup(monkeypatch):
    monkeypatch.setenv("MERCURIUS_MODE", "")
    system = SystemInitializer()
    system.initialize_environment()
    assert "MERCURIUS_MODE" in os.environ
    assert os.environ["MERCURIUS_MODE"] == "full"

### --- tests/test_josch_bridge.py --- ###
from integrations.bridge_josch import send_command_to_pc

def test_send_command_format():
    resp = send_command_to_pc("echo test")
    assert isinstance(resp, dict)

### --- tests/test_logger.py --- ###
from utils.logger import setup_logger


def test_setup_logger():
    logger = setup_logger("test_logger")
    logger.info("log message")
    assert logger.name == "test_logger"

### --- tests/test_memory.py --- ###
# tests/test_memory.py
import os
import tempfile
from memory.long_term_memory import LongTermMemory

def test_save_and_load():
    with tempfile.TemporaryDirectory() as tmpdir:
        json_path = os.path.join(tmpdir, "test_exp.json")
        mem = LongTermMemory(json_path)
        mem.save_experience({"tags": ["unit"], "result": "ok"})
        data = mem.get_all()
        assert data and "tags" in data[-1]

### --- tests/test_messaging.py --- ###
from modules.messaging.rabbitmq_messenger import publish_message

def test_publish_message_no_server():
    ok = publish_message('test_queue', 'hello')
    assert ok in (True, False)

### --- tests/test_modular_end2end.py --- ###
# tests/test_modular_end2end.py

"""
Test End-to-End per Mercurius∞
Simula i flussi completi: video -> trascrizione -> generazione codice -> sandbox -> auto-fix -> comando -> log.
Autore: Mercurius∞ AI Engineer
"""

import os
import sys
import pytest

# Importa i moduli core di Mercurius∞
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

pytest.skip("Dipendenze pesanti non disponibili", allow_module_level=True)

from learning.video_learner import VideoLearner
from core.sandbox_executor import SandboxExecutor
from modules.local.localai_adapter import LocalAI
from modules.local.leon_ai_bridge import LeonAI

import datetime

RESULT_LOG = "end2end_test_results.log"

def log_result(test_name, result, details=""):
    timestamp = datetime.datetime.now().isoformat()
    with open(RESULT_LOG, "a", encoding="utf-8") as logf:
        logf.write(f"[{timestamp}] {test_name} — {'SUCCESS' if result else 'FAIL'}\n{details}\n\n")
    print(f"{test_name}: {'✅' if result else '❌'}")

# Test 1: Video locale → Trascrizione
def test_video_to_text():
    print("\n--- Test 1: Video locale → Trascrizione ---")
    video_path = "tests/sample.mp3"  # Puoi sostituire con un file audio/video locale reale
    vl = VideoLearner()
    if not os.path.exists(video_path):
        log_result("test_video_to_text", False, "File video/audio di test non trovato.")
        return False
    transcript = vl.extract_insights_from_video(video_path)
    passed = isinstance(transcript, str) and len(transcript.strip()) > 0 and not transcript.startswith("[❌")
    log_result("test_video_to_text", passed, transcript)
    return passed

# Test 2: Prompt a LocalAI → Risposta testuale
def test_localai_text_generation():
    print("\n--- Test 2: Prompt a LocalAI ---")
    ai = LocalAI()
    prompt = "Scrivi una poesia sull'intelligenza artificiale."
    response = ai.execute_task(prompt)
    passed = isinstance(response, str) and len(response.strip()) > 10
    log_result("test_localai_text_generation", passed, response)
    return passed

# Test 3: Codice errato → Sandbox → Auto-fix
def test_sandbox_autofix():
    print("\n--- Test 3: Codice errato → Sandbox → Auto-fix ---")
    code_with_bug = "for i in range(5)\n    print(i)"  # Manca i due punti!
    sandbox = SandboxExecutor(timeout_seconds=3)
    static_ok = sandbox.static_analysis(code_with_bug)
    # static_analysis dovrebbe fallire, quindi tentiamo subito autofix
    if not static_ok:
        fix = sandbox.autofix_with_llm(code_with_bug, "SyntaxError: expected ':'")
        # Ora testiamo il fix se esiste
        result = sandbox.run_sandboxed(fix)
        passed = result.get("success", False)
        log_result("test_sandbox_autofix", passed, result.get("output", fix))
        return passed
    else:
        log_result("test_sandbox_autofix", False, "Il codice errato è stato accettato erroneamente.")
        return False

# Test 4: Comando locale via LeonAI
def test_leonai_command():
    print("\n--- Test 4: LeonAI comando locale ---")
    leon = LeonAI()
    command = "echo Mercurius è operativo!"
    output = leon.run_command(command)
    passed = "Mercurius" in output
    log_result("test_leonai_command", passed, output)
    return passed

# Test 5: Pipeline completa — Video → Trascrizione → Generazione codice → Sandbox
def test_full_pipeline():
    print("\n--- Test 5: Pipeline completa ---")
    ai = LocalAI()
    sandbox = SandboxExecutor(timeout_seconds=3)
    video_path = "tests/sample.mp3"  # Sostituisci con un tuo file di test

    # Step 1: Trascrizione
    vl = VideoLearner()
    if not os.path.exists(video_path):
        log_result("test_full_pipeline", False, "File video/audio di test non trovato.")
        return False
    transcript = vl.extract_insights_from_video(video_path)

    # Step 2: Generazione codice dalla trascrizione
    prompt = f"Genera un semplice script Python che stampa la frase:\n{transcript.strip().split('.')[0]}"
    code = ai.execute_task(prompt)

    # Step 3: Validazione Sandbox
    result = sandbox.run_sandboxed(code)
    passed = result.get("success", False)
    log_result("test_full_pipeline", passed, result.get("output", code))
    return passed

if __name__ == "__main__":
    print("🧪 Mercurius∞ — Test End-to-End")
    all_passed = True
    tests = [
        test_video_to_text,
        test_localai_text_generation,
        test_sandbox_autofix,
        test_leonai_command,
        test_full_pipeline,
    ]
    for test in tests:
        try:
            all_passed &= test()
        except Exception as e:
            log_result(test.__name__, False, f"Exception: {e}")
            all_passed = False
    print("\n=== RISULTATO FINALE ===")
    print("Tutti i test superati!" if all_passed else "Alcuni test NON superati — vedi log.")

### --- tests/test_multimodal.py --- ###
"""
Test: test_multimodal.py
Responsabilità: Verifica il flusso integrato multimodale del sistema
Autore: Mercurius∞ Engineer Mode
"""

import unittest
import pytest

pytest.skip("Test multimodale richiede dipendenze audio/video", allow_module_level=True)

from orchestrator.multimodal_controller import MultimodalController


class TestMultimodalInteraction(unittest.TestCase):

    def setUp(self):
        self.controller = MultimodalController()

    def test_text_input_simulation(self):
        """
        Simula input vocale tramite testo e verifica esecuzione logica.
        """
        result = self.controller.listen_and_interpret(simulate_input="analizza l'ambiente")
        self.assertEqual(result["action"], "analizza_ambiente")

    def test_gesture_input_simulation(self):
        """
        Simula interpretazione di gesto riconosciuto.
        """
        result = self.controller.receive_gesture("saluto")
        self.assertEqual(result["action"], "interagisci_utente")

    def test_full_cycle_text_command(self):
        """
        Testa un ciclo completo da comando a pianificazione + esecuzione.
        """
        self.controller.run_full_cycle(input_text="vai alla base")


if __name__ == "__main__":
    unittest.main()

### --- tests/test_neuro_learning.py --- ###
"""Test base per motore di apprendimento visivo.""" 

from modules.Neo.neuro_learning_engine import parse_video_and_generate_knowledge

def test_video_learning():
    result = parse_video_and_generate_knowledge("Plasticità sinaptica")
    assert "concept" in result
    print("✅ Test neuro-learning passed")

### --- tests/test_orchestrator.py --- ###
"""
Test: test_orchestrator.py
Responsabilità: Validazione AutonomyController e processi decisionali
Autore: Mercurius∞ Engineer Mode
"""

import unittest
from orchestrator.autonomy_controller import AutonomyController


class TestAutonomyController(unittest.TestCase):

    def setUp(self):
        self.controller = AutonomyController()

    def test_process_experience(self):
        """
        Valida la riflessione e l'apprendimento su azione positiva.
        """
        feedback = self.controller.process_experience(
            action="test_comando",
            outcome="Eseguito correttamente",
            success=True,
            context={"livello": "base"}
        )
        self.assertIn("Apprendimento", feedback["learning"])
        self.assertIn("successo", feedback["reflection"])

    def test_summarize_autonomy(self):
        """
        Verifica il report cognitivo riepilogativo.
        """
        self.controller.process_experience("cmd", "done", True, {})
        summary = self.controller.summarize_autonomy()
        self.assertIn("successes", summary["reflection_summary"])


if __name__ == "__main__":
    unittest.main()

### --- tests/test_planner.py --- ###
"""
Test: test_planner.py
Responsabilità: Verifica per ActionPlanner e GoalManager
Autore: Mercurius∞ Engineer Mode
"""

import unittest
from modules.planner import ActionPlanner
from modules.goal_manager import GoalManager


class TestActionPlanner(unittest.TestCase):

    def setUp(self):
        self.planner = ActionPlanner()

    def test_generate_plan_for_known_goal(self):
        plan = self.planner.generate_plan("analizza_ambiente", {})
        self.assertGreater(len(plan), 0)
        self.assertTrue(all("action" in step for step in plan))

    def test_validate_plan(self):
        plan = self.planner.generate_plan("interagisci_utente", {})
        self.assertTrue(self.planner.validate_plan(plan))

    def test_describe_plan(self):
        plan = self.planner.generate_plan("raggiungi_destinazione", {"destinazione": "Base"})
        description = self.planner.describe_plan(plan)
        self.assertIn("calcola_percorso", description)
        self.assertIn("Base", description)

    def test_plan_summary(self):
        self.planner.generate_plan("analizza_ambiente", {})
        summary = self.planner.plan_summary()
        self.assertIn("step_count", summary)
        self.assertGreater(summary["step_count"], 0)


class TestGoalManager(unittest.TestCase):

    def setUp(self):
        self.manager = GoalManager()

    def test_add_and_sort_goals(self):
        self.manager.add_goal("goal1", priority=2)
        self.manager.add_goal("goal2", priority=5)
        top = self.manager.get_next_goal()
        self.assertEqual(top.name, "goal2")

    def test_goal_status_transition(self):
        self.manager.add_goal("goalX")
        g = self.manager.get_next_goal()
        self.assertEqual(g.status, "active")
        self.manager.complete_goal("goalX")
        all_goals = self.manager.all_goals()
        self.assertEqual(all_goals[0]["status"], "completed")

    def test_active_and_pending_filter(self):
        self.manager.add_goal("goalY", priority=1)
        self.manager.add_goal("goalZ", priority=2)
        self.manager.get_next_goal()
        active = self.manager.active_goals()
        pending = self.manager.pending_goals()
        self.assertEqual(len(active), 1)
        self.assertEqual(len(pending), 1)


if __name__ == "__main__":
    unittest.main()

### --- tests/test_policy.py --- ###
# tests/test_policy.py
import pytest

pytest.skip("PolicyManager richiede dipendenze yaml", allow_module_level=True)

from safety.policy_manager import PolicyManager

def test_policy_block():
    mgr = PolicyManager()
    mgr.add_policy("no_secrets", "password=", "block")
    assert mgr.check("here is password=123")["name"] == "no_secrets"

### --- tests/test_reasoner_dispatcher.py --- ###
from modules.reasoner_dispatcher import ReasonerDispatcher


class DummyAgent:
    def __init__(self, resp: str):
        self.resp = resp

    def elaborate(self, prompt):
        return self.resp

    def generate(self, prompt):
        return self.resp

    def analyze(self, prompt):
        return self.resp

    def validate(self, prompt):
        return self.resp


def test_dispatcher_combines_responses():
    dispatcher = ReasonerDispatcher()
    dispatcher.reasoners = {
        "chatgpt4": DummyAgent("a"),
        "ollama3": DummyAgent("b"),
        "azr": DummyAgent("c"),
        "gpt4o": DummyAgent("final"),
    }
    result = dispatcher.dispatch("ciao")
    assert result == "final"

### --- tests/test_secure_executor.py --- ###
from modules.sandbox_executor.secure_executor import SecureExecutor

def test_successful_execution():
    executor = SecureExecutor(timeout=2)
    result = executor.execute("x = 1 + 1\nprint(x)")
    assert "2" in result["output"]
    assert result["error"] == ""
    assert result["stderr"] == ""

def test_timeout_execution():
    executor = SecureExecutor(timeout=1)
    result = executor.execute("while True: pass")
    assert result["error"] == "Execution timed out."

def test_error_handling():
    executor = SecureExecutor()
    result = executor.execute("raise ValueError('Errore di test')")
    assert "ValueError" in result["error"]

### --- tests/test_supervisione.py --- ###
"""
Test: test_supervisione.py
Responsabilità: Verifica comportamento dei moduli di supervisione e telemetria
Autore: Mercurius∞ Engineer Mode
"""

import unittest
import pytest

pytest.skip("Tests di supervisione richiedono psutil", allow_module_level=True)

from modules.supervisor import Supervisor
from utils.telemetry import Telemetry


class TestSupervisor(unittest.TestCase):

    def setUp(self):
        self.supervisor = Supervisor()

    def test_observe_and_report(self):
        self.supervisor.observe("scan", "ok", True, {"sensor": "lidar"})
        self.supervisor.observe("move", "collision", False, {"speed": "fast"})

        report = self.supervisor.performance_report()
        self.assertEqual(report["actions_total"], 2)
        self.assertEqual(report["successes"], 1)
        self.assertEqual(report["failures"], 1)
        self.assertGreaterEqual(report["success_rate"], 0.0)

    def test_last_actions(self):
        self.supervisor.observe("test1", "done", True, {})
        self.supervisor.observe("test2", "done", True, {})
        last = self.supervisor.last_actions(1)
        self.assertEqual(len(last), 1)
        self.assertEqual(last[0]["action"], "test2")


class TestTelemetry(unittest.TestCase):

    def test_system_info_keys(self):
        info = Telemetry.system_info()
        self.assertIn("platform", info)
        self.assertIn("memory_total_MB", info)

    def test_current_usage_structure(self):
        usage = Telemetry.current_usage()
        self.assertIn("cpu_percent", usage)
        self.assertIn("memory_used_MB", usage)

    def test_process_info(self):
        process = Telemetry.process_info()
        self.assertIn("pid", process)
        self.assertGreaterEqual(process["memory_MB"], 0)


if __name__ == "__main__":
    unittest.main()

### --- tests/test_task_manager_cli.py --- ###
import sys
import types

# crea moduli Localai.local_ai e Leonai.leon_ai fittizi prima dell'import
localai_stub = types.SimpleNamespace(LocalAI=lambda: None)
leonai_stub = types.SimpleNamespace(LeonAI=lambda: None)
sys.modules.setdefault('modules.Localai.local_ai', localai_stub)
sys.modules.setdefault('modules.Leonai.leon_ai', leonai_stub)

import importlib
modules_cli = importlib.import_module('modules.task_manager_cli')


def test_create_agent(monkeypatch):
    called = {}

    def fake_bootstrap():
        called['ok'] = True

    monkeypatch.setattr(modules_cli, 'bootstrap_agents', fake_bootstrap)
    modules_cli.create_agent('AgentX')
    assert called.get('ok')

### --- tests/test_video_pipeline.py --- ###


### --- tools/conflict_inspector.py --- ###
"""Basic project conflict analyzer."""
from __future__ import annotations

import pkgutil
from collections import defaultdict


def scan_conflicts() -> None:
    packages = defaultdict(list)
    for module in pkgutil.iter_modules():
        root = module.name.split('.')[0].lower()
        packages[root].append(module.name)
    conflicts = {k: v for k, v in packages.items() if len(v) > 1}
    if not conflicts:
        print("No obvious module name conflicts found.")
        return
    print("Potential conflicts detected:")
    for base, mods in conflicts.items():
        joined = ', '.join(mods)
        print(f" - {base}: {joined}")


if __name__ == "__main__":
    scan_conflicts()

### --- tools/console.py --- ###
"""
console.py
==========
Console interattiva CLI per lanciare operazioni Mercurius∞.
Permette esecuzioni batch, test, AZR e analisi performance.
"""

from core.pipeline_controller import PipelineController
from core.auto_tester import AutoTester
from utils.config_loader import load_config
from modules.experience.experience_memory import ExperienceMemory
from modules.metrics.performance_metrics import PerformanceMetrics

def main():
    config = load_config("config.yaml")
    pipeline = PipelineController(config)
    tester = AutoTester()
    memory = ExperienceMemory(config)

    print("=== Mercurius∞ CLI ===")
    print("1. Esegui una sessione")
    print("2. Simula 3 sessioni")
    print("3. Avvia test automatici")
    print("4. Mostra metriche esperienziali")
    print("5. Esci")

    choice = input("Scelta: ")

    if choice == "1":
        pipeline.run_batch_session()
    elif choice == "2":
        pipeline.simulate_multiple_sessions(3)
    elif choice == "3":
        tester.run()
        tester.test_signal_confidence()
        tester.test_adaptive_behavior()
    elif choice == "4":
        summary = PerformanceMetrics(memory.get_recent_experiences()).summary()
        print("📊 Metriche Esperienziali:")
        for k, v in summary.items():
            print(f"- {k}: {v}")
    else:
        print("Uscita...")

if __name__ == "__main__":
    main()

### --- tools/feedback_collector.py --- ###
"""
feedback_collector.py
=====================
Modulo per raccolta di feedback operativi su performance in tempo reale.
"""

class FeedbackCollector:
    def __init__(self):
        self.log = []

    def record(self, symbol, action, result, confidence, feedback):
        """Registra un feedback strutturato su ogni azione."""
        entry = {
            "symbol": symbol,
            "action": action,
            "profit": result.get("profit", 0),
            "confidence": confidence,
            "feedback": feedback
        }
        self.log.append(entry)

    def summary(self):
        """Statistiche rapide del feedback operativo."""
        if not self.log:
            return {}
        total = len(self.log)
        avg_profit = sum(f["profit"] for f in self.log) / total
        avg_conf = sum(f["confidence"] for f in self.log) / total
        return {
            "total": total,
            "avg_profit": avg_profit,
            "avg_confidence": avg_conf
        }

    def clear(self):
        self.log.clear()

### --- tools/live_logger.py --- ###
"""
live_logger.py
==============
Modulo per stream di log interattivi su terminale o file. Usato per monitoraggio real-time.
"""

import logging
import sys

def setup_stream_logger(name="MercuriusLive", level=logging.INFO):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(level)
        handler = logging.StreamHandler(sys.stdout)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

def redirect_logs_to_file(name="MercuriusFile", filename="output.log"):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(logging.DEBUG)
        file_handler = logging.FileHandler(filename)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    return logger

### --- trading/fin_gpt.py --- ###
class FinGPTAgent:
    def __init__(self):
        self.name = "FinGPT"

    def execute_task(self, market_context: str, parameters: dict = {}) -> str:
        return f"[{self.name}] Analisi sentiment su: {market_context}"

### --- trading/finrl_agent.py --- ###
class FinRLAgent:
    def __init__(self):
        self.name = "FinRL"

    def train(self, dataset_path: str, environment: str = "stocks") -> str:
        return f"[{self.name}] Addestramento RL su dataset: {dataset_path} in env: {environment}"

    def simulate(self, steps: int = 500) -> str:
        return f"[{self.name}] Simulazione completata per {steps} step RL"

    def deploy(self, model_name: str) -> str:
        return f"[{self.name}] Strategia RL deployata: {model_name}"

### --- trading/freqtrade_agent.py --- ###
class FreqtradeAgent:
    def __init__(self):
        self.name = "Freqtrade"

    def execute_task(self, strategy_name: str, action: str = "backtest") -> str:
        return f"[{self.name}] Strategia '{strategy_name}' eseguita in modalità {action}."

### --- trading/openbb_wrapper.py --- ###
class OpenBBWrapper:
    def __init__(self):
        self.name = "OpenBB"

    def execute_task(self, command: str, options: dict = {}) -> str:
        return f"[{self.name}] Comando OpenBB eseguito: {command}"

### --- trading/qlib_adapter.py --- ###
class QlibAdapter:
    def __init__(self):
        self.name = "Qlib"

    def execute_task(self, symbol: str, timeframe: str, mode: str = "forecast") -> str:
        return f"[{self.name}] {mode.upper()} per {symbol} su {timeframe} eseguita."

### --- trading/trading_core.py --- ###
# trading/trading_core.py

"""
Modulo: trading_core.py
Descrizione: Integrazione centralizzata per operazioni di trading con TradingView, MetaTrader5 e Interactive Brokers.
Gestisce segnali, esecuzione ordini e monitoraggio stato.
Supporta import opzionali e fallback dinamici.
"""

import logging
from abc import ABC, abstractmethod

# ─── Import dinamici ─────────────────────────────────────────────────────────
try:
    import MetaTrader5 as mt5
except ImportError:
    mt5 = None

try:
    from ib_insync import IB, Stock
except ImportError:
    IB = None
    Stock = None

# ─── Logging Base ────────────────────────────────────────────────────────────
logging.basicConfig(level=logging.INFO)

# ─── Interfaccia Trading Generale ─────────────────────────────────────────────
class TradingInterface(ABC):
    @abstractmethod
    def connect(self): pass

    @abstractmethod
    def execute_order(self, symbol: str, action: str, quantity: float): pass

    @abstractmethod
    def get_status(self) -> str: pass

# ─── TradingView ──────────────────────────────────────────────────────────────
class TradingViewInterface(TradingInterface):
    def connect(self):
        logging.info("✅ TradingView: Nessuna connessione richiesta (webhook o scraping).")

    def execute_order(self, symbol: str, action: str, quantity: float):
        logging.info(f"📡 Segnale da TradingView: {action.upper()} {quantity} {symbol}")

    def get_status(self) -> str:
        return "✔️ TradingView operativo (webhook)"

# ─── MetaTrader5 ──────────────────────────────────────────────────────────────
class MetaTraderInterface(TradingInterface):
    def connect(self):
        if mt5 and mt5.initialize():
            logging.info("✅ Connessione MT5 avviata.")
        else:
            logging.warning("⚠️ MT5 non disponibile o inizializzazione fallita.")

    def execute_order(self, symbol: str, action: str, quantity: float):
        if not mt5:
            return logging.error("❌ MT5 non disponibile.")
        try:
            type_order = mt5.ORDER_TYPE_BUY if action.lower() == "buy" else mt5.ORDER_TYPE_SELL
            request = {
                "action": mt5.TRADE_ACTION_DEAL,
                "symbol": symbol,
                "volume": quantity,
                "type": type_order,
                "price": mt5.symbol_info_tick(symbol).ask,
                "deviation": 10,
                "magic": 234000,
                "comment": "Mercurius∞ Order",
                "type_time": mt5.ORDER_TIME_GTC,
                "type_filling": mt5.ORDER_FILLING_IOC,
            }
            result = mt5.order_send(request)
            logging.info(f"📈 Ordine MT5 eseguito: {result}")
        except Exception as e:
            logging.error(f"❌ Errore invio ordine MT5: {e}")

    def get_status(self) -> str:
        return "🔗 MT5: connesso" if mt5 and mt5.initialize() else "🚫 MT5: non connesso"

# ─── Interactive Brokers (IBKR) ───────────────────────────────────────────────
class IBKRInterface(TradingInterface):
    def __init__(self):
        self.ib = IB() if IB else None

    def connect(self):
        if self.ib:
            try:
                self.ib.connect("127.0.0.1", 7497, clientId=1)
                logging.info("✅ Connessione IBKR attiva.")
            except Exception as e:
                logging.error(f"❌ Errore IBKR: {e}")
        else:
            logging.warning("⚠️ IB_insync non disponibile.")

    def execute_order(self, symbol: str, action: str, quantity: float):
        if not self.ib:
            return logging.error("❌ IB non inizializzato.")
        try:
            stock = Stock(symbol, "SMART", "USD")
            order_type = "BUY" if action.lower() == "buy" else "SELL"
            self.ib.qualifyContracts(stock)
            order = self.ib.marketOrder(order_type, quantity)
            trade = self.ib.placeOrder(stock, order)
            logging.info(f"📊 IBKR Ordine: {order_type} {quantity} {symbol}")
            return trade
        except Exception as e:
            logging.error(f"❌ Errore ordine IBKR: {e}")

    def get_status(self) -> str:
        return "🔗 IBKR: connesso" if self.ib and self.ib.isConnected() else "🚫 IBKR: disconnesso"

### --- trainer/self_trainer.py --- ###
# trainer/self_trainer.py
"""
Modulo: self_trainer.py
Descrizione: Addestramento self-supervised a partire dalle esperienze accumulate.
"""

from memory.long_term_memory import LongTermMemory
import openai
import os
from pathlib import Path

class SelfTrainer:
    def __init__(self, model_name="gpt-3.5-turbo"):
        self.memory = LongTermMemory()
        self.model = model_name
        openai.api_key = os.getenv("OPENAI_API_KEY")

    def build_prompt(self, experiences):
        prompt = "Sei un assistente AI che migliora le proprie strategie di trading.\n"
        prompt += "Ecco le ultime esperienze:\n"
        for exp in experiences[-10:]:
            prompt += f"- Profit: {exp['result']['profit']}, Qty: {exp['trade']['quantity']}\n"
        prompt += "\nSuggerisci tre modi per migliorare la strategia."
        return prompt

    def train_once(self, save_to: Path | None = None):
        data = self.memory.get_all()
        if not data:
            return "Nessuna esperienza."
        prompt = self.build_prompt(data)
        try:
            resp = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=300,
                temperature=0.3,
            )
            advice = resp["choices"][0]["message"]["content"]
            if save_to:
                save_to.write_text(advice, encoding="utf-8")
            return advice
        except Exception as e:
            return f"⚠️ Training error: {e}"

### --- trainer/trainer_trigger.py --- ###
# trainer/trainer_trigger.py
"""
Modulo: trainer_trigger.py
Descrizione: Innesca SelfTrainer quando ci sono suff. nuove esperienze o in orario schedulato.
"""

import time
import threading
from pathlib import Path
from modules.experience.experience_memory import ExperienceMemory
from trainer.self_trainer import SelfTrainer

class TrainerTrigger:
    def __init__(self, exp_memory: ExperienceMemory, check_interval=600, min_new_exp=25):
        self.exp_memory = exp_memory
        self.check_interval = check_interval
        self.min_new_exp = min_new_exp
        self.trainer = SelfTrainer()
        self._last_count = 0
        threading.Thread(target=self._loop, daemon=True).start()

    def _loop(self):
        while True:
            current_count = len(self.exp_memory.store.get_all())
            if current_count - self._last_count >= self.min_new_exp:
                print("🛠️ TrainerTrigger: Nuove esperienze sufficienti, avvio training...")
                advice = self.trainer.train_once(save_to=Path("logs/latest_strategy_advice.md"))
                print(f"📚 Suggerimenti strategici:\n{advice}\n")
                self._last_count = current_count
            time.sleep(self.check_interval)

### --- update_project_tree.py --- ###
import os
from pathlib import Path

# File extensions to include
EXTENSIONS = {'.py', '.json', '.yaml', '.yml', '.md', '.toml', '.txt'}

ROOT_DIR = Path(__file__).resolve().parent
OUTPUT_FILE = ROOT_DIR / 'project_tree'

def should_include(path: Path) -> bool:
    """Return True if file should be included in the project tree."""
    if '.git' in path.parts:
        return False
    return path.is_file() and path.suffix.lower() in EXTENSIONS

def collect_files(root: Path):
    return sorted(p for p in root.rglob('*') if should_include(p))

def build_tree(files):
    lines = []
    for file_path in files:
        rel = file_path.relative_to(ROOT_DIR)
        lines.append(f"### --- {rel.as_posix()} --- ###")
        try:
            content = file_path.read_text(encoding='utf-8')
        except Exception:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
        lines.append(content.rstrip('\n'))
        lines.append('')
    return '\n'.join(lines) + '\n'

def main():
    files = collect_files(ROOT_DIR)
    tree_content = build_tree(files)
    OUTPUT_FILE.write_text(tree_content, encoding='utf-8')

if __name__ == '__main__':
    main()

### --- updater/__init__.py --- ###


### --- updater/auto_updater.py --- ###
# updater/auto_updater.py
"""
Modulo: auto_updater.py
Descrizione: Aggiorna Mercurius∞ da remoto (GitHub) o da pacchetto tar/zip.
• Scarica la nuova versione
• Esegue migrazioni (requirements, db)
• Riavvia il processo principale
"""

import subprocess
import sys
from pathlib import Path
from typing import Literal, Optional
from analytics.behavior_logger import BehaviorLogger

logger = BehaviorLogger()


class AutoUpdater:
    def __init__(self, repo_url: str, branch: str = "main"):
        self.repo_url = repo_url
        self.branch = branch
        self.repo_dir = Path(".").resolve()

    # ---------- public ----------
    def update(self, source: Literal["git", "package"] = "git", pkg_file: Optional[str] = None):
        if source == "git":
            return self._pull_git()
        if source == "package" and pkg_file:
            return self._extract_package(pkg_file)
        raise ValueError("Sorgente update non valida.")

    # ---------- internal ----------
    def _pull_git(self):
        cmd = ["git", "pull", self.repo_url, self.branch]
        res = subprocess.run(cmd, cwd=self.repo_dir, text=True, capture_output=True)
        logger.log("auto_update", {"method": "git", "stdout": res.stdout, "stderr": res.stderr})
        if res.returncode == 0:
            self._post_update()
            return "✅ Update da Git completato."
        return f"❌ Git pull error: {res.stderr}"

    def _extract_package(self, pkg_file: str):
        import tarfile, zipfile, shutil, tempfile

        tmp = Path(tempfile.mkdtemp())
        if pkg_file.endswith(".tar.gz"):
            with tarfile.open(pkg_file) as tar:
                tar.extractall(tmp)
        elif pkg_file.endswith(".zip"):
            with zipfile.ZipFile(pkg_file) as zf:
                zf.extractall(tmp)
        else:
            return "Formato pacchetto non supportato."

        # Copia sopra il codice
        for item in tmp.iterdir():
            target = self.repo_dir / item.name
            if target.exists():
                shutil.rmtree(target, ignore_errors=True)
            shutil.move(item, target)
        logger.log("auto_update", {"method": "package", "file": pkg_file})
        self._post_update()
        return "✅ Update da package completato."

    def _post_update(self):
        subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt", "-q"])
        logger.log("auto_update", {"action": "deps_installed"})

### --- utils/config_loader.py --- ###
"""
config_loader.py
================
Carica la configurazione da file YAML (mock per ora).
"""

def load_config(path):
    """
    Mock del caricamento configurazione.
    In un sistema reale, caricherebbe da YAML/JSON.
    """
    return {
        "symbols": ["AAPL", "TSLA", "GOOG"],
        "base_trade_qty": 100,
        "min_confidence": 0.55,
        "retrain_threshold": 0.65
    }

### --- utils/environment.py --- ###
"""
Modulo: environment.py
Responsabilità: Caricare e gestire le variabili di ambiente per Mercurius∞
Autore: Mercurius∞ Engineer Mode
"""

import os
from dotenv import load_dotenv

class Environment:
    """
    Carica il file .env e fornisce accesso centralizzato alle variabili di ambiente.
    """

    def __init__(self, dotenv_path: str = ".env"):
        self.loaded = False
        self.dotenv_path = dotenv_path
        self.load_environment()

    def load_environment(self):
        """
        Carica le variabili da .env nel sistema.
        """
        if os.path.exists(self.dotenv_path):
            load_dotenv(dotenv_path=self.dotenv_path)
            self.loaded = True
        else:
            raise FileNotFoundError(f"File .env non trovato in {self.dotenv_path}")

    def get(self, key: str, default=None):
        """
        Recupera una variabile d'ambiente.
        """
        return os.getenv(key, default)

    def get_openai_config(self) -> dict:
        return {
            "use_openai": self.get("USE_OPENAI") == "1",
            "api_key": self.get("OPENAI_API_KEY"),
            "chat_model": self.get("OPENAI_CHAT_MODEL"),
            "embed_model": self.get("OPENAI_EMBED_MODEL")
        }

    def get_web_monitor_credentials(self) -> dict:
        return {
            "user": self.get("WM_USER"),
            "password": self.get("WM_PASS")
        }

    def get_mcp_config(self) -> dict:
        return {
            "token": self.get("MCP_TOKEN"),
            "introspect_url": self.get("MCP_INTROSPECT_URL")
        }

    def get_mercurius_api_key(self) -> str:
        return self.get("MERCURIUS_API_KEY")

    def get_run_mode(self) -> str:
        """Restituisce la modalità operativa di AION."""
        return self.get("AION_RUN_MODE", "dialogic-autonomous")

### --- utils/logger.py --- ###
"""
logger.py
=========
Configurazione logging per il sistema Mercurius∞.
"""

import logging

def setup_logger(name="MercuriusLogger"):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(logging.DEBUG)
        ch = logging.StreamHandler()
        ch.setLevel(logging.DEBUG)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        logger.addHandler(ch)
    return logger

def get_file_logger(name="MercuriusFileLogger", filename="mercurius.log"):
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(logging.INFO)
        fh = logging.FileHandler(filename)
        formatter = logging.Formatter('[%(asctime)s] %(levelname)s - %(message)s')
        fh.setFormatter(formatter)
        logger.addHandler(fh)
    return logger

### --- utils/telemetry.py --- ###
"""
Modulo: telemetry.py
Responsabilità: Raccolta telemetria interna (risorse, moduli, stato sistema)
Autore: Mercurius∞ Engineer Mode
"""

import psutil
import platform
import os
import time
from typing import Dict


class Telemetry:
    """
    Fornisce dati interni sullo stato del sistema e delle risorse.
    """

    @staticmethod
    def system_info() -> Dict:
        return {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.machine(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_MB": round(psutil.virtual_memory().total / (1024 ** 2), 2),
        }

    @staticmethod
    def current_usage() -> Dict:
        mem = psutil.virtual_memory()
        return {
            "cpu_percent": psutil.cpu_percent(interval=0.5),
            "memory_used_MB": round(mem.used / (1024 ** 2), 2),
            "memory_percent": mem.percent,
            "active_processes": len(psutil.pids()),
            "uptime_sec": int(time.time() - psutil.boot_time())
        }

    @staticmethod
    def process_info(pid: int = os.getpid()) -> Dict:
        p = psutil.Process(pid)
        return {
            "pid": pid,
            "name": p.name(),
            "status": p.status(),
            "cpu_percent": p.cpu_percent(interval=0.5),
            "memory_MB": round(p.memory_info().rss / (1024 ** 2), 2),
            "threads": p.num_threads()
        }

### --- vision/__init__.py --- ###
from .ocr_module import extract_text_from_image

__all__ = ["extract_text_from_image"]

### --- vision/capture.py --- ###
# vision/capture.py

"""
Modulo: capture.py
Descrizione: Acquisizione video da IP Webcam per Mercurius∞. Utilizza OpenCV per estrarre frame in tempo reale.
"""

import cv2
import numpy as np
from typing import Optional


def get_frame_from_ip(ip_url: str) -> Optional[np.ndarray]:
    """
    Recupera un frame dall'indirizzo IP di una webcam.
    """
    cap = cv2.VideoCapture(ip_url)
    if not cap.isOpened():
        print("❌ Impossibile connettersi alla webcam IP.")
        return None

    ret, frame = cap.read()
    cap.release()

    if not ret:
        print("⚠️ Nessun frame catturato.")
        return None

    return frame

### --- vision/image_vision.py --- ###
# vision/image_vision.py

"""
Modulo: image_vision.py
Descrizione: Analisi di immagini statiche con OCR per l'estrazione di testo e concetti visuali.
Usa pytesseract per lettura OCR e OpenCV per preprocessing.
"""

import pytesseract
import cv2
from typing import List


class ImageVision:
    def __init__(self):
        pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"  # Aggiorna se necessario

    def read_text_from_image(self, image_path: str) -> str:
        """
        Estrae il testo da un'immagine tramite OCR.
        """
        try:
            image = cv2.imread(image_path)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            text = pytesseract.image_to_string(gray)
            return text.strip()
        except Exception as e:
            return f"[ERRORE OCR]: {e}"

    def extract_labels(self, image_path: str) -> List[str]:
        """
        Placeholder per estensione con modello YOLO/Vision per rilevamento oggetti.
        """
        return ["[analisi visiva non implementata]"]

### --- vision/ip_webcam_vision.py --- ###
"""YOLO based detection from IP webcam stream."""
import cv2
from vision.object_vision import ObjectVision

class IPWebcamVision(ObjectVision):
    def start_stream(self, ip_url: str):
        cap = cv2.VideoCapture(ip_url)
        if not cap.isOpened():
            raise RuntimeError("Cannot open IP camera")
        print("📡 Streaming IP webcam... press 'q' to quit")
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            results = self.model(frame, verbose=False)[0]
            annotated = self.box_annotator.annotate(
                scene=frame,
                detections=results.boxes
            )
            cv2.imshow("Mercurius∞ IP Cam", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

### --- vision/object_vision.py --- ###
# vision/object_vision.py

"""
Modulo: object_vision.py
Descrizione: Riconoscimento oggetti in tempo reale da webcam con YOLOv8.
"""

import cv2
import supervision as sv
from ultralytics import YOLO


class ObjectVision:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)
        self.box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=1, text_scale=0.5)

    def start_detection(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            raise RuntimeError("Camera non accessibile.")

        print("🎥 Avvio rilevamento oggetti YOLOv8... Premi 'q' per uscire.")

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            results = self.model(frame, verbose=False)[0]
            detections = sv.Detections.from_ultralytics(results)
            labels = [f"{c} {s:.2f}" for c, s in zip(results.names.values(), results.boxes.conf.cpu().numpy())]

            annotated = self.box_annotator.annotate(scene=frame, detections=detections, labels=labels)
            cv2.imshow("Mercurius∞ Vision", annotated)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        cap.release()
        cv2.destroyAllWindows()

    def contextual_reaction(self, detected_items: list) -> str:
        if "person" in detected_items:
            return "👁️ Persona rilevata. Inizio monitoraggio ambientale."
        elif "keyboard" in detected_items:
            return "⌨️ Attività utente rilevata. Modalità lavoro attiva."
        else:
            return "🔍 Nessun oggetto prioritario rilevato."

### --- vision/ocr_module.py --- ###
"""
Modulo: ocr_module.py
Descrizione: Estrae testo da immagini tramite OCR (Tesseract o alternativa).
"""

try:
    import pytesseract
    from PIL import Image
except ImportError:
    raise ImportError("Modulo OCR non installato: usa `pip install pytesseract pillow`")

def extract_text_from_image(image_path: str) -> str:
    """
    Estrae il testo da un'immagine (jpg, png) usando OCR.
    """
    try:
        img = Image.open(image_path)
        text = pytesseract.image_to_string(img, lang='ita')  # o 'eng' se preferisci
        return text.strip()
    except Exception as e:
        return f"[❌ Errore OCR]: {str(e)}"

### --- vision/ocr_reader.py --- ###
# vision/ocr_reader.py

"""
Modulo: ocr_reader.py
Descrizione: Estrazione testi da immagini o webcam tramite OCR (Tesseract).
Supporta JPEG, PNG, flussi video.
"""

import pytesseract
import cv2


class OCRReader:
    def __init__(self):
        pass

    def read_text_from_image(self, path: str) -> str:
        img = cv2.imread(path)
        return pytesseract.image_to_string(img, lang="ita+eng")

    def read_from_camera(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            text = pytesseract.image_to_string(frame)
            print(f"[OCR] {text.strip()}")
            cv2.imshow("OCR Live", frame)
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
        cap.release()
        cv2.destroyAllWindows()

### --- vision/voice_trigger.py --- ###
# voice/voice_trigger.py

"""
Modulo: voice_trigger.py
Descrizione: Attivazione vocale tramite parola chiave "Hey Mercurius" utilizzando STT.
"""

import speech_recognition as sr


def listen_for_trigger(trigger_word: str = "hey mercurius") -> bool:
    """
    Ascolta il microfono per attivazione vocale.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎙️ Ascolto in corso... (parola chiave: 'Hey Mercurius')")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio).lower()
        print(f"🗣️ Rilevato: {text}")
        return trigger_word in text
    except sr.UnknownValueError:
        print("⚠️ Audio non riconosciuto.")
    except sr.RequestError:
        print("❌ Errore nel servizio di riconoscimento.")

    return False

### --- vision/yolo_handler.py --- ###
# vision/yolo_handler.py

"""
Modulo: yolo_handler.py
Descrizione: Riconoscimento oggetti con YOLOv5/YOLOv8 tramite OpenCV per Mercurius∞.
"""

from typing import List
import torch
import numpy as np

# Caricamento modello YOLO (richiede modello pre-addestrato disponibile localmente)
try:
    model = torch.hub.load('ultralytics/yolov5', 'yolov5s', trust_repo=True)
except Exception as e:
    print("⚠️ Errore nel caricamento del modello YOLO:", e)
    model = None


def detect_objects(image: np.ndarray) -> List[str]:
    """
    Rileva oggetti in un'immagine e restituisce le etichette.
    """
    if model is None:
        return []

    results = model(image)
    labels = results.pandas().xyxy[0]['name'].tolist()
    return labels

### --- voice/README.md --- ###
# 🎙️ Modulo Vocale – Attivazione

Gestisce input vocali e hotword per l'attivazione GENESIS.

## Componenti

- `activation_hook.py`: listener per "Hey Mercurius, attiva GENESIS"

### --- voice/__init__.py --- ###


### --- voice/coqui_tts.py --- ###
# voice/coqui_tts.py

"""
Modulo: coqui_tts.py
Descrizione: Sintesi vocale offline con Coqui TTS.
"""

### --- voice/elevenlabs_tts.py --- ###
# voice/elevenlabs_tts.py

"""
Modulo: elevenlabs_tts.py
Descrizione: Voce naturale con API ElevenLabs – stile Jarvis.
"""

import requests
import os

class ElevenLabsTTS:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("ELEVENLABS_API_KEY")

    def speak(self, text: str, voice_id="EXAVITQu4vr4xnSDxMaL"):
        url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
        headers = {
            "xi-api-key": self.api_key,
            "Content-Type": "application/json"
        }
        json_data = {
            "text": text,
            "model_id": "eleven_monolingual_v1",
            "voice_settings": {"stability": 0.5, "similarity_boost": 0.75}
        }
        response = requests.post(url, json=json_data, headers=headers)
        with open("output_11labs.wav", "wb") as f:
            f.write(response.content)

### --- voice/engine/coqui_tts.py --- ###
class CoquiTTS:
    def __init__(self):
        self.name = "CoquiTTS"

    def speak(self, phrase: str) -> str:
        return f"[{self.name}] Audio generato per: {phrase}"

### --- voice/engine/elevenlabs_tts.py --- ###
class ElevenLabsTTS:
    def __init__(self):
        self.name = "ElevenLabs"

    def synthesize(self, text: str, voice: str = "Jarvis") -> str:
        return f"[{self.name}] Sintesi vocale: '{text}' con voce {voice}"

### --- voice/engine/whisper_stt.py --- ###
class WhisperSTT:
    def __init__(self):
        self.name = "Whisper"

    def transcribe(self, audio_path: str) -> str:
        return f"[{self.name}] Trascrizione simulata del file: {audio_path}"

### --- voice/nari_tts.py --- ###
# voice/nari_tts.py

"""
Modulo: nari_tts.py
Descrizione: Sintesi vocale con il modello Nari Dia TTS.
"""

import soundfile as sf
from dia.model import Dia

class NariDiaTTS:
    def __init__(self, model_name="nari-labs/Dia-1.6B"):
        self.model = Dia.from_pretrained(model_name)

    def speak(self, text: str, output_path="output.wav"):
        output = self.model.generate(text)
        sf.write(output_path, output, 44100)

### --- voice/stt.py --- ###
# voice/stt.py

"""
Modulo: stt.py
Descrizione: Riconoscimento vocale da microfono in testo utilizzando SpeechRecognition (Google STT).
"""

import speech_recognition as sr


def transcribe_audio() -> str:
    """
    Converte l'audio acquisito da microfono in testo.
    """
    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("🎧 In ascolto...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)

    try:
        text = recognizer.recognize_google(audio, language="it-IT")
        print(f"📝 Riconosciuto: {text}")
        return text
    except sr.UnknownValueError:
        return "[Voce non riconosciuta]"
    except sr.RequestError:
        return "[Errore nel riconoscimento vocale]"

### --- voice/tts.py --- ###
# voice/tts.py (aggiornato)

"""
Modulo: tts.py
Descrizione: Sintesi vocale con fallback a gTTS se pyttsx3 non disponibile.
"""

try:
    import pyttsx3
    ENGINE = pyttsx3.init()
    USE_PYTTS = True
except ImportError:
    from gtts import gTTS
    import os
    USE_PYTTS = False


def speak(text: str):
    if USE_PYTTS:
        ENGINE.say(text)
        ENGINE.runAndWait()
    else:
        tts = gTTS(text=text, lang="it")
        file_path = "temp_audio.mp3"
        tts.save(file_path)
        os.system(f"start {file_path}" if os.name == "nt" else f"xdg-open {file_path}")

### --- voice/voice_bridge.py --- ###
"""voice_bridge.py
Output vocale tramite engine TTS locale.
"""

from __future__ import annotations

import pyttsx3

_engine = pyttsx3.init()


def speak(text: str) -> None:
    """Riproduce testo tramite sintesi vocale."""
    _engine.say(text)
    _engine.runAndWait()

### --- voice/voice_identity.py --- ###
# voice/voice_identity.py

"""
Modulo: voice_identity.py
Descrizione: Riconoscimento vocale degli speaker e saluti personalizzati.
"""

import os
import speech_recognition as sr
import json
from datetime import datetime
import hashlib


class VoiceIdentityManager:
    def __init__(self, db_path="logs/voice_profiles.json"):
        self.db_path = db_path
        if not os.path.exists(self.db_path):
            with open(self.db_path, "w") as f:
                json.dump({}, f)
        self.db = self._load_db()

    def _load_db(self):
        with open(self.db_path, "r") as f:
            return json.load(f)

    def identify_speaker(self, audio: sr.AudioData, recognizer: sr.Recognizer) -> str:
        try:
            text = recognizer.recognize_google(audio, language="it-IT")
            voice_id = self._voice_hash(audio)
            if voice_id in self.db:
                return f"🎙️ Bentornato {self.db[voice_id]['titolo']} {self.db[voice_id]['nome']}!"
            else:
                print("Voce non riconosciuta. Chi sei?")
                return self.register_new_voice(voice_id, text)
        except Exception:
            return "❌ Voce non comprensibile."

    def register_new_voice(self, voice_id: str, input_text: str) -> str:
        name = input_text.strip().split()[-1].capitalize()
        titolo = "Signor" if name[-1] not in "aeiou" else "Signora"
        self.db[voice_id] = {"nome": name, "titolo": titolo, "registrato": datetime.now().isoformat()}
        with open(self.db_path, "w") as f:
            json.dump(self.db, f, indent=2)
        return f"🎙️ Piacere {titolo} {name}, registrazione completata."

    def _voice_hash(self, audio: sr.AudioData) -> str:
        return hashlib.sha256(audio.get_raw_data()).hexdigest()[:16]

### --- voice/vosk_stt.py --- ###
# voice/vosk_stt.py

"""
Modulo: vosk_stt.py
Descrizione: Riconoscimento vocale locale con Vosk.
"""

import sounddevice as sd
import queue
import vosk
import json

class VoskSTT:
    def __init__(self, model_path="model"):
        self.model = vosk.Model(model_path)
        self.q = queue.Queue()

    def listen(self, duration=5, fs=16000):
        def callback(indata, frames, time, status):
            self.q.put(bytes(indata))
        with sd.RawInputStream(samplerate=fs, blocksize=8000, dtype="int16", channels=1, callback=callback):
            rec = vosk.KaldiRecognizer(self.model, fs)
            for _ in range(int(duration * fs / 8000)):
                data = self.q.get()
                if rec.AcceptWaveform(data):
                    res = json.loads(rec.Result())
                    return res.get("text", "")
            return ""

### --- voice/whisper_engine.py --- ###
# voice/whisper_engine.py

"""
Modulo: whisper_engine.py
Descrizione: Sintesi vocale inversa (STT) ad alta precisione con Whisper v3.
Supporta più lingue e trascrizione offline tramite modelli locali o OpenAI API.
"""

import os
import tempfile
import whisper


class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def transcribe_audio_file(self, audio_path: str, language: str = "it") -> str:
        result = self.model.transcribe(audio_path, language=language)
        return result.get("text", "[Nessun testo estratto]")

    def transcribe_microphone(self, duration=5, tmp_format="micro_input.wav") -> str:
        import sounddevice as sd
        import scipy.io.wavfile

        samplerate = 16000
        recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
        sd.wait()

        tmp_path = os.path.join(tempfile.gettempdir(), tmp_format)
        scipy.io.wavfile.write(tmp_path, samplerate, recording)
        return self.transcribe_audio_file(tmp_path)

### --- voice/whisper_stt.py --- ###
# voice/whisper_stt.py

"""
Modulo: whisper_stt.py
Descrizione: Trascrizione vocale avanzata multilingua tramite Whisper Large-V3.
"""

import whisper
import sounddevice as sd
import numpy as np
import tempfile
import wave

class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def record_audio(self, duration=5, fs=16000, device_index=None) -> str:
        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, device=device_index)
        sd.wait()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            wav_file = f.name
            with wave.open(wav_file, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(fs)
                wf.writeframes((audio * 32767).astype(np.int16).tobytes())
        return wav_file

    def transcribe_live_audio(self, duration=5, device_index=None) -> str:
        file_path = self.record_audio(duration, device_index=device_index)
        return self.transcribe_file(file_path)

    def transcribe_file(self, file_path: str) -> str:
        result = self.model.transcribe(file_path)
        return result["text"]

### --- voice/yolov8_engine.py --- ###
# vision/yolov8_engine.py

"""
Modulo: yolov8_engine.py
Descrizione: Riconoscimento in tempo reale di oggetti, volti e gesti con YOLOv8.
Supporta flussi da webcam o video file.
"""

import cv2
from ultralytics import YOLO


class VisionAI:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)

    def detect_from_image(self, image_path: str) -> list:
        results = self.model(image_path)
        return results[0].names

    def detect_from_webcam(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            results = self.model(frame)
            annotated = results[0].plot()
            cv2.imshow("Mercurius Vision", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()

