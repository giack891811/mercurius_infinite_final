Questa Ã¨ la parte 57 di project_tree. Continua da quella precedente.

def speak(text: str):
    if USE_PYTTS:
        ENGINE.say(text)
        ENGINE.runAndWait()
    else:
        tts = gTTS(text=text, lang="it")
        file_path = "temp_audio.mp3"
        tts.save(file_path)
        os.system(f"start {file_path}" if os.name == "nt" else f"xdg-open {file_path}")

### --- voice/voice_bridge.py --- ###
"""voice_bridge.py
Output vocale tramite engine TTS locale.
"""

from __future__ import annotations

import pyttsx3

_engine = pyttsx3.init()


def speak(text: str) -> None:
    """Riproduce testo tramite sintesi vocale."""
    _engine.say(text)
    _engine.runAndWait()

### --- voice/voice_identity.py --- ###
# voice/voice_identity.py

"""
Modulo: voice_identity.py
Descrizione: Riconoscimento vocale degli speaker e saluti personalizzati.
"""

import os
import speech_recognition as sr
import json
from datetime import datetime
import hashlib


class VoiceIdentityManager:
    def __init__(self, db_path="logs/voice_profiles.json"):
        self.db_path = db_path
        if not os.path.exists(self.db_path):
            with open(self.db_path, "w") as f:
                json.dump({}, f)
        self.db = self._load_db()

    def _load_db(self):
        with open(self.db_path, "r") as f:
            return json.load(f)

    def identify_speaker(self, audio: sr.AudioData, recognizer: sr.Recognizer) -> str:
        try:
            text = recognizer.recognize_google(audio, language="it-IT")
            voice_id = self._voice_hash(audio)
            if voice_id in self.db:
                return f"ðŸŽ™ï¸ Bentornato {self.db[voice_id]['titolo']} {self.db[voice_id]['nome']}!"
            else:
                print("Voce non riconosciuta. Chi sei?")
                return self.register_new_voice(voice_id, text)
        except Exception:
            return "âŒ Voce non comprensibile."

    def register_new_voice(self, voice_id: str, input_text: str) -> str:
        name = input_text.strip().split()[-1].capitalize()
        titolo = "Signor" if name[-1] not in "aeiou" else "Signora"
        self.db[voice_id] = {"nome": name, "titolo": titolo, "registrato": datetime.now().isoformat()}
        with open(self.db_path, "w") as f:
            json.dump(self.db, f, indent=2)
        return f"ðŸŽ™ï¸ Piacere {titolo} {name}, registrazione completata."

    def _voice_hash(self, audio: sr.AudioData) -> str:
        return hashlib.sha256(audio.get_raw_data()).hexdigest()[:16]

### --- voice/vosk_stt.py --- ###
# voice/vosk_stt.py

"""
Modulo: vosk_stt.py
Descrizione: Riconoscimento vocale locale con Vosk.
"""

import sounddevice as sd
import queue
import vosk
import json

class VoskSTT:
    def __init__(self, model_path="model"):
        self.model = vosk.Model(model_path)
        self.q = queue.Queue()

    def listen(self, duration=5, fs=16000):
        def callback(indata, frames, time, status):
            self.q.put(bytes(indata))
        with sd.RawInputStream(samplerate=fs, blocksize=8000, dtype="int16", channels=1, callback=callback):
            rec = vosk.KaldiRecognizer(self.model, fs)
            for _ in range(int(duration * fs / 8000)):
                data = self.q.get()
                if rec.AcceptWaveform(data):
                    res = json.loads(rec.Result())
                    return res.get("text", "")
            return ""

### --- voice/whisper_engine.py --- ###
# voice/whisper_engine.py

"""
Modulo: whisper_engine.py
Descrizione: Sintesi vocale inversa (STT) ad alta precisione con Whisper v3.
Supporta piÃ¹ lingue e trascrizione offline tramite modelli locali o OpenAI API.
"""

import os
import tempfile
import whisper


class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def transcribe_audio_file(self, audio_path: str, language: str = "it") -> str:
        result = self.model.transcribe(audio_path, language=language)
        return result.get("text", "[Nessun testo estratto]")

    def transcribe_microphone(self, duration=5, tmp_format="micro_input.wav") -> str:
        import sounddevice as sd
        import scipy.io.wavfile

        samplerate = 16000
        recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)
        sd.wait()

        tmp_path = os.path.join(tempfile.gettempdir(), tmp_format)
        scipy.io.wavfile.write(tmp_path, samplerate, recording)
        return self.transcribe_audio_file(tmp_path)

### --- voice/whisper_stt.py --- ###
# voice/whisper_stt.py

"""
Modulo: whisper_stt.py
Descrizione: Trascrizione vocale avanzata multilingua tramite Whisper Large-V3.
"""

import whisper
import sounddevice as sd
import numpy as np
import tempfile
import wave

class WhisperSTT:
    def __init__(self, model_name="large-v3"):
        self.model = whisper.load_model(model_name)

    def record_audio(self, duration=5, fs=16000, device_index=None) -> str:
        audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, device=device_index)
        sd.wait()
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
            wav_file = f.name
            with wave.open(wav_file, "wb") as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(fs)
                wf.writeframes((audio * 32767).astype(np.int16).tobytes())
        return wav_file

    def transcribe_live_audio(self, duration=5, device_index=None) -> str:
        file_path = self.record_audio(duration, device_index=device_index)
        return self.transcribe_file(file_path)

    def transcribe_file(self, file_path: str) -> str:
        result = self.model.transcribe(file_path)
        return result["text"]

### --- voice/yolov8_engine.py --- ###
# vision/yolov8_engine.py

"""
Modulo: yolov8_engine.py
Descrizione: Riconoscimento in tempo reale di oggetti, volti e gesti con YOLOv8.
Supporta flussi da webcam o video file.
"""

import cv2
from ultralytics import YOLO


class VisionAI:
    def __init__(self, model_path="yolov8n.pt"):
        self.model = YOLO(model_path)

    def detect_from_image(self, image_path: str) -> list:
        results = self.model(image_path)
        return results[0].names

    def detect_from_webcam(self, camera_index=0):
        cap = cv2.VideoCapture(camera_index)
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            results = self.model(frame)
            annotated = results[0].plot()
            cv2.imshow("Mercurius Vision", annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()
