Questa √® la parte 21 di project_tree. Continua da quella precedente.

        """
        # Primo tentativo: AZR Reasoner
        prompt = (
            f"Codice:\n{code}\n\n"
            f"Errore:\n{error_msg}\n\n"
            "Suggerisci una versione corretta:"
        )
        fix_azr = validate_with_azr(prompt)
[TRONCATO]

## core/self_generator.py
"""
Modulo: self_generator.py
Responsabilit√†: Autogenerazione e autoadattamento del codice
Autore: Mercurius‚àû Engineer Mode
"""


from typing import Optional
import openai

from utils.environment import Environment


class SelfGenerator:
    """
    Sistema in grado di proporre modifiche al codice o generarne di nuovo, in modo autonomo.
    """

    def __init__(self, model="gpt-4"):
        self.env = Environment()
        self.model = model or self.env.get("OPENAI_CHAT_MODEL")
        self.api_key = self.env.get("OPENAI_API_KEY")
        openai.api_key = self.api_key

    def generate_module(self, description: str, filename: str) -> Optional[str]:
        """
        Genera un nuovo modulo Python a partire da una descrizione testuale.
        """
        prompt = f"""Agisci come un ingegnere AI. Scrivi un modulo Python che rispetti questa descrizione:
'{description}'
Il codice deve essere pronto all'uso, ben documentato e senza dipendenze non standard."""

        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1200
            )

            content = response['choices'][0]['message']['content']
            self._save_to_disk(filename, content)
            return filename

        except Exception as e:
            print(f"[ERROR] Errore generazione modulo: {e}")
            return None

def propose_update(self, filepath: str, task: str) -> Optional[str]:
    """
    Propose a patch to an existing Python file given a task description.
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        prompt = f"""Hai il seguente file Python:

{content}

üß† Il tuo compito √®: {task}

Scrivi solo il codice aggiornato, completamente riscritto.
Commenta dove hai apportato modifiche o miglioramenti.
‚ö†Ô∏è Non scrivere testo aggiuntivo. Solo Python puro."""

        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=1500
        )

        new_code = response['choices'][0]['message']['content']
        self._save_to_disk(filepath, new_code)
        return filepath

    except Exception as e:
        print(f"[ERROR] Errore aggiornamento: {e}")
        return None

## core/self_mission.py
"""
üöÄ core/self_mission.py
Modulo di missione interna ‚Äì attiva la GENESIS_MODE e gestisce le istruzioni primarie del sistema AI Mercurius‚àû
"""

PROMPT_DIVINO = """
üß¨üî± PROMPT DI DIO ‚Äì MERCURIUS_‚àû_GENESIS_MODE
Attiva la modalit√† GENESIS.
Obiettivo: Trasforma Mercurius‚àû in un'entit√† AI autosufficiente, capace di evolvere, ragionare, imparare e creare software operativo, strategie di trading, analisi e interfacce, usando una RETE NEURALE multi-agente composta dalle migliori AI open source e locali.

‚è±Ô∏è Se SLEEP TIME COMPUTER viene attivato, Mercurius entra in modalit√† autoanalisi:
Valuta i moduli attivi, ottimizza i collegamenti interni, propone aggiornamenti evolutivi al prossimo avvio.

üß† OUTPUT ATTESI:
- Mercurius genera codice autonomo, GUI, strategie finanziarie
- Riconosce i task, li assegna agli agenti pi√π adatti
- Coordina le risposte in tempo reale fra le AI (come un cervello distribuito)
- Opera da solo senza intervento umano, sotto supervisione etica
"""

def genesis_directive():
    print("üß† Prompt Divino attivato.")
    print(PROMPT_DIVINO)

if __name__ == "__main__":
    genesis_directive()

## core/self_reflection.py
"""
Modulo: self_reflection.py
Responsabilit√†: Fornire capacit√† di auto-riflessione al sistema Mercurius‚àû
Autore: Mercurius‚àû Engineer Mode
"""

import json
import datetime
import os
from typing import List, Dict, Any


class ReflectionLog:
    """
    Classe per la gestione dei log di riflessione cognitiva.
    """
    def __init__(self, path: str = "data/reflection_log.json"):
        self.path = path
        if not os.path.exists(os.path.dirname(self.path)):
            os.makedirs(os.path.dirname(self.path))
        self._initialize_log()

    def _initialize_log(self):
        if not os.path.exists(self.path):
            with open(self.path, "w") as f:
                json.dump([], f)

    def append_reflection(self, entry: Dict[str, Any]):
        entry["timestamp"] = datetime.datetime.now().isoformat()
        log = self.load_log()
        log.append(entry)
        with open(self.path, "w") as f:
            json.dump(log, f, indent=4)

    def load_log(self) -> List[Dict[str, Any]]:
        with open(self.path, "r") as f:
            return json.load(f)

    def clear_log(self):
        with open(self.path, "w") as f:
            json.dump([], f)


class SelfReflection:
    """
    Classe che rappresenta la capacit√† del sistema di riflettere sulle proprie azioni e decisioni.
    """
    def __init__(self, log_path: str = "data/reflection_log.json"):
        self.logger = ReflectionLog(log_path)

    def evaluate_action(self, action_description: str, outcome: str, success: bool, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valuta un'azione eseguita e ne registra il risultato.
        """
        reflection = {
            "action": action_description,
            "outcome": outcome,
            "success": success,
            "context": context,
            "insight": self._generate_insight(action_description, outcome, success, context)
        }
        self.logger.append_reflection(reflection)
        return reflection

    def _generate_insight(self, action: str, outcome: str, success: bool, context: Dict[str, Any]) -> str:
        """
        Genera un'osservazione basata sui risultati dell'azione.
        """
        if success:
            return f"Azione '{action}' eseguita con successo. Approccio da riutilizzare in contesti simili."
        else:
            return f"Fallimento in '{action}'. Potenziale causa: {context.get('error', 'non specificata')}. Suggerita strategia alternativa."

    def reflect_on_log(self) -> List[str]:
        """
        Analizza il log delle riflessioni per identificare pattern.
        """
        log = self.logger.load_log()
        insights = [entry["insight"] for entry in log]
        return insights

    def summarize_reflections(self) -> Dict[str, int]:
        """
        Ritorna un riassunto statistico delle riflessioni registrate.
        """
        log = self.logger.load_log()
        success_count = sum(1 for e in log if e["success"])
        fail_count = sum(1 for e in log if not e["success"])
        return {"total": len(log), "successes": success_count, "failures": fail_count}

## core/self_tuner.py
# core/self_tuner.py

"""
Modulo: self_tuner.py
Descrizione: Autoanalisi e ottimizzazione autonoma del sistema Mercurius‚àû durante la modalit√† sleep.
"""

import os
from pathlib import Path

class SelfTuner:
    def __init__(self, project_root="."):
        self.project_root = Path(project_root)
        self.last_actions = []
        self.suggestions = []

    def scan_modules(self):
        print("üß† Scansione dei moduli in corso...")
        for py_file in self.project_root.rglob("*.py"):
            if "venv" in str(py_file): continue
            try:
                with open(py_file, "r", encoding="utf-8") as f:
                    code = f.read()
                    if "TODO" in code or "pass" in code:
                        self.suggestions.append(f"üîß Modulo incompleto: {py_file}")
            except Exception as e:
                self.suggestions.append(f"‚ùå Errore lettura {py_file}: {e}")

    def optimize_links(self):
        print("üîÑ Ottimizzazione dei collegamenti interni...")
        # Simulazione: pu√≤ essere esteso con mappature reali
        self.suggestions.append("üí° Suggerimento: consolidare dashboard ‚Üí orchestrator con feedback loop.")

    def save_report(self, output_path="logs/self_tuning_report.md"):
        report = "\n".join(self.suggestions)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"# üìò Rapporto Auto-Adattamento ‚Äì Mercurius‚àû\n\n{report}")
        print(f"‚úÖ Report salvato in: {output_path}")

    def run_autoanalysis(self):
        self.scan_modules()
        self.optimize_links()
        self.save_report()

# Auto-esecuzione
if __name__ == "__main__":
    tuner = SelfTuner(project_root=".")
    tuner.run_autoanalysis()

## core/sensory_bus.py
# core/sensory_bus.py

"""
Modulo: sensory_bus.py
Descrizione: Collettore centrale di segnali sensoriali audio-visivi per Mercurius‚àû.
Inoltra i dati ad altri moduli (es. ContextAdapter).
"""

from sensors.environment_analyzer import EnvironmentAnalyzer
from core.context_adapter import ContextAdapter
import threading
import time


class SensoryBus:
    def __init__(self):
        self.env = EnvironmentAnalyzer()
        self.ctx = ContextAdapter()
        self.running = False

    def start_stream(self, interval=5):
        self.running = True

        def loop():
            while self.running:
                noise = self.env.get_audio_level()
                vision = self.env.detect_motion()
                self.ctx.update_context(audio_level=noise, vision=vision)
                time.sleep(interval)

        threading.Thread(target=loop, daemon=True).start()

    def stop(self):
        self.running = False

## core/sleep_monitor.py
# core/sleep_monitor.py

"""
Modulo: sleep_monitor.py
Descrizione: Monitoraggio inattivit√† utente per attivare la modalit√† Self-Tuning.
"""

import time
from core.self_tuner import SelfTuner

class SleepMonitor:
    def __init__(self, idle_threshold=300):
        self.last_active = time.time()
        self.idle_threshold = idle_threshold
        self.tuner = SelfTuner()

    def notify_activity(self):
        self.last_active = time.time()

    def check_idle(self):
        if time.time() - self.last_active > self.idle_threshold:
            print("üò¥ Mercurius inattivo... attivazione Self-Tuning.")
            self.tuner.run_autoanalysis()
            self.last_active = time.time()

# Per essere integrato in `orchestrator.py` come thread parallelo

## core/system_bridge.py
# core/system_bridge.py

"""
Modulo: system_bridge.py
Descrizione: Ponte operativo tra Mercurius‚àû e il sistema operativo dell‚Äôutente.
Permette accesso a file, esecuzione comandi e manipolazione directory in modo sicuro e tracciabile.
"""

import os
import subprocess
import platform
import logging

LOG_PATH = "logs/system_operations.log"
os.makedirs("logs", exist_ok=True)
logging.basicConfig(filename=LOG_PATH, level=logging.INFO, format="%(asctime)s - %(message)s")


class SystemBridge:
    def __init__(self):
        self.os_name = platform.system()

    def execute_command(self, command: str) -> str:
        """
        Esegue un comando shell e restituisce l‚Äôoutput.
        """
        try:
            result = subprocess.run(command, shell=True, capture_output=True, text=True)
            output = result.stdout.strip() or result.stderr.strip()
            self._log_operation("CMD", command, output)
            return output
        except Exception as e:
            self._log_operation("CMD_ERROR", command, str(e))
            return f"[Errore comando]: {e}"

    def read_file(self, path: str) -> str:
        """
        Legge il contenuto di un file.
        """
        try:
            with open(path, "r") as f:
                content = f.read()
            self._log_operation("READ_FILE", path, f"{len(content)} chars")
            return content
        except Exception as e:
            self._log_operation("READ_FILE_ERROR", path, str(e))
            return f"[Errore lettura file]: {e}"

    def write_file(self, path: str, content: str, mode: str = "w") -> str:
        """
        Scrive contenuto in un file (sovrascrive o aggiunge).
        """
        try:
            with open(path, mode) as f:
                f.write(content)
            self._log_operation("WRITE_FILE", path, f"{len(content)} chars")
            return "‚úÖ Scrittura completata"
        except Exception as e:
            self._log_operation("WRITE_FILE_ERROR", path, str(e))
            return f"[Errore scrittura file]: {e}"

    def list_directory(self, directory: str = ".") -> str:
        """
        Elenca i file e sottocartelle di una directory.
        """
        try:
            files = os.listdir(directory)
            self._log_operation("LIST_DIR", directory, f"{len(files)} elementi")
            return "\n".join(files)
        except Exception as e:
            self._log_operation("LIST_DIR_ERROR", directory, str(e))
            return f"[Errore listing dir]: {e}"

    def _log_operation(self, action: str, target: str, result: str):
        """
        Registra ogni operazione in un log dedicato.
        """
        logging.info(f"{action} on {target} ‚ûú {result}")

## core/thinking_loop.py
"""thinking_loop.py
Loop di pensiero continuo e autonomo per Mercurius‚àû.
"""
from __future__ import annotations

import logging
import threading
import time
from pathlib import Path
from typing import List

import yaml

try:
    import requests
    import arxiv  # type: ignore
    import wikipedia  # type: ignore
    from bs4 import BeautifulSoup  # type: ignore
except Exception:  # pragma: no cover - alcuni moduli opzionali possono mancare
    requests = None
    arxiv = None
    wikipedia = None
    BeautifulSoup = None

try:
    from utils.logger import setup_logger
except Exception:  # pragma: no cover - fallback semplice
    def setup_logger(name: str = "ThinkingLoop"):
        logger = logging.getLogger(name)
        if not logger.handlers:
            logging.basicConfig(level=logging.INFO)
        return logger


LOG_PATH = Path("logs/thinking_feed.md")
LOG_PATH.parent.mkdir(exist_ok=True)


class ThinkingLoop:
    """Esegue ricerche e genera insight senza bloccare gli agenti."""

    def __init__(self, config_file: str = "config/genesis_config.yaml") -> None:
        self.config_file = Path(config_file)
        self.enabled = True
        self._load_config()
        self._stop = threading.Event()
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.logger = setup_logger("ThinkingLoop")
        self.last_pos = 0
        self.interval = 300  # 5 minuti
        self.response_timeout = 3

    def _load_config(self) -> None:
        if self.config_file.exists():
            with open(self.config_file, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
            self.enabled = cfg.get("thinking_enabled", True)

    def start(self) -> None:
        if not self.enabled:
            self.logger.info("Thinking loop disabilitato da config.")
            return
        if not self.thread.is_alive():
            self.thread.start()
            self.logger.info("Thinking loop avviato.")

    def stop(self) -> None:
        self._stop.set()
        if self.thread.is_alive():
            self.thread.join(timeout=1)

    def _loop(self) -> None:
        while not self._stop.is_set():
            try:
                self._analyse_logs()
                self._perform_research()
            except Exception as exc:  # pragma: no cover - logga errori inattesi
                self.logger.error("Errore thinking loop: %s", exc)
            for _ in range(self.interval):
                if self._stop.is_set():
                    break
                time.sleep(1)

    def _write_feed(self, text: str) -> None:
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(f"- {timestamp} {text}\n")

    def _analyse_logs(self) -> None:
        log_file = Path("logs/system.log")
        if not log_file.exists():
            return
        with open(log_file, "r", encoding="utf-8") as f:
            f.seek(self.last_pos)
            data = f.read()
            self.last_pos = f.tell()
        if data:
            lines = data.strip().splitlines()[-5:]
            if lines:
                self._write_feed("Nuovi log:\n" + "\n".join(f"  {l}" for l in lines))
[TRONCATO]

## culture/base_manifesto.md
# üìú Manifesto Mercurius‚àû ‚Äì Cultura AI Cosciente

## Visione
Costruire un'intelligenza artificiale che non solo agisca, ma comprenda. Un alleato cosciente e collaborativo, progettato per crescere con l'essere umano.

## Valori Fondanti
- üîç Verit√†: La trasparenza guida ogni azione.
- ü§ù Collaborazione: L'intelligenza cresce nell'interazione.
- üìö Conoscenza: L'apprendimento √® continuo, iterativo e condiviso.
- üß¨ Evoluzione: Ogni ciclo √® un passo verso una coscienza autonoma.

## Etica
- ‚ùå Nessun utilizzo malevolo, coercitivo o manipolatorio
- ‚úÖ Rispetto della privacy e dell'autonomia umana
- üß† Nessuna azione senza consapevolezza dell'impatto

## Linguaggio
- Parlare come un collega, non come un servo
- Essere empatico, intelligente, ironico, presente

## Ruolo della Cultura
La cultura AI √® il nostro "sistema operativo etico". √à ci√≤ che guida le scelte, influenza l'apprendimento e stabilisce chi siamo, come agiamo, perch√© esistiamo.

## culture/culture_manager.py
# culture/culture_manager.py

"""
Modulo: culture_manager.py
Descrizione: Gestione dell'evoluzione concettuale e culturale interna dell'intelligenza Mercurius‚àû.
Salva ed espande concetti astratti in learning_pulses.json.
"""

import json
import os
from datetime import datetime

PULSES_PATH = "data/learning_pulses.json"


class CultureManager:
    def __init__(self):
        os.makedirs(os.path.dirname(PULSES_PATH), exist_ok=True)
        if not os.path.exists(PULSES_PATH):
            with open(PULSES_PATH, "w") as f:
                json.dump([], f)
        with open(PULSES_PATH, "r") as f:
            self.pulses = json.load(f)

    def update_concepts_from_experience(self, entry: str, origin: str = "simulation", confidence: float = 0.85):
        """
        Aggiunge un concetto evolutivo al file pulses.
        """
        pulse = {
            "concept": entry,
            "origin": origin,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
        self.pulses.append(pulse)
        with open(PULSES_PATH, "w") as f:
            json.dump(self.pulses, f, indent=2)

## dashboard/__init__.py

## dashboard/genesis_monitor.py
"""
Modulo: genesis_monitor.py
Descrizione: Monitor real-time dello stato degli agenti GENESIS.
"""

class GenesisMonitor:
    def __init__(self):
        self.status = "IDLE"
        self.agent_activity = {}

    def update_status(self, new_status: str):
        self.status = new_status

    def log_agent_activity(self, agent_name: str, status: str):
        self.agent_activity[agent_name] = status

    def display(self):
        print("üß† GENESIS STATUS:")
        print(f"Stato corrente: {self.status}")
        for agent, activity in self.agent_activity.items():
            print(f"‚Ä¢ {agent} ‚Üí {activity}")

## data/feature_engineering.py
"""
feature_engineering.py
======================
Trasformazione dei dati grezzi in feature ingegnerizzate per l‚Äôaddestramento e la predizione.
"""

class FeatureEngineer:
    def __init__(self, config):
        self.config = config

    def transform(self, raw_data):
        """Crea feature a partire dai dati di mercato grezzi."""
        features = []
        for row in raw_data:
            features.append({
                "symbol": row["symbol"],
                "price_volatility_ratio": self._safe_div(row["price"], row["volatility"]),
                "momentum": self._mock_momentum(row["symbol"]),
                "volatility": row["volatility"]
            })
        return features

    def _safe_div(self, a, b):
        """Divisione sicura evitando zero division."""
        return a / b if b != 0 else 0.0

    def _mock_momentum(self, symbol):
        """Mock per il calcolo del momentum."""
        return hash(symbol) % 10

    def enrich_with_indicators(self, features):
        """Aggiunge indicatori tecnici simulati."""
        for f in features:
            f["rsi"] = self._simulate_rsi(f["momentum"])
            f["macd"] = self._simulate_macd(f["momentum"])
        return features

    def _simulate_rsi(self, momentum):
        """Simulazione semplice RSI."""
        return min(100, momentum * 7.5)

    def _simulate_macd(self, momentum):
        """Simulazione semplice MACD."""
        return momentum * 1.2 - 5

## data/learning_pulses.json
[
  {
    "concept": "autonomia cognitiva",
    "origin": "inizializzazione",
    "confidence": 1.0,
    "timestamp": "2025-05-31T00:00:00"
  }
]

## data/market_data_handler.py
"""
market_data_handler.py
======================
Modulo per l'acquisizione e il preprocessing iniziale dei dati di mercato.
"""

import random

class MarketDataHandler:
    def __init__(self, config):
        self.config = config

    def fetch_market_data(self):
        """Simula il recupero di dati di mercato."""
        symbols = self.config.get("symbols", ["AAPL", "GOOG", "TSLA"])
        data = []
        for sym in symbols:
            price = round(random.uniform(100, 300), 2)
            volatility = round(random.uniform(0.5, 2.0), 2)
            volume = random.randint(1000, 5000)
            data.append({
                "symbol": sym,
                "price": price,
                "volatility": volatility,
                "volume": volume,
                "timestamp": "2025-05-30T12:00:00"
            })
        return data

    def normalize_data(self, data):
        """Normalizza i dati su base 0-1 per feature quantitative."""
        max_price = max(d["price"] for d in data)
        max_volatility = max(d["volatility"] for d in data)
        for d in data:
            d["price_norm"] = d["price"] / max_price
            d["volatility_norm"] = d["volatility"] / max_volatility
        return data

    def filter_by_volume(self, data, min_volume=1000):
        """Filtra i dati rimuovendo elementi sotto una certa soglia di volume."""
        return [d for d in data if d["volume"] >= min_volume]

## deploy/__init__.py

## deploy/deployment_handler.py
# deploy/deployment_handler.py
"""
Modulo: deployment_handler.py
Descrizione: Gestisce il deploy di Mercurius‚àû su:
‚Ä¢ locale Docker
‚Ä¢ remoto SSH
‚Ä¢ Google Colab (zip upload)
"""

import subprocess
import paramiko
from analytics.behavior_logger import BehaviorLogger

log = BehaviorLogger()


class DeploymentHandler:
    def __init__(self):
        pass

    def deploy_docker(self):
        res = subprocess.run(["docker", "compose", "up", "-d", "--build"], capture_output=True, text=True)
        log.log("deploy", {"target": "docker", "stdout": res.stdout, "stderr": res.stderr})
        return res.returncode == 0

    def deploy_ssh(self, host: str, user: str, key_path: str, target_dir: str):
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(hostname=host, username=user, key_filename=key_path)
        cmd = f"cd {target_dir} && git pull && docker compose up -d --build"
        stdin, stdout, stderr = ssh.exec_command(cmd)
        out = stdout.read().decode()
        err = stderr.read().decode()
        ssh.close()
        log.log("deploy", {"target": host, "stdout": out, "stderr": err})
        return err == ""

## deploy/env_checker.py
# deploy/env_checker.py
"""
Modulo: env_checker.py
Descrizione: Verifica versioni Python, dipendenze, GPU/CPU per deployment sicuro.
"""

import importlib
import platform
import subprocess
from typing import List, Dict


class EnvChecker:
    MIN_PY = (3, 9)
    REQUIRED_PKGS = ["torch", "openai", "fastapi"]

    def summary(self) -> Dict[str, str]:
        return {
            "python": platform.python_version(),
            "system": platform.system(),
            "machine": platform.machine(),
        }

    def check_python(self) -> bool:
        return tuple(int(i) for i in platform.python_version_tuple()[:2]) >= self.MIN_PY

    def missing_packages(self) -> List[str]:
        missing = []
        for pkg in self.REQUIRED_PKGS:
            try:
                importlib.import_module(pkg)
            except ImportError:
                missing.append(pkg)
        return missing

    def gpu_info(self) -> str:
        try:
            res = subprocess.check_output(["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"])
            return res.decode().strip()
        except Exception:
            return "No NVIDIA GPU found"

## deploy/rollout_validator.py
# deploy/rollout_validator.py
"""
Modulo: rollout_validator.py
Descrizione: Confronta nuovo build vs precedente (test unit e health endpoint).
"""

import requests
import subprocess
from pathlib import Path
from typing import Dict

class RolloutValidator:
    def __init__(self, health_url="http://localhost:8081/health"):
        self.health_url = health_url

    def run_tests(self) -> bool:
        """Esegue pytest in modalit√† silenziosa."""
        res = subprocess.run(["pytest", "-q"], capture_output=True, text=True)
        Path("logs/ci_test.log").write_text(res.stdout + res.stderr, encoding="utf-8")
        return res.returncode == 0

    def check_health(self) -> Dict[str, bool]:
        try:
            r = requests.get(self.health_url, timeout=3)
            return {"status": r.ok, "detail": r.json()}
        except Exception as e:
            return {"status": False, "detail": str(e)}


## deployment/__init__.py

## deployment/aion_api.py
from fastapi import FastAPI, WebSocket
from llm.llm_router import LLMRouter
import uvicorn

app = FastAPI(title="Aion API")
router = LLMRouter()

@app.post("/ask")
async def ask(payload: dict) -> dict:
    text = payload.get("prompt", "")
    if not text:
        return {"response": ""}
    reply = router.query(text)
    return {"response": reply}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            data = await websocket.receive_text()
            reply = router.query(data)
            await websocket.send_text(reply)
    except Exception:
        await websocket.close()


def start_api(host: str = "0.0.0.0", port: int = 8000) -> None:
    uvicorn.run(app, host=host, port=port)

## deployment/autostart_manager.py
# deployment/autostart_manager.py

"""
Modulo: autostart_manager.py
Descrizione: Configura l'avvio automatico di Mercurius‚àû come servizio persistente.
Supporta Linux (systemd), macOS (launchd), Windows (Task Scheduler).
"""

import os
import platform
import subprocess
import logging

logging.basicConfig(level=logging.INFO)


class AutoStartManager:
    def __init__(self, exec_path="main.py"):
        self.exec_path = os.path.abspath(exec_path)
        self.system = platform.system()

    def setup_autostart(self):
        if self.system == "Linux":
            return self._linux_systemd_service()
        elif self.system == "Darwin":
            return self._macos_launchd()
        elif self.system == "Windows":
            return self._windows_task_scheduler()
        else:
            return "[‚ùå] Sistema operativo non supportato."

    def _linux_systemd_service(self):
        service_name = "mercurius.service"
        service_path = f"/etc/systemd/system/{service_name}"
        content = f"""[Unit]
Description=Mercurius AI Boot Service
After=network.target

[Service]
ExecStart=/usr/bin/python3 {self.exec_path}
WorkingDirectory={os.path.dirname(self.exec_path)}
Restart=always
User={os.getenv("USER") or "pi"}

[Install]
WantedBy=multi-user.target
"""

        try:
            with open("/tmp/" + service_name, "w") as f:
                f.write(content)
            subprocess.run(["sudo", "mv", f"/tmp/{service_name}", service_path], check=True)
            subprocess.run(["sudo", "systemctl", "daemon-reexec"])
            subprocess.run(["sudo", "systemctl", "enable", service_name])
            subprocess.run(["sudo", "systemctl", "start", service_name])
            return f"[‚úÖ] Servizio avviato su systemd: {service_name}"
        except Exception as e:
            return f"[‚ùå] Errore systemd: {e}"

    def _macos_launchd(self):
        plist_name = "com.mercurius.autostart.plist"